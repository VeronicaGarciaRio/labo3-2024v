{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en esta version cambio el modelo para evitar sobreajuste, incorporo a los clusters y a la red 4 variables nuevas: dif entre dic y feb, totales 2019 y 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "import joblib\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense, Dropout, Activation\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Importa EarlyStopping desde callbacks\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import tensorflow as tf\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>111.51691</td>\n",
       "      <td>109.05244</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>178.49426</td>\n",
       "      <td>176.02980</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>240.59870</td>\n",
       "      <td>236.65556</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id  product_id  periodo  plan_precios_cuidados  cust_request_qty  \\\n",
       "0         10001       20001   201812                    0.0              20.0   \n",
       "1         10001       20001   201901                    0.0              53.0   \n",
       "2         10001       20001   201902                    0.0              39.0   \n",
       "3         10001       20001   201903                    0.0              23.0   \n",
       "4         10001       20001   201904                    0.0              33.0   \n",
       "5         10001       20001   201905                    0.0              31.0   \n",
       "6         10001       20001   201906                    0.0               7.0   \n",
       "7         10001       20001   201907                    0.0              14.0   \n",
       "8         10001       20001   201908                    0.0               9.0   \n",
       "9         10001       20001   201909                    0.0              18.0   \n",
       "10        10001       20001   201910                    0.0              21.0   \n",
       "11        10001       20001   201911                    0.0              21.0   \n",
       "\n",
       "    cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0         254.62373  254.62373   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1         393.26092  386.60688   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2         309.90610  309.90610   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3         142.87158  130.54927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4         364.37071  364.37071   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "5         439.90647  439.90647   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "6          65.92436   65.92436   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "7         144.78714  144.78714   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "8          33.63991   33.63991   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "9         111.51691  109.05244   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "10        178.49426  176.02980   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "11        240.59870  236.65556   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "   descripcion quarter  month  close_quarter   age mes_inicial  \n",
       "0       genoma      Q4     12            1.0  23.0  2018-12-01  \n",
       "1       genoma      Q1      1            0.0  24.0  2018-12-01  \n",
       "2       genoma      Q1      2            0.0  25.0  2018-12-01  \n",
       "3       genoma      Q1      3            1.0  26.0  2018-12-01  \n",
       "4       genoma      Q2      4            0.0  27.0  2018-12-01  \n",
       "5       genoma      Q2      5            0.0  28.0  2018-12-01  \n",
       "6       genoma      Q2      6            1.0  29.0  2018-12-01  \n",
       "7       genoma      Q3      7            0.0  30.0  2018-12-01  \n",
       "8       genoma      Q3      8            0.0  31.0  2018-12-01  \n",
       "9       genoma      Q3      9            1.0  32.0  2018-12-01  \n",
       "10      genoma      Q4     10            0.0  33.0  2018-12-01  \n",
       "11      genoma      Q4     11            0.0  34.0  2018-12-01  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar y preprocesar los datos\n",
    "file_path= \"C:/Users/vgarciario/desktop/UA MASTER/labo3/scripts/labo3-2024v/final_dataset_completo_con_ceros.csv\"\n",
    "#file_path = \"C:/Users/Usuario/desktop/vero2/final_dataset_completo_con_ceros.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head(12)\n",
    "\n",
    "# filtered_df = df[df['product_id'].between(20001, 20012)]\n",
    "\n",
    "# Opcional: Ver las primeras filas del DataFrame filtrado\n",
    "# print(filtered_df.head())\n",
    "\n",
    "# df=filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'product_id', 'periodo', 'plan_precios_cuidados',\n",
       "       'cust_request_qty', 'cust_request_tn', 'tn', 'cat1', 'cat2', 'cat3',\n",
       "       'brand', 'sku_size', 'descripcion', 'quarter', 'month', 'close_quarter',\n",
       "       'age', 'mes_inicial'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], var_name='periodo', value_name='tn')\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas categóricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Agrupar las ventas por periodo, cat1, cat2, cat3, brand y customer_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m', errors='coerce')\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({'tn': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "# Paso 2: Calcular los ratios incluyendo customer_id\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 12)]\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['tn'].sum().reset_index()\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index()\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['ratio'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.87535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10002</td>\n",
       "      <td>0.27780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10003</td>\n",
       "      <td>0.27256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10004</td>\n",
       "      <td>0.13628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10005</td>\n",
       "      <td>0.06290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     periodo  cat1  cat2  cat3  brand  customer_id       tn\n",
       "0 2018-12-01     0     0     4     22        10001  0.87535\n",
       "1 2018-12-01     0     0     4     22        10002  0.27780\n",
       "2 2018-12-01     0     0     4     22        10003  0.27256\n",
       "3 2018-12-01     0     0     4     22        10004  0.13628\n",
       "4 2018-12-01     0     0     4     22        10005  0.06290"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que `grouped_df` es tu DataFrame\n",
    "# Asegúrate de que la columna 'periodo' esté en formato de fecha\n",
    "grouped_df['periodo'] = pd.to_datetime(grouped_df['periodo'])\n",
    "\n",
    "# Filtrar los datos necesarios para los cálculos\n",
    "df_dic2017 = grouped_df[grouped_df['periodo'] == '2017-12-01']\n",
    "df_feb2018 = grouped_df[grouped_df['periodo'] == '2018-02-01']\n",
    "df_dic2018 = grouped_df[grouped_df['periodo'] == '2018-12-01']\n",
    "df_feb2019 = grouped_df[grouped_df['periodo'] == '2019-02-01']\n",
    "df_2018 = grouped_df[grouped_df['periodo'].dt.year == 2018]\n",
    "df_2019 = grouped_df[grouped_df['periodo'].dt.year == 2019]\n",
    "\n",
    "# Merge para obtener las variaciones\n",
    "merged_2018 = pd.merge(df_feb2018, df_dic2017, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('_feb2018', '_dic2017'))\n",
    "merged_2019 = pd.merge(df_feb2019, df_dic2018, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('_feb2019', '_dic2018'))\n",
    "\n",
    "# Calcular las variaciones\n",
    "merged_2018['variacion_feb2018_vs_dic2017'] = merged_2018['tn_feb2018'] - merged_2018['tn_dic2017']\n",
    "merged_2019['variacion_feb2019_vs_dic2018'] = merged_2019['tn_feb2019'] - merged_2019['tn_dic2018']\n",
    "\n",
    "# Agrupar y sumar para obtener las sumas totales de tn para 2018 y 2019\n",
    "sum_2018 = df_2018.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index().rename(columns={'tn': 'suma_total_2018'})\n",
    "sum_2019 = df_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index().rename(columns={'tn': 'suma_total_2019'})\n",
    "\n",
    "# Unir todos los resultados en un solo DataFrame\n",
    "final_df = merged_2018[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2018_vs_dic2017']].merge(\n",
    "    merged_2019[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2019_vs_dic2018']],\n",
    "    on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    ").merge(\n",
    "    sum_2018, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    ").merge(\n",
    "    sum_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    ")\n",
    "\n",
    "# Agregar las columnas 'tn' y 'periodo' originales\n",
    "final_df = final_df.merge(grouped_df[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn', 'periodo']], on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='left')\n",
    "\n",
    "# Rellenar NaNs con 0\n",
    "final_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0     0     0     4     22        10001                           0.0   \n",
       "1     0     0     4     22        10001                           0.0   \n",
       "2     0     0     4     22        10001                           0.0   \n",
       "3     0     0     4     22        10001                           0.0   \n",
       "4     0     0     4     22        10001                           0.0   \n",
       "\n",
       "   variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019       tn  \\\n",
       "0                      -0.60803          0.87535          3.28124  0.87535   \n",
       "1                      -0.60803          0.87535          3.28124  0.30925   \n",
       "2                      -0.60803          0.87535          3.28124  0.26732   \n",
       "3                      -0.60803          0.87535          3.28124  0.38264   \n",
       "4                      -0.60803          0.87535          3.28124  0.06290   \n",
       "\n",
       "     periodo  \n",
       "0 2018-12-01  \n",
       "1 2019-01-01  \n",
       "2 2019-02-01  \n",
       "3 2019-03-01  \n",
       "4 2019-04-01  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df tiene datos originales, final_df_es esta escalado\n",
    "final_df_esc=final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.136657</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.136657</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>-0.078741</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.136657</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>-0.084338</td>\n",
       "      <td>2019-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.136657</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>-0.068945</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.136657</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>-0.111625</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0     0     0     4     22        10001                           0.0   \n",
       "1     0     0     4     22        10001                           0.0   \n",
       "2     0     0     4     22        10001                           0.0   \n",
       "3     0     0     4     22        10001                           0.0   \n",
       "4     0     0     4     22        10001                           0.0   \n",
       "\n",
       "   variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019        tn  \\\n",
       "0                     -0.136657         0.023427        -0.093435 -0.003176   \n",
       "1                     -0.136657         0.023427        -0.093435 -0.078741   \n",
       "2                     -0.136657         0.023427        -0.093435 -0.084338   \n",
       "3                     -0.136657         0.023427        -0.093435 -0.068945   \n",
       "4                     -0.136657         0.023427        -0.093435 -0.111625   \n",
       "\n",
       "     periodo  \n",
       "0 2018-12-01  \n",
       "1 2019-01-01  \n",
       "2 2019-02-01  \n",
       "3 2019-03-01  \n",
       "4 2019-04-01  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columnas a escalar\n",
    "columns_to_scale = [\n",
    "    'variacion_feb2018_vs_dic2017',\n",
    "    'variacion_feb2019_vs_dic2018',\n",
    "    'suma_total_2018',\n",
    "    'suma_total_2019',\n",
    "    'tn'\n",
    "]\n",
    "\n",
    "# Inicializar el StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar y transformar las columnas seleccionadas\n",
    "final_df_esc[columns_to_scale] = scaler.fit_transform(final_df_esc[columns_to_scale])\n",
    "\n",
    "final_df_esc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>2018-12-01</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <th>2019-02-01</th>\n",
       "      <th>2019-03-01</th>\n",
       "      <th>2019-04-01</th>\n",
       "      <th>2019-05-01</th>\n",
       "      <th>2019-06-01</th>\n",
       "      <th>2019-07-01</th>\n",
       "      <th>2019-08-01</th>\n",
       "      <th>2019-09-01</th>\n",
       "      <th>2019-10-01</th>\n",
       "      <th>2019-11-01</th>\n",
       "      <th>2019-12-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">22</th>\n",
       "      <th>10001</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.136657</th>\n",
       "      <th>0.023427</th>\n",
       "      <th>-0.093435</th>\n",
       "      <td>-0.003176</td>\n",
       "      <td>-0.078741</td>\n",
       "      <td>-0.084338</td>\n",
       "      <td>-0.068945</td>\n",
       "      <td>-0.111625</td>\n",
       "      <td>-0.073842</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.033961</td>\n",
       "      <td>-0.050054</td>\n",
       "      <td>-0.066147</td>\n",
       "      <td>-0.074543</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.063241</th>\n",
       "      <th>-0.066108</th>\n",
       "      <th>-0.118003</th>\n",
       "      <td>-0.082939</td>\n",
       "      <td>-0.110926</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.081539</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.073842</td>\n",
       "      <td>-0.093083</td>\n",
       "      <td>-0.112324</td>\n",
       "      <td>-0.074543</td>\n",
       "      <td>-0.117222</td>\n",
       "      <td>-0.120021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.062076</th>\n",
       "      <th>-0.066893</th>\n",
       "      <th>-0.102973</th>\n",
       "      <td>-0.083639</td>\n",
       "      <td>-0.092734</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.065447</td>\n",
       "      <td>-0.047255</td>\n",
       "      <td>-0.047255</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.083638</td>\n",
       "      <td>-0.047255</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "      <td>-0.120021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.089413</th>\n",
       "      <th>-0.087313</th>\n",
       "      <th>-0.066632</th>\n",
       "      <td>-0.101830</td>\n",
       "      <td>-0.083639</td>\n",
       "      <td>-0.047255</td>\n",
       "      <td>-0.038159</td>\n",
       "      <td>-0.074543</td>\n",
       "      <td>-0.101830</td>\n",
       "      <td>-0.010872</td>\n",
       "      <td>-0.083639</td>\n",
       "      <td>-0.051803</td>\n",
       "      <td>-0.019968</td>\n",
       "      <td>-0.019968</td>\n",
       "      <td>-0.065447</td>\n",
       "      <td>-0.120021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.014832</th>\n",
       "      <th>-0.098308</th>\n",
       "      <th>-0.124383</th>\n",
       "      <td>-0.111625</td>\n",
       "      <td>-0.113725</td>\n",
       "      <td>-0.101830</td>\n",
       "      <td>-0.113024</td>\n",
       "      <td>-0.113024</td>\n",
       "      <td>-0.110226</td>\n",
       "      <td>-0.108826</td>\n",
       "      <td>-0.113725</td>\n",
       "      <td>-0.107777</td>\n",
       "      <td>-0.101830</td>\n",
       "      <td>-0.112324</td>\n",
       "      <td>-0.115823</td>\n",
       "      <td>-0.119322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "periodo                                                                                                                     2018-12-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.003176   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.082939   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.083639   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.101830   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.111625   \n",
       "\n",
       "periodo                                                                                                                     2019-01-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.078741   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.110926   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.092734   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.083639   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.113725   \n",
       "\n",
       "periodo                                                                                                                     2019-02-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.084338   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.120021   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.047255   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.101830   \n",
       "\n",
       "periodo                                                                                                                     2019-03-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.068945   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.120021   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.065447   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.038159   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.113024   \n",
       "\n",
       "periodo                                                                                                                     2019-04-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.111625   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.120021   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.047255   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.074543   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.113024   \n",
       "\n",
       "periodo                                                                                                                     2019-05-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.073842   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.081539   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.047255   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.101830   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.110226   \n",
       "\n",
       "periodo                                                                                                                     2019-06-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.120021   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.120021   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.010872   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.108826   \n",
       "\n",
       "periodo                                                                                                                     2019-07-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.033961   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.073842   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.083639   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.113725   \n",
       "\n",
       "periodo                                                                                                                     2019-08-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.050054   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.093083   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.083638   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.051803   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.107777   \n",
       "\n",
       "periodo                                                                                                                     2019-09-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.066147   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.112324   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.047255   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.019968   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.101830   \n",
       "\n",
       "periodo                                                                                                                     2019-10-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.074543   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.074543   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.019968   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.112324   \n",
       "\n",
       "periodo                                                                                                                     2019-11-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.120021   \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.117222   \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021   \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.065447   \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.115823   \n",
       "\n",
       "periodo                                                                                                                     2019-12-01  \n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019              \n",
       "0    0    4    22    10001       0.0                          -0.136657                     0.023427       -0.093435         -0.120021  \n",
       "                     10002       0.0                          -0.063241                    -0.066108       -0.118003         -0.120021  \n",
       "                     10003       0.0                          -0.062076                    -0.066893       -0.102973         -0.120021  \n",
       "                     10004       0.0                           0.089413                    -0.087313       -0.066632         -0.120021  \n",
       "                     10005       0.0                           0.014832                    -0.098308       -0.124383         -0.119322  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivotear el DataFrame\n",
    "pivoted_df = final_df_esc.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2018_vs_dic2017', 'variacion_feb2019_vs_dic2018', 'suma_total_2018', 'suma_total_2019'], columns='periodo', values='tn').fillna(0)\n",
    "pivoted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40383, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convertir el DataFrame pivoteado a un array numpy\n",
    "X = pivoted_df.values\n",
    "\n",
    "# Asegurar que X tiene la forma (n_samples, n_timesteps, n_features)\n",
    "X = np.expand_dims(X, axis=-1)  # Agregar una dimensión para n_features si es necesario\n",
    "\n",
    "# Comprobar la forma de X\n",
    "print(X.shape)  # Debería ser (n_samples, n_timesteps, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TimeSeriesKMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtw\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Ajustar el modelo usando las series temporales escaladas\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Obtener los clusters asignados a cada serie temporal\u001b[39;00m\n\u001b[0;32m      9\u001b[0m clusters \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:821\u001b[0m, in \u001b[0;36mTimeSeriesKMeans.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_successful \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    820\u001b[0m n_attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 821\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_one_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minertia_ \u001b[38;5;241m<\u001b[39m min_inertia:\n\u001b[0;32m    823\u001b[0m     best_correct_centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:675\u001b[0m, in \u001b[0;36mTimeSeriesKMeans._fit_one_init\u001b[1;34m(self, X, x_squared_norms, rs)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect metric: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftdtw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[0;32m    674\u001b[0m             )\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_ \u001b[38;5;241m=\u001b[39m \u001b[43m_k_init_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdist_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrs\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    679\u001b[0m     indices \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mchoice(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:125\u001b[0m, in \u001b[0;36m_k_init_metric\u001b[1;34m(X, n_clusters, cdist_metric, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    122\u001b[0m numpy\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mcdist_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[0;32m    128\u001b[0m numpy\u001b[38;5;241m.\u001b[39mminimum(\n\u001b[0;32m    129\u001b[0m     closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:657\u001b[0m, in \u001b[0;36mTimeSeriesKMeans._fit_one_init.<locals>.metric_fun\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetric_fun\u001b[39m(x, y):\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_dtw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_params\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\dtw_variants.py:1934\u001b[0m, in \u001b[0;36mcdist_dtw\u001b[1;34m(dataset1, dataset2, global_constraint, sakoe_chiba_radius, itakura_max_slope, n_jobs, verbose, be)\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cross-similarity matrix using Dynamic Time Warping (DTW)\u001b[39;00m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;124;03msimilarity measure.\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;124;03m       Signal Processing, vol. 26(1), pp. 43--49, 1978.\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1933\u001b[0m be \u001b[38;5;241m=\u001b[39m instantiate_backend(be, dataset1, dataset2)\n\u001b[1;32m-> 1934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cdist_generic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_diagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43msakoe_chiba_radius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msakoe_chiba_radius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitakura_max_slope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitakura_max_slope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\utils.py:101\u001b[0m, in \u001b[0;36m_cdist_generic\u001b[1;34m(dist_fun, dataset1, dataset2, n_jobs, verbose, compute_diagonal, dtype, be, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     dataset2 \u001b[38;5;241m=\u001b[39m to_time_series_dataset(dataset2, dtype\u001b[38;5;241m=\u001b[39mdtype, be\u001b[38;5;241m=\u001b[39mbe)\n\u001b[1;32m--> 101\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m be\u001b[38;5;241m.\u001b[39mreshape(be\u001b[38;5;241m.\u001b[39marray(matrix), (\u001b[38;5;28mlen\u001b[39m(dataset1), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\dtw_variants.py:776\u001b[0m, in \u001b[0;36mdtw\u001b[1;34m(s1, s2, global_constraint, sakoe_chiba_radius, itakura_max_slope, be)\u001b[0m\n\u001b[0;32m    774\u001b[0m be \u001b[38;5;241m=\u001b[39m instantiate_backend(be, s1, s2)\n\u001b[0;32m    775\u001b[0m s1 \u001b[38;5;241m=\u001b[39m to_time_series(s1, remove_nans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, be\u001b[38;5;241m=\u001b[39mbe)\n\u001b[1;32m--> 776\u001b[0m s2 \u001b[38;5;241m=\u001b[39m \u001b[43mto_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_nans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    780\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the input time series contains only nans or has zero length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    781\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\utils\\utils.py:165\u001b[0m, in \u001b[0;36mto_time_series\u001b[1;34m(ts, remove_nans, be)\u001b[0m\n\u001b[0;32m    163\u001b[0m     ts_out \u001b[38;5;241m=\u001b[39m be\u001b[38;5;241m.\u001b[39mcast(ts_out, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_nans:\n\u001b[1;32m--> 165\u001b[0m     ts_out \u001b[38;5;241m=\u001b[39m ts_out[: \u001b[43mts_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbe\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ts_out\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\utils\\utils.py:446\u001b[0m, in \u001b[0;36mts_size\u001b[1;34m(ts, be)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns actual time series size.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03mFinal timesteps that have `NaN` values for all dimensions will be removed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m3\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m be \u001b[38;5;241m=\u001b[39m instantiate_backend(be, ts)\n\u001b[1;32m--> 446\u001b[0m ts_ \u001b[38;5;241m=\u001b[39m \u001b[43mto_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m sz \u001b[38;5;241m=\u001b[39m be\u001b[38;5;241m.\u001b[39mshape(ts_)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m sz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m be\u001b[38;5;241m.\u001b[39mall(be\u001b[38;5;241m.\u001b[39misnan(ts_[sz \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])):\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\utils\\utils.py:159\u001b[0m, in \u001b[0;36mto_time_series\u001b[1;34m(ts, remove_nans, be)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transforms a time series so that it fits the format used in ``tslearn``\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mmodels.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mto_time_series_dataset : Transforms a dataset of time series\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m be \u001b[38;5;241m=\u001b[39m instantiate_backend(be, ts)\n\u001b[1;32m--> 159\u001b[0m ts_out \u001b[38;5;241m=\u001b[39m be\u001b[38;5;241m.\u001b[39marray(ts)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ts_out\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    161\u001b[0m     ts_out \u001b[38;5;241m=\u001b[39m be\u001b[38;5;241m.\u001b[39mreshape(ts_out, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Crear el modelo TimeSeriesKMeans\n",
    "n_clusters = 15\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "\n",
    "# Ajustar el modelo usando las series temporales escaladas\n",
    "model.fit(X)\n",
    "\n",
    "# Obtener los clusters asignados a cada serie temporal\n",
    "clusters = model.labels_\n",
    "\n",
    "# Agregar los clusters al DataFrame pivoteado\n",
    "X['cluster'] = clusters\n",
    "\n",
    "# Reiniciar el índice para que `pivoted_df` tenga los índices como columnas normales\n",
    "X.reset_index(inplace=True)\n",
    "\n",
    "# Fusionar `final_df` con `pivoted_df` usando `left_on` y `right_on`\n",
    "grouped_dff = final_df.merge(X[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'cluster']], \n",
    "                             on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], \n",
    "                             how='left')\n",
    "\n",
    "# Mostrar el DataFrame final con los números de grupo\n",
    "display(grouped_dff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dff.to_csv(\"C:/Users/Usuario/desktop/vero2/grouped_dff.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
      "0         0     0     4     22        10001                           0.0   \n",
      "1         0     0     4     22        10002                           0.0   \n",
      "2         0     0     4     22        10003                           0.0   \n",
      "3         0     0     4     22        10004                           0.0   \n",
      "4         0     0     4     22        10005                           0.0   \n",
      "...     ...   ...   ...    ...          ...                           ...   \n",
      "40378     3    13    82     32        10363                           0.0   \n",
      "40379     3    13    82     32        10367                           0.0   \n",
      "40380     3    13    82     32        10482                           0.0   \n",
      "40381     3    13    82     32        10513                           0.0   \n",
      "40382     3    13    82     32        10552                           0.0   \n",
      "\n",
      "       variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019  cluster  \n",
      "0                          -0.60803          0.87535         3.281240        0  \n",
      "1                          -0.27780          0.27780         1.323515       11  \n",
      "2                          -0.27256          0.27256         2.521215       11  \n",
      "3                           0.40885          0.13628         5.417185        7  \n",
      "4                           0.07338          0.06290         0.815065       12  \n",
      "...                             ...              ...              ...      ...  \n",
      "40378                       0.00000          0.00000         0.000000        5  \n",
      "40379                      -0.00146          0.00146         0.001460       14  \n",
      "40380                       0.00000          0.00000         0.001460        6  \n",
      "40381                       0.00000          0.00000         0.000730        6  \n",
      "40382                       0.00000          0.00000         0.001460       12  \n",
      "\n",
      "[40383 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#control\n",
    "# Dropear la columna 'periodo'\n",
    "check = grouped_dff.drop(columns=['periodo'])\n",
    "\n",
    "# Agrupar por 'cat1', 'cat2', 'cat3', 'brand', 'customer_id' y obtener el cluster asignado\n",
    "result_df = check.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018','suma_total_2018','suma_total_2019']).agg({\n",
    "    'cluster': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cat1, cat2, cat3, brand, customer_id, variacion_feb2018_vs_dic2017, variacion_feb2019_vs_dic2018, suma_total_2018, suma_total_2019, cluster]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'cat1', 'cat2', 'cat3', 'brand', 'customer_id' y contar clusters únicos\n",
    "cluster_counts = check.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018','suma_total_2018','suma_total_2019']).agg({\n",
    "    'cluster': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Filtrar combinaciones con más de un cluster\n",
    "multiple_clusters = cluster_counts[cluster_counts['cluster'] > 1]\n",
    "\n",
    "# Mostrar combinaciones con más de un cluster\n",
    "print(multiple_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasta aca ok con grouped_dff guardado en vero2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para acumular las filas de resultados\n",
    "resultados_por_producto = []\n",
    "\n",
    "# Elegir un número de pasos de tiempo\n",
    "n_steps = 13  # Ventana de tiempo de 18 meses\n",
    "n_features = 10 # Cambia esto si tienes más características\n",
    "step_ahead= 2\n",
    "\n",
    "def crear_secuencias(datos, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(datos) - n_steps - step_ahead + 1):\n",
    "        end_ix = i + n_steps\n",
    "        out_end_ix = end_ix + step_ahead - 1\n",
    "        seq_x, seq_y = datos[i:end_ix], datos[out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=256, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=(input_shape[0], input_shape[1])))\n",
    "    model.add(Dropout(0.5))  # Increased dropout rate\n",
    "    model.add(LSTM(units=128, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))  # Increased dropout rate\n",
    "    model.add(Dense(64, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Use the updated early_stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reescalar 'tn' en grouped_df\n",
    "scaler = StandardScaler()\n",
    "grouped_dff['tn_scaled'] = scaler.fit_transform(grouped_dff[['tn']])\n",
    "grouped_dff['suma_total_2019_scaled'] = scaler.fit_transform(grouped_dff[['suma_total_2019']])\n",
    "grouped_dff['suma_total_2018_scaled'] = scaler.fit_transform(grouped_dff[['suma_total_2018']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora tengo grouped_dff con escalado en tn: se agrega columna tn_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "      <th>cluster</th>\n",
       "      <th>tn_scaled</th>\n",
       "      <th>suma_total_2019_scaled</th>\n",
       "      <th>suma_total_2018_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.078741</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.084338</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.068945</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.111625</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0     0     0     4     22        10001                           0.0   \n",
       "1     0     0     4     22        10001                           0.0   \n",
       "2     0     0     4     22        10001                           0.0   \n",
       "3     0     0     4     22        10001                           0.0   \n",
       "4     0     0     4     22        10001                           0.0   \n",
       "\n",
       "   variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019       tn  \\\n",
       "0                      -0.60803          0.87535          3.28124  0.87535   \n",
       "1                      -0.60803          0.87535          3.28124  0.30925   \n",
       "2                      -0.60803          0.87535          3.28124  0.26732   \n",
       "3                      -0.60803          0.87535          3.28124  0.38264   \n",
       "4                      -0.60803          0.87535          3.28124  0.06290   \n",
       "\n",
       "     periodo  cluster  tn_scaled  suma_total_2019_scaled  \\\n",
       "0 2018-12-01        0  -0.003176               -0.093435   \n",
       "1 2019-01-01        0  -0.078741               -0.093435   \n",
       "2 2019-02-01        0  -0.084338               -0.093435   \n",
       "3 2019-03-01        0  -0.068945               -0.093435   \n",
       "4 2019-04-01        0  -0.111625               -0.093435   \n",
       "\n",
       "   suma_total_2018_scaled  \n",
       "0                0.023427  \n",
       "1                0.023427  \n",
       "2                0.023427  \n",
       "3                0.023427  \n",
       "4                0.023427  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver de dejar solo la fecha y tn y cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          periodo  tn_scaled  suma_total_2019_scaled  suma_total_2018_scaled  \\\n",
      "0      2018-12-01  -0.003176               -0.093435                0.023427   \n",
      "1      2019-01-01  -0.078741               -0.093435                0.023427   \n",
      "2      2019-02-01  -0.084338               -0.093435                0.023427   \n",
      "3      2019-03-01  -0.068945               -0.093435                0.023427   \n",
      "4      2019-04-01  -0.111625               -0.093435                0.023427   \n",
      "...           ...        ...                     ...                     ...   \n",
      "434443 2019-08-01  -0.120021               -0.134593               -0.107733   \n",
      "434444 2019-09-01  -0.120021               -0.134593               -0.107733   \n",
      "434445 2019-10-01  -0.120021               -0.134593               -0.107733   \n",
      "434446 2019-11-01  -0.120021               -0.134593               -0.107733   \n",
      "434447 2019-12-01  -0.120021               -0.134593               -0.107733   \n",
      "\n",
      "        cluster  variacion_feb2018_vs_dic2017  variacion_feb2019_vs_dic2018  \n",
      "0             0                           0.0                      -0.60803  \n",
      "1             0                           0.0                      -0.60803  \n",
      "2             0                           0.0                      -0.60803  \n",
      "3             0                           0.0                      -0.60803  \n",
      "4             0                           0.0                      -0.60803  \n",
      "...         ...                           ...                           ...  \n",
      "434443       12                           0.0                       0.00000  \n",
      "434444       12                           0.0                       0.00000  \n",
      "434445       12                           0.0                       0.00000  \n",
      "434446       12                           0.0                       0.00000  \n",
      "434447       12                           0.0                       0.00000  \n",
      "\n",
      "[434448 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar solo las columnas 'periodo', 'tn' y 'cluster'\n",
    "time_series_dff = grouped_dff[['periodo', 'tn_scaled', 'suma_total_2019_scaled','suma_total_2018_scaled','cluster','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']]\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(time_series_dff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO DEJE EPOCHS EN 100 para version 2001 paciencia 20, \n",
    "version 20011: 200 epochs, paciencia 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir dimensiones de X_padded y y antes de entrenar\n",
    "print(f'Dimensiones de X_padded: {X_padded.shape}')\n",
    "print(f'Dimensiones de y: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando cluster numero: 0\n",
      "Epoch 1/100\n",
      "10/10 - 9s - 889ms/step - loss: 7.1365 - val_loss: 6.7846 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "10/10 - 3s - 323ms/step - loss: 6.8962 - val_loss: 7.0124 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "10/10 - 3s - 303ms/step - loss: 6.7143 - val_loss: 6.5734 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "10/10 - 3s - 303ms/step - loss: 6.5323 - val_loss: 6.2919 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "10/10 - 3s - 300ms/step - loss: 6.3984 - val_loss: 6.3032 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "10/10 - 3s - 302ms/step - loss: 6.1968 - val_loss: 6.2002 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "10/10 - 3s - 304ms/step - loss: 6.0634 - val_loss: 5.9017 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "10/10 - 3s - 293ms/step - loss: 5.8915 - val_loss: 5.8784 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "10/10 - 3s - 304ms/step - loss: 5.7615 - val_loss: 5.5661 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "10/10 - 3s - 287ms/step - loss: 5.6111 - val_loss: 5.7368 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "10/10 - 3s - 293ms/step - loss: 5.4885 - val_loss: 5.5678 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "10/10 - 3s - 301ms/step - loss: 5.3527 - val_loss: 5.3258 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "10/10 - 3s - 296ms/step - loss: 5.2526 - val_loss: 4.9540 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "10/10 - 3s - 290ms/step - loss: 5.1376 - val_loss: 5.1615 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "10/10 - 3s - 298ms/step - loss: 4.9834 - val_loss: 5.3134 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "10/10 - 3s - 296ms/step - loss: 4.9148 - val_loss: 4.9947 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "10/10 - 3s - 282ms/step - loss: 4.7832 - val_loss: 4.6347 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "10/10 - 3s - 293ms/step - loss: 4.6800 - val_loss: 4.5365 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "10/10 - 3s - 294ms/step - loss: 4.5867 - val_loss: 4.5422 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "10/10 - 3s - 288ms/step - loss: 4.4705 - val_loss: 4.5600 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "10/10 - 3s - 286ms/step - loss: 4.3908 - val_loss: 4.3896 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "10/10 - 3s - 297ms/step - loss: 4.2959 - val_loss: 4.2162 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "10/10 - 3s - 297ms/step - loss: 4.2156 - val_loss: 4.2898 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "10/10 - 3s - 300ms/step - loss: 4.1396 - val_loss: 4.1272 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "10/10 - 3s - 303ms/step - loss: 4.0390 - val_loss: 4.1128 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "10/10 - 3s - 287ms/step - loss: 3.9502 - val_loss: 3.9467 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "10/10 - 3s - 274ms/step - loss: 3.8765 - val_loss: 3.9024 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "10/10 - 3s - 306ms/step - loss: 3.7995 - val_loss: 3.8288 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "10/10 - 3s - 298ms/step - loss: 3.7545 - val_loss: 3.9804 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "10/10 - 3s - 312ms/step - loss: 3.6752 - val_loss: 3.7433 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "10/10 - 3s - 287ms/step - loss: 3.5878 - val_loss: 3.5222 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "10/10 - 3s - 290ms/step - loss: 3.5460 - val_loss: 3.6983 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "10/10 - 3s - 290ms/step - loss: 3.4712 - val_loss: 3.5180 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "10/10 - 3s - 299ms/step - loss: 3.4083 - val_loss: 3.3138 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "10/10 - 3s - 284ms/step - loss: 3.3671 - val_loss: 3.2839 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "10/10 - 3s - 297ms/step - loss: 3.3111 - val_loss: 3.2930 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "10/10 - 3s - 299ms/step - loss: 3.2414 - val_loss: 3.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "10/10 - 3s - 282ms/step - loss: 3.1933 - val_loss: 3.4636 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "10/10 - 3s - 300ms/step - loss: 3.1232 - val_loss: 3.1214 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "10/10 - 3s - 285ms/step - loss: 3.1003 - val_loss: 3.1028 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "10/10 - 3s - 299ms/step - loss: 3.0524 - val_loss: 3.1653 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "10/10 - 3s - 303ms/step - loss: 2.9923 - val_loss: 3.0611 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "10/10 - 3s - 285ms/step - loss: 2.9505 - val_loss: 3.0264 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "10/10 - 3s - 294ms/step - loss: 2.9005 - val_loss: 3.1072 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "10/10 - 3s - 281ms/step - loss: 2.8562 - val_loss: 3.0288 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "10/10 - 3s - 304ms/step - loss: 2.8142 - val_loss: 2.8750 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "10/10 - 3s - 286ms/step - loss: 2.7706 - val_loss: 2.7453 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "10/10 - 3s - 275ms/step - loss: 2.7455 - val_loss: 2.8215 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "10/10 - 3s - 291ms/step - loss: 2.6999 - val_loss: 2.8728 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "10/10 - 3s - 291ms/step - loss: 2.6658 - val_loss: 2.9053 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "10/10 - 3s - 296ms/step - loss: 2.6348 - val_loss: 2.8810 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "10/10 - 3s - 279ms/step - loss: 2.6004 - val_loss: 2.7002 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "10/10 - 3s - 300ms/step - loss: 2.5672 - val_loss: 2.6554 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "10/10 - 3s - 297ms/step - loss: 2.5468 - val_loss: 2.8543 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "10/10 - 3s - 290ms/step - loss: 2.5207 - val_loss: 2.7041 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "10/10 - 3s - 308ms/step - loss: 2.4857 - val_loss: 2.6539 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "10/10 - 3s - 287ms/step - loss: 2.4579 - val_loss: 2.7323 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "10/10 - 3s - 300ms/step - loss: 2.4313 - val_loss: 2.6380 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "10/10 - 3s - 304ms/step - loss: 2.3969 - val_loss: 2.6325 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "10/10 - 3s - 292ms/step - loss: 2.3712 - val_loss: 2.5826 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "10/10 - 3s - 284ms/step - loss: 2.3521 - val_loss: 2.3463 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "10/10 - 3s - 289ms/step - loss: 2.3358 - val_loss: 2.4100 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "10/10 - 3s - 288ms/step - loss: 2.2922 - val_loss: 2.6055 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "10/10 - 3s - 277ms/step - loss: 2.2911 - val_loss: 2.6685 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "10/10 - 3s - 292ms/step - loss: 2.2602 - val_loss: 2.3848 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "10/10 - 3s - 304ms/step - loss: 2.2467 - val_loss: 2.3223 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "10/10 - 3s - 295ms/step - loss: 2.2156 - val_loss: 2.3910 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "10/10 - 3s - 288ms/step - loss: 2.1962 - val_loss: 2.4549 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "10/10 - 3s - 278ms/step - loss: 2.1761 - val_loss: 2.5156 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "10/10 - 3s - 294ms/step - loss: 2.1480 - val_loss: 2.5417 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "10/10 - 3s - 295ms/step - loss: 2.1256 - val_loss: 2.2742 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "10/10 - 3s - 293ms/step - loss: 2.1113 - val_loss: 2.1589 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "10/10 - 3s - 297ms/step - loss: 2.0983 - val_loss: 2.2095 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "10/10 - 3s - 286ms/step - loss: 2.0872 - val_loss: 2.5084 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "10/10 - 3s - 300ms/step - loss: 2.0833 - val_loss: 2.5543 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "10/10 - 3s - 297ms/step - loss: 2.0402 - val_loss: 2.1425 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "10/10 - 3s - 293ms/step - loss: 2.0534 - val_loss: 2.1010 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "10/10 - 3s - 298ms/step - loss: 2.0263 - val_loss: 2.4242 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "10/10 - 3s - 284ms/step - loss: 2.0154 - val_loss: 2.3690 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "10/10 - 3s - 299ms/step - loss: 1.9932 - val_loss: 2.2854 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "10/10 - 3s - 311ms/step - loss: 1.9710 - val_loss: 2.2272 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "10/10 - 3s - 273ms/step - loss: 1.9567 - val_loss: 2.1828 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "10/10 - 3s - 293ms/step - loss: 1.9451 - val_loss: 2.1539 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "10/10 - 3s - 271ms/step - loss: 1.9323 - val_loss: 2.0205 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "10/10 - 3s - 269ms/step - loss: 1.9305 - val_loss: 2.0200 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "10/10 - 3s - 298ms/step - loss: 1.9146 - val_loss: 2.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "10/10 - 3s - 295ms/step - loss: 1.8967 - val_loss: 2.2270 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "10/10 - 3s - 295ms/step - loss: 1.8928 - val_loss: 2.2459 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "10/10 - 3s - 294ms/step - loss: 1.8838 - val_loss: 2.1985 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "10/10 - 3s - 290ms/step - loss: 1.8669 - val_loss: 1.9471 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "10/10 - 3s - 282ms/step - loss: 1.8680 - val_loss: 2.0329 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "10/10 - 3s - 290ms/step - loss: 1.8464 - val_loss: 2.1036 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "10/10 - 3s - 286ms/step - loss: 1.8478 - val_loss: 2.0985 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "10/10 - 3s - 290ms/step - loss: 1.8115 - val_loss: 2.1518 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "10/10 - 3s - 296ms/step - loss: 1.8129 - val_loss: 2.0645 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "10/10 - 3s - 297ms/step - loss: 1.8067 - val_loss: 2.0070 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "10/10 - 3s - 278ms/step - loss: 1.7913 - val_loss: 1.9101 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "10/10 - 3s - 282ms/step - loss: 1.7898 - val_loss: 1.9917 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "10/10 - 3s - 291ms/step - loss: 1.7779 - val_loss: 2.2038 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "10/10 - 3s - 300ms/step - loss: 1.7763 - val_loss: 2.1485 - learning_rate: 1.0000e-04\n",
      "Entrenando cluster numero: 1\n",
      "Epoch 1/100\n",
      "22/22 - 13s - 594ms/step - loss: 7.5267 - val_loss: 6.0582 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "22/22 - 9s - 387ms/step - loss: 6.5812 - val_loss: 6.1682 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "22/22 - 8s - 373ms/step - loss: 6.1960 - val_loss: 5.6949 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "22/22 - 8s - 368ms/step - loss: 5.8688 - val_loss: 5.4734 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "22/22 - 8s - 376ms/step - loss: 5.5485 - val_loss: 5.1098 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "22/22 - 8s - 368ms/step - loss: 5.2482 - val_loss: 4.8264 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "22/22 - 8s - 370ms/step - loss: 4.9766 - val_loss: 4.6271 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "22/22 - 8s - 366ms/step - loss: 4.7351 - val_loss: 4.4235 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "22/22 - 8s - 361ms/step - loss: 4.4634 - val_loss: 4.2818 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "22/22 - 8s - 350ms/step - loss: 4.2670 - val_loss: 3.9451 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "22/22 - 7s - 339ms/step - loss: 4.0706 - val_loss: 3.9587 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "22/22 - 8s - 344ms/step - loss: 3.8815 - val_loss: 3.6186 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "22/22 - 8s - 348ms/step - loss: 3.7000 - val_loss: 3.5926 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "22/22 - 8s - 344ms/step - loss: 3.5392 - val_loss: 3.3659 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "22/22 - 8s - 346ms/step - loss: 3.3838 - val_loss: 3.2283 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "22/22 - 8s - 342ms/step - loss: 3.2474 - val_loss: 3.1992 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "22/22 - 7s - 329ms/step - loss: 3.0967 - val_loss: 3.0395 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "22/22 - 7s - 331ms/step - loss: 2.9776 - val_loss: 2.8174 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "22/22 - 7s - 326ms/step - loss: 2.8653 - val_loss: 2.7425 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "22/22 - 8s - 359ms/step - loss: 2.7679 - val_loss: 2.6339 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "22/22 - 7s - 328ms/step - loss: 2.6627 - val_loss: 2.7077 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "22/22 - 7s - 341ms/step - loss: 2.5738 - val_loss: 2.6623 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "22/22 - 7s - 333ms/step - loss: 2.4880 - val_loss: 2.5489 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "22/22 - 7s - 334ms/step - loss: 2.4015 - val_loss: 2.6928 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "22/22 - 7s - 331ms/step - loss: 2.3405 - val_loss: 2.5848 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "22/22 - 7s - 328ms/step - loss: 2.2564 - val_loss: 2.3677 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "22/22 - 8s - 346ms/step - loss: 2.1876 - val_loss: 2.2798 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "22/22 - 8s - 342ms/step - loss: 2.1348 - val_loss: 2.0617 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "22/22 - 7s - 328ms/step - loss: 2.0805 - val_loss: 2.3627 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "22/22 - 7s - 334ms/step - loss: 2.0101 - val_loss: 2.0112 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "22/22 - 8s - 348ms/step - loss: 1.9706 - val_loss: 2.1855 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "22/22 - 8s - 347ms/step - loss: 1.9216 - val_loss: 2.0344 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "22/22 - 8s - 352ms/step - loss: 1.8718 - val_loss: 2.0892 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "22/22 - 7s - 331ms/step - loss: 1.8394 - val_loss: 1.5881 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "22/22 - 8s - 343ms/step - loss: 1.8228 - val_loss: 1.9526 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "22/22 - 7s - 326ms/step - loss: 1.7480 - val_loss: 2.1357 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "22/22 - 7s - 332ms/step - loss: 1.7145 - val_loss: 1.7708 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "22/22 - 7s - 333ms/step - loss: 1.6716 - val_loss: 1.9961 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "22/22 - 7s - 333ms/step - loss: 1.6393 - val_loss: 2.4917 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.6488 - val_loss: 2.1730 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "22/22 - 7s - 336ms/step - loss: 1.6111 - val_loss: 1.8647 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "22/22 - 8s - 348ms/step - loss: 1.5653 - val_loss: 1.6899 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.5318 - val_loss: 1.6557 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "22/22 - 7s - 325ms/step - loss: 1.5176 - val_loss: 1.8920 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "22/22 - 7s - 332ms/step - loss: 1.4786 - val_loss: 1.8460 - learning_rate: 2.0000e-05\n",
      "Epoch 46/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.4679 - val_loss: 1.7663 - learning_rate: 2.0000e-05\n",
      "Epoch 47/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.4709 - val_loss: 1.9759 - learning_rate: 2.0000e-05\n",
      "Epoch 48/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.4636 - val_loss: 1.9239 - learning_rate: 2.0000e-05\n",
      "Epoch 49/100\n",
      "22/22 - 7s - 332ms/step - loss: 1.4621 - val_loss: 1.7830 - learning_rate: 2.0000e-05\n",
      "Epoch 50/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.4422 - val_loss: 1.7332 - learning_rate: 2.0000e-05\n",
      "Epoch 51/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.4537 - val_loss: 1.8989 - learning_rate: 2.0000e-05\n",
      "Epoch 52/100\n",
      "22/22 - 7s - 330ms/step - loss: 1.4506 - val_loss: 1.9308 - learning_rate: 2.0000e-05\n",
      "Epoch 53/100\n",
      "22/22 - 7s - 328ms/step - loss: 1.4433 - val_loss: 1.7361 - learning_rate: 2.0000e-05\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "22/22 - 7s - 339ms/step - loss: 1.4358 - val_loss: 2.0854 - learning_rate: 2.0000e-05\n",
      "Epoch 55/100\n",
      "22/22 - 7s - 320ms/step - loss: 1.4363 - val_loss: 1.7357 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.4271 - val_loss: 1.7426 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.4242 - val_loss: 1.7300 - learning_rate: 1.0000e-05\n",
      "Epoch 58/100\n",
      "22/22 - 7s - 341ms/step - loss: 1.4197 - val_loss: 1.8488 - learning_rate: 1.0000e-05\n",
      "Epoch 59/100\n",
      "22/22 - 7s - 338ms/step - loss: 1.4122 - val_loss: 1.8073 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "22/22 - 7s - 324ms/step - loss: 1.4171 - val_loss: 1.7993 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "22/22 - 7s - 332ms/step - loss: 1.4146 - val_loss: 1.8891 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "22/22 - 7s - 333ms/step - loss: 1.4015 - val_loss: 1.8504 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "22/22 - 7s - 335ms/step - loss: 1.4096 - val_loss: 1.9608 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "22/22 - 7s - 331ms/step - loss: 1.4046 - val_loss: 1.9928 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "22/22 - 7s - 316ms/step - loss: 1.4121 - val_loss: 1.8257 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "22/22 - 7s - 330ms/step - loss: 1.3992 - val_loss: 1.7305 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.4030 - val_loss: 1.7059 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "22/22 - 8s - 346ms/step - loss: 1.3931 - val_loss: 1.7949 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.3993 - val_loss: 1.8144 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "22/22 - 7s - 327ms/step - loss: 1.3834 - val_loss: 1.8185 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "22/22 - 7s - 336ms/step - loss: 1.3962 - val_loss: 1.8976 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.3906 - val_loss: 1.7725 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "22/22 - 7s - 327ms/step - loss: 1.3900 - val_loss: 1.9406 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "22/22 - 7s - 335ms/step - loss: 1.3871 - val_loss: 1.7785 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "22/22 - 7s - 334ms/step - loss: 1.3688 - val_loss: 1.7449 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.3717 - val_loss: 1.8056 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "22/22 - 7s - 330ms/step - loss: 1.3752 - val_loss: 1.8736 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.3701 - val_loss: 1.8840 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "22/22 - 7s - 340ms/step - loss: 1.3643 - val_loss: 1.7711 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "22/22 - 7s - 338ms/step - loss: 1.3755 - val_loss: 1.7850 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "22/22 - 7s - 341ms/step - loss: 1.3704 - val_loss: 1.8713 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "22/22 - 7s - 333ms/step - loss: 1.3709 - val_loss: 1.9679 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "22/22 - 8s - 345ms/step - loss: 1.3705 - val_loss: 1.8244 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "22/22 - 7s - 334ms/step - loss: 1.3541 - val_loss: 1.7733 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "22/22 - 7s - 328ms/step - loss: 1.3532 - val_loss: 1.6984 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.3506 - val_loss: 1.7159 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "22/22 - 7s - 338ms/step - loss: 1.3374 - val_loss: 1.7037 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "22/22 - 7s - 338ms/step - loss: 1.3471 - val_loss: 1.7438 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.3422 - val_loss: 1.7264 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "22/22 - 7s - 326ms/step - loss: 1.3405 - val_loss: 1.8254 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "22/22 - 8s - 342ms/step - loss: 1.3421 - val_loss: 1.8113 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "22/22 - 7s - 339ms/step - loss: 1.3409 - val_loss: 1.7872 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.3413 - val_loss: 1.8023 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "22/22 - 7s - 329ms/step - loss: 1.3250 - val_loss: 1.6233 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "22/22 - 7s - 336ms/step - loss: 1.3349 - val_loss: 1.7217 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "22/22 - 7s - 335ms/step - loss: 1.3264 - val_loss: 1.7565 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "22/22 - 7s - 325ms/step - loss: 1.3158 - val_loss: 1.7269 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "22/22 - 7s - 333ms/step - loss: 1.3228 - val_loss: 1.7634 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "22/22 - 8s - 343ms/step - loss: 1.3236 - val_loss: 1.9357 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "22/22 - 7s - 337ms/step - loss: 1.3194 - val_loss: 1.7167 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 2\n",
      "Epoch 1/100\n",
      "19/19 - 6s - 332ms/step - loss: 7.4672 - val_loss: 6.0595 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "19/19 - 2s - 88ms/step - loss: 6.6402 - val_loss: 6.1001 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "19/19 - 2s - 84ms/step - loss: 6.2617 - val_loss: 5.7416 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "19/19 - 1s - 71ms/step - loss: 5.9330 - val_loss: 5.4349 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "19/19 - 2s - 80ms/step - loss: 5.6565 - val_loss: 5.2892 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "19/19 - 2s - 88ms/step - loss: 5.3557 - val_loss: 4.9675 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "19/19 - 1s - 76ms/step - loss: 5.1161 - val_loss: 4.7993 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "19/19 - 2s - 79ms/step - loss: 4.8748 - val_loss: 4.5464 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "19/19 - 1s - 76ms/step - loss: 4.6745 - val_loss: 4.3130 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "19/19 - 1s - 78ms/step - loss: 4.4513 - val_loss: 4.2651 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "19/19 - 1s - 78ms/step - loss: 4.2443 - val_loss: 4.1754 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "19/19 - 2s - 80ms/step - loss: 4.0660 - val_loss: 3.9887 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "19/19 - 2s - 79ms/step - loss: 3.8911 - val_loss: 3.7378 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "19/19 - 2s - 84ms/step - loss: 3.7322 - val_loss: 3.7692 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "19/19 - 2s - 82ms/step - loss: 3.5790 - val_loss: 3.4368 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "19/19 - 1s - 71ms/step - loss: 3.4296 - val_loss: 3.3371 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "19/19 - 2s - 82ms/step - loss: 3.2918 - val_loss: 3.2299 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "19/19 - 1s - 73ms/step - loss: 3.1675 - val_loss: 3.1154 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "19/19 - 2s - 82ms/step - loss: 3.0412 - val_loss: 3.0976 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "19/19 - 1s - 73ms/step - loss: 2.9372 - val_loss: 2.9687 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "19/19 - 1s - 75ms/step - loss: 2.8335 - val_loss: 2.9075 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "19/19 - 2s - 79ms/step - loss: 2.7245 - val_loss: 2.7578 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "19/19 - 2s - 82ms/step - loss: 2.6550 - val_loss: 2.7877 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "19/19 - 1s - 77ms/step - loss: 2.5485 - val_loss: 2.6448 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "19/19 - 2s - 85ms/step - loss: 2.4824 - val_loss: 2.7229 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "19/19 - 1s - 79ms/step - loss: 2.3970 - val_loss: 2.4564 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "19/19 - 2s - 82ms/step - loss: 2.3286 - val_loss: 2.6146 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "19/19 - 1s - 72ms/step - loss: 2.2636 - val_loss: 2.5430 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "19/19 - 1s - 72ms/step - loss: 2.2071 - val_loss: 2.3008 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "19/19 - 1s - 75ms/step - loss: 2.1296 - val_loss: 2.2694 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "19/19 - 1s - 76ms/step - loss: 2.0686 - val_loss: 2.1879 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "19/19 - 1s - 73ms/step - loss: 2.0364 - val_loss: 2.1350 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "19/19 - 1s - 76ms/step - loss: 1.9682 - val_loss: 2.1617 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.9130 - val_loss: 2.0712 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.8739 - val_loss: 1.9684 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.8343 - val_loss: 1.9524 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "19/19 - 1s - 76ms/step - loss: 1.7996 - val_loss: 1.9361 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "19/19 - 1s - 74ms/step - loss: 1.7549 - val_loss: 1.8674 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "19/19 - 1s - 76ms/step - loss: 1.7248 - val_loss: 1.9138 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "19/19 - 1s - 73ms/step - loss: 1.6921 - val_loss: 2.0199 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "19/19 - 1s - 74ms/step - loss: 1.6572 - val_loss: 2.0047 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "19/19 - 1s - 73ms/step - loss: 1.6222 - val_loss: 1.9273 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.5984 - val_loss: 1.7201 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "19/19 - 1s - 63ms/step - loss: 1.5731 - val_loss: 1.6612 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "19/19 - 1s - 68ms/step - loss: 1.5314 - val_loss: 1.5701 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "19/19 - 1s - 71ms/step - loss: 1.5178 - val_loss: 1.6362 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.4933 - val_loss: 1.6748 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.4660 - val_loss: 1.7328 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "19/19 - 1s - 71ms/step - loss: 1.4392 - val_loss: 1.7319 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "19/19 - 1s - 75ms/step - loss: 1.4301 - val_loss: 1.8484 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "19/19 - 1s - 72ms/step - loss: 1.4186 - val_loss: 1.6192 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "19/19 - 1s - 71ms/step - loss: 1.3770 - val_loss: 1.6573 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "19/19 - 1s - 75ms/step - loss: 1.3570 - val_loss: 1.6551 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "19/19 - 1s - 74ms/step - loss: 1.3423 - val_loss: 1.7394 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "19/19 - 2s - 84ms/step - loss: 1.3448 - val_loss: 1.4426 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "19/19 - 1s - 73ms/step - loss: 1.3171 - val_loss: 1.5390 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.2969 - val_loss: 1.6562 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "19/19 - 2s - 80ms/step - loss: 1.2737 - val_loss: 1.6368 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "19/19 - 1s - 77ms/step - loss: 1.2664 - val_loss: 1.5051 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "19/19 - 1s - 75ms/step - loss: 1.2438 - val_loss: 1.5569 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "19/19 - 1s - 63ms/step - loss: 1.2432 - val_loss: 1.5066 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "19/19 - 1s - 67ms/step - loss: 1.2294 - val_loss: 1.5890 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "19/19 - 1s - 77ms/step - loss: 1.2061 - val_loss: 1.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "19/19 - 1s - 75ms/step - loss: 1.1918 - val_loss: 1.5340 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "19/19 - 1s - 64ms/step - loss: 1.1684 - val_loss: 1.6011 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "19/19 - 1s - 76ms/step - loss: 1.1644 - val_loss: 1.6444 - learning_rate: 2.0000e-05\n",
      "Epoch 67/100\n",
      "19/19 - 1s - 69ms/step - loss: 1.1604 - val_loss: 1.5217 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "19/19 - 1s - 65ms/step - loss: 1.1564 - val_loss: 1.5012 - learning_rate: 2.0000e-05\n",
      "Epoch 69/100\n",
      "19/19 - 1s - 73ms/step - loss: 1.1582 - val_loss: 1.4459 - learning_rate: 2.0000e-05\n",
      "Epoch 70/100\n",
      "19/19 - 1s - 74ms/step - loss: 1.1566 - val_loss: 1.4619 - learning_rate: 2.0000e-05\n",
      "Epoch 71/100\n",
      "19/19 - 1s - 66ms/step - loss: 1.1481 - val_loss: 1.4868 - learning_rate: 2.0000e-05\n",
      "Epoch 72/100\n",
      "19/19 - 1s - 56ms/step - loss: 1.1524 - val_loss: 1.6154 - learning_rate: 2.0000e-05\n",
      "Epoch 73/100\n",
      "19/19 - 2s - 84ms/step - loss: 1.1508 - val_loss: 1.5266 - learning_rate: 2.0000e-05\n",
      "Epoch 74/100\n",
      "19/19 - 1s - 69ms/step - loss: 1.1518 - val_loss: 1.5456 - learning_rate: 2.0000e-05\n",
      "Epoch 75/100\n",
      "19/19 - 1s - 69ms/step - loss: 1.1395 - val_loss: 1.3796 - learning_rate: 2.0000e-05\n",
      "Epoch 76/100\n",
      "19/19 - 1s - 70ms/step - loss: 1.1368 - val_loss: 1.5150 - learning_rate: 2.0000e-05\n",
      "Epoch 77/100\n",
      "19/19 - 1s - 62ms/step - loss: 1.1349 - val_loss: 1.5511 - learning_rate: 2.0000e-05\n",
      "Epoch 78/100\n",
      "19/19 - 1s - 78ms/step - loss: 1.1338 - val_loss: 1.4196 - learning_rate: 2.0000e-05\n",
      "Epoch 79/100\n",
      "19/19 - 1s - 63ms/step - loss: 1.1312 - val_loss: 1.4392 - learning_rate: 2.0000e-05\n",
      "Epoch 80/100\n",
      "19/19 - 1s - 63ms/step - loss: 1.1253 - val_loss: 1.4007 - learning_rate: 2.0000e-05\n",
      "Epoch 81/100\n",
      "19/19 - 1s - 67ms/step - loss: 1.1234 - val_loss: 1.5487 - learning_rate: 2.0000e-05\n",
      "Epoch 82/100\n",
      "19/19 - 1s - 54ms/step - loss: 1.1161 - val_loss: 1.5648 - learning_rate: 2.0000e-05\n",
      "Epoch 83/100\n",
      "19/19 - 1s - 72ms/step - loss: 1.1236 - val_loss: 1.5326 - learning_rate: 2.0000e-05\n",
      "Epoch 84/100\n",
      "19/19 - 1s - 60ms/step - loss: 1.1156 - val_loss: 1.4749 - learning_rate: 2.0000e-05\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 68ms/step - loss: 1.1158 - val_loss: 1.5305 - learning_rate: 2.0000e-05\n",
      "Epoch 86/100\n",
      "19/19 - 1s - 61ms/step - loss: 1.1161 - val_loss: 1.4271 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "19/19 - 1s - 72ms/step - loss: 1.1184 - val_loss: 1.4836 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "19/19 - 1s - 64ms/step - loss: 1.1060 - val_loss: 1.4585 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "19/19 - 1s - 62ms/step - loss: 1.1063 - val_loss: 1.4304 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "19/19 - 1s - 59ms/step - loss: 1.1034 - val_loss: 1.4340 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0965 - val_loss: 1.5115 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1031 - val_loss: 1.4692 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1038 - val_loss: 1.4415 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0996 - val_loss: 1.4883 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "19/19 - 1s - 49ms/step - loss: 1.0941 - val_loss: 1.4329 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0961 - val_loss: 1.4299 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0978 - val_loss: 1.4466 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0967 - val_loss: 1.4597 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0967 - val_loss: 1.4463 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0950 - val_loss: 1.4795 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 3\n",
      "Epoch 1/100\n",
      "19/19 - 6s - 295ms/step - loss: 7.2574 - val_loss: 6.4204 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "19/19 - 1s - 49ms/step - loss: 6.6817 - val_loss: 6.4828 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "19/19 - 1s - 47ms/step - loss: 6.2956 - val_loss: 6.1037 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "19/19 - 1s - 47ms/step - loss: 5.9794 - val_loss: 5.8290 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "19/19 - 1s - 45ms/step - loss: 5.6836 - val_loss: 5.5656 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "19/19 - 1s - 47ms/step - loss: 5.4176 - val_loss: 5.3705 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "19/19 - 1s - 47ms/step - loss: 5.1547 - val_loss: 5.0440 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.8983 - val_loss: 4.8256 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.6849 - val_loss: 4.7306 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.4598 - val_loss: 4.4673 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.2831 - val_loss: 4.2954 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.0970 - val_loss: 4.0672 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.9272 - val_loss: 4.0441 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.7563 - val_loss: 3.9495 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.6088 - val_loss: 3.6133 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.4535 - val_loss: 3.5892 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.3246 - val_loss: 3.4807 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.1994 - val_loss: 3.5488 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.0940 - val_loss: 3.2570 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.9646 - val_loss: 3.1737 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.8706 - val_loss: 2.9567 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.7544 - val_loss: 2.8970 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "19/19 - 1s - 48ms/step - loss: 2.6829 - val_loss: 3.0347 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.6000 - val_loss: 2.7238 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.5143 - val_loss: 2.7692 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.4376 - val_loss: 2.8101 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.3557 - val_loss: 2.9502 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.3021 - val_loss: 2.4244 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.2280 - val_loss: 2.2091 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.1598 - val_loss: 2.0144 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.1023 - val_loss: 2.4725 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.0467 - val_loss: 2.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.9861 - val_loss: 2.0250 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.9345 - val_loss: 2.4654 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.8894 - val_loss: 2.1754 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.8491 - val_loss: 2.3225 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.8196 - val_loss: 2.3173 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.7830 - val_loss: 1.9194 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.7359 - val_loss: 2.2589 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "19/19 - 1s - 51ms/step - loss: 1.7065 - val_loss: 2.2051 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.6574 - val_loss: 2.1246 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.6281 - val_loss: 2.0907 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.5978 - val_loss: 1.9535 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.5620 - val_loss: 1.9433 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.5475 - val_loss: 2.0501 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.5281 - val_loss: 2.1171 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.4824 - val_loss: 1.4521 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.4738 - val_loss: 1.7162 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.4466 - val_loss: 2.1260 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "19/19 - 1s - 51ms/step - loss: 1.4186 - val_loss: 1.6605 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "19/19 - 1s - 50ms/step - loss: 1.3971 - val_loss: 1.8050 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "19/19 - 1s - 52ms/step - loss: 1.3821 - val_loss: 1.6623 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "19/19 - 1s - 54ms/step - loss: 1.3533 - val_loss: 1.7177 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.3528 - val_loss: 1.8669 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.3738 - val_loss: 1.7400 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.3324 - val_loss: 1.4694 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "19/19 - 1s - 50ms/step - loss: 1.3245 - val_loss: 1.6318 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "19/19 - 1s - 49ms/step - loss: 1.2671 - val_loss: 1.6687 - learning_rate: 2.0000e-05\n",
      "Epoch 59/100\n",
      "19/19 - 1s - 52ms/step - loss: 1.2632 - val_loss: 1.5402 - learning_rate: 2.0000e-05\n",
      "Epoch 60/100\n",
      "19/19 - 1s - 50ms/step - loss: 1.2795 - val_loss: 1.7701 - learning_rate: 2.0000e-05\n",
      "Epoch 61/100\n",
      "19/19 - 1s - 49ms/step - loss: 1.2573 - val_loss: 1.7595 - learning_rate: 2.0000e-05\n",
      "Epoch 62/100\n",
      "19/19 - 1s - 50ms/step - loss: 1.2500 - val_loss: 1.8473 - learning_rate: 2.0000e-05\n",
      "Epoch 63/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.2551 - val_loss: 1.7593 - learning_rate: 2.0000e-05\n",
      "Epoch 64/100\n",
      "19/19 - 1s - 53ms/step - loss: 1.2545 - val_loss: 1.6575 - learning_rate: 2.0000e-05\n",
      "Epoch 65/100\n",
      "19/19 - 1s - 51ms/step - loss: 1.2371 - val_loss: 1.8369 - learning_rate: 2.0000e-05\n",
      "Epoch 66/100\n",
      "19/19 - 1s - 52ms/step - loss: 1.2426 - val_loss: 1.6910 - learning_rate: 2.0000e-05\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 46ms/step - loss: 1.2426 - val_loss: 1.6515 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2354 - val_loss: 1.7588 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2260 - val_loss: 1.5549 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2259 - val_loss: 1.7911 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2187 - val_loss: 1.8185 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2204 - val_loss: 1.6499 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2285 - val_loss: 1.7981 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2272 - val_loss: 1.8000 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2249 - val_loss: 1.6793 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2280 - val_loss: 1.5809 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2170 - val_loss: 1.6850 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2153 - val_loss: 1.7011 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2115 - val_loss: 1.9247 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2140 - val_loss: 1.6379 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2153 - val_loss: 1.5913 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2023 - val_loss: 1.6611 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "19/19 - 1s - 45ms/step - loss: 1.2097 - val_loss: 1.6926 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1992 - val_loss: 1.5444 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.1979 - val_loss: 1.6050 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1981 - val_loss: 1.4918 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2057 - val_loss: 1.5532 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1959 - val_loss: 1.7372 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1869 - val_loss: 1.7833 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1799 - val_loss: 1.7444 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1945 - val_loss: 1.5513 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "19/19 - 1s - 50ms/step - loss: 1.1945 - val_loss: 1.4671 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1939 - val_loss: 1.5005 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1726 - val_loss: 1.6219 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1875 - val_loss: 1.8303 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1658 - val_loss: 1.7122 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.1844 - val_loss: 1.7962 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "19/19 - 1s - 45ms/step - loss: 1.1750 - val_loss: 1.6889 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1755 - val_loss: 1.6774 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1772 - val_loss: 1.7217 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 4\n",
      "Epoch 1/100\n",
      "11/11 - 5s - 471ms/step - loss: 7.4194 - val_loss: 6.3156 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "11/11 - 1s - 52ms/step - loss: 6.9044 - val_loss: 6.2473 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "11/11 - 1s - 46ms/step - loss: 6.6536 - val_loss: 6.4123 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "11/11 - 1s - 48ms/step - loss: 6.4737 - val_loss: 5.9827 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "11/11 - 1s - 47ms/step - loss: 6.2424 - val_loss: 5.8424 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "11/11 - 1s - 46ms/step - loss: 6.0927 - val_loss: 5.7532 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "11/11 - 1s - 46ms/step - loss: 5.9009 - val_loss: 5.5694 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "11/11 - 0s - 45ms/step - loss: 5.7522 - val_loss: 5.4262 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "11/11 - 1s - 49ms/step - loss: 5.6186 - val_loss: 5.2529 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "11/11 - 1s - 46ms/step - loss: 5.4423 - val_loss: 5.1920 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "11/11 - 1s - 46ms/step - loss: 5.2986 - val_loss: 5.0324 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "11/11 - 1s - 46ms/step - loss: 5.1777 - val_loss: 4.7943 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "11/11 - 1s - 46ms/step - loss: 5.0401 - val_loss: 4.7169 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "11/11 - 1s - 45ms/step - loss: 4.9105 - val_loss: 4.7593 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "11/11 - 1s - 46ms/step - loss: 4.7618 - val_loss: 4.5084 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "11/11 - 1s - 47ms/step - loss: 4.6392 - val_loss: 4.3888 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "11/11 - 1s - 48ms/step - loss: 4.5241 - val_loss: 4.4128 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "11/11 - 1s - 46ms/step - loss: 4.4213 - val_loss: 4.2651 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "11/11 - 1s - 46ms/step - loss: 4.3049 - val_loss: 4.1840 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "11/11 - 1s - 47ms/step - loss: 4.2026 - val_loss: 4.1890 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "11/11 - 0s - 45ms/step - loss: 4.1056 - val_loss: 4.0602 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "11/11 - 1s - 46ms/step - loss: 4.0083 - val_loss: 3.8576 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "11/11 - 1s - 46ms/step - loss: 3.9071 - val_loss: 3.9509 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "11/11 - 1s - 49ms/step - loss: 3.8279 - val_loss: 3.7112 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "11/11 - 0s - 45ms/step - loss: 3.7488 - val_loss: 3.5856 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "11/11 - 1s - 61ms/step - loss: 3.6674 - val_loss: 3.5544 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "11/11 - 1s - 47ms/step - loss: 3.5883 - val_loss: 3.3507 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "11/11 - 1s - 52ms/step - loss: 3.4947 - val_loss: 3.5659 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "11/11 - 1s - 45ms/step - loss: 3.4171 - val_loss: 3.3734 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "11/11 - 1s - 46ms/step - loss: 3.3490 - val_loss: 3.3957 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "11/11 - 0s - 45ms/step - loss: 3.2658 - val_loss: 3.1449 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "11/11 - 1s - 47ms/step - loss: 3.2117 - val_loss: 3.3166 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "11/11 - 1s - 45ms/step - loss: 3.1509 - val_loss: 3.1682 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "11/11 - 1s - 48ms/step - loss: 3.0655 - val_loss: 3.1359 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "11/11 - 1s - 46ms/step - loss: 3.0667 - val_loss: 2.9592 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "11/11 - 1s - 47ms/step - loss: 2.9426 - val_loss: 3.0756 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.8883 - val_loss: 2.9002 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.8330 - val_loss: 2.9952 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "11/11 - 0s - 45ms/step - loss: 2.8039 - val_loss: 2.8541 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.7519 - val_loss: 2.9803 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.7396 - val_loss: 2.7002 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "11/11 - 1s - 48ms/step - loss: 2.6599 - val_loss: 2.6535 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "11/11 - 1s - 45ms/step - loss: 2.6133 - val_loss: 2.6702 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "11/11 - 1s - 48ms/step - loss: 2.5653 - val_loss: 2.5980 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "11/11 - 1s - 47ms/step - loss: 2.5185 - val_loss: 2.6590 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "11/11 - 1s - 47ms/step - loss: 2.4728 - val_loss: 2.5772 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "11/11 - 0s - 45ms/step - loss: 2.4597 - val_loss: 2.5190 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "11/11 - 0s - 45ms/step - loss: 2.4298 - val_loss: 2.6352 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "11/11 - 1s - 47ms/step - loss: 2.3821 - val_loss: 2.3897 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "11/11 - 1s - 45ms/step - loss: 2.3428 - val_loss: 2.5668 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "11/11 - 1s - 48ms/step - loss: 2.3138 - val_loss: 2.3403 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.2927 - val_loss: 2.3637 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "11/11 - 1s - 47ms/step - loss: 2.2513 - val_loss: 2.3164 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "11/11 - 1s - 45ms/step - loss: 2.1934 - val_loss: 2.4512 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "11/11 - 1s - 45ms/step - loss: 2.1694 - val_loss: 2.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "11/11 - 0s - 45ms/step - loss: 2.1506 - val_loss: 2.2423 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "11/11 - 1s - 48ms/step - loss: 2.1497 - val_loss: 2.2785 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.0899 - val_loss: 2.0664 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.0757 - val_loss: 2.3594 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "11/11 - 1s - 46ms/step - loss: 2.0613 - val_loss: 2.1810 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "11/11 - 1s - 48ms/step - loss: 2.0058 - val_loss: 2.1772 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.9969 - val_loss: 2.2061 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.9771 - val_loss: 2.0743 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.9572 - val_loss: 2.1547 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "11/11 - 1s - 49ms/step - loss: 1.9424 - val_loss: 2.1957 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.9122 - val_loss: 2.0640 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.9026 - val_loss: 2.0494 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.8812 - val_loss: 2.2277 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "11/11 - 1s - 45ms/step - loss: 1.8482 - val_loss: 1.8828 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.8275 - val_loss: 1.9866 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.8154 - val_loss: 2.1842 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.7992 - val_loss: 2.1927 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "11/11 - 1s - 45ms/step - loss: 1.7816 - val_loss: 1.9924 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.7937 - val_loss: 2.1139 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.7442 - val_loss: 1.8397 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.7687 - val_loss: 1.8903 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "11/11 - 1s - 47ms/step - loss: 1.7260 - val_loss: 2.0401 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "11/11 - 1s - 45ms/step - loss: 1.6990 - val_loss: 1.7455 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.6908 - val_loss: 1.9263 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.6723 - val_loss: 1.8842 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.6584 - val_loss: 1.7404 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "11/11 - 1s - 47ms/step - loss: 1.6760 - val_loss: 2.1135 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.6348 - val_loss: 1.8717 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "11/11 - 1s - 53ms/step - loss: 1.6184 - val_loss: 1.8466 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "11/11 - 1s - 47ms/step - loss: 1.6003 - val_loss: 1.8722 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.5652 - val_loss: 1.9354 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5946 - val_loss: 1.8340 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.5759 - val_loss: 1.9701 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.5600 - val_loss: 1.6896 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.5562 - val_loss: 1.9447 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "11/11 - 1s - 46ms/step - loss: 1.5397 - val_loss: 1.8359 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5181 - val_loss: 1.8026 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5288 - val_loss: 2.0186 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5232 - val_loss: 1.6862 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5107 - val_loss: 1.8060 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.4947 - val_loss: 1.7928 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.5025 - val_loss: 1.8984 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.4583 - val_loss: 1.7713 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "11/11 - 1s - 48ms/step - loss: 1.4743 - val_loss: 1.7001 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "11/11 - 0s - 45ms/step - loss: 1.4698 - val_loss: 1.9258 - learning_rate: 1.0000e-04\n",
      "Entrenando cluster numero: 5\n",
      "Epoch 1/100\n",
      "3/3 - 4s - 1s/step - loss: 9.2083 - val_loss: 11.3305 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "3/3 - 0s - 37ms/step - loss: 8.5063 - val_loss: 9.9109 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "3/3 - 0s - 34ms/step - loss: 7.8561 - val_loss: 8.7659 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "3/3 - 0s - 38ms/step - loss: 7.3940 - val_loss: 7.8708 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "3/3 - 0s - 41ms/step - loss: 7.0719 - val_loss: 7.2128 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.8960 - val_loss: 6.7631 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "3/3 - 0s - 27ms/step - loss: 6.7760 - val_loss: 6.4872 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "3/3 - 0s - 27ms/step - loss: 6.8015 - val_loss: 6.3184 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "3/3 - 0s - 30ms/step - loss: 6.7610 - val_loss: 6.2238 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "3/3 - 0s - 31ms/step - loss: 6.7010 - val_loss: 6.1708 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "3/3 - 0s - 29ms/step - loss: 6.6598 - val_loss: 6.1512 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.6302 - val_loss: 6.1606 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "3/3 - 0s - 41ms/step - loss: 6.5637 - val_loss: 6.1882 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "3/3 - 0s - 27ms/step - loss: 6.4316 - val_loss: 6.2374 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.4224 - val_loss: 6.3065 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "3/3 - 0s - 32ms/step - loss: 6.3177 - val_loss: 6.3101 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.3234 - val_loss: 6.2764 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.2776 - val_loss: 6.2210 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "3/3 - 0s - 33ms/step - loss: 6.2206 - val_loss: 6.1715 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "3/3 - 0s - 37ms/step - loss: 6.1740 - val_loss: 6.1092 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "3/3 - 0s - 32ms/step - loss: 6.0322 - val_loss: 6.0241 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "3/3 - 0s - 28ms/step - loss: 6.0896 - val_loss: 5.9508 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "3/3 - 0s - 33ms/step - loss: 6.0355 - val_loss: 5.8843 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "3/3 - 0s - 32ms/step - loss: 5.9851 - val_loss: 5.8321 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "3/3 - 0s - 32ms/step - loss: 5.9352 - val_loss: 5.7741 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.9013 - val_loss: 5.7194 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "3/3 - 0s - 30ms/step - loss: 5.8347 - val_loss: 5.6705 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.8247 - val_loss: 5.6379 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "3/3 - 0s - 32ms/step - loss: 5.7544 - val_loss: 5.5711 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "3/3 - 0s - 29ms/step - loss: 5.6742 - val_loss: 5.5388 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "3/3 - 0s - 27ms/step - loss: 5.6540 - val_loss: 5.5461 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "3/3 - 0s - 33ms/step - loss: 5.6029 - val_loss: 5.5325 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "3/3 - 0s - 35ms/step - loss: 5.5376 - val_loss: 5.5117 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "3/3 - 0s - 37ms/step - loss: 5.5414 - val_loss: 5.4800 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.4592 - val_loss: 5.4279 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "3/3 - 0s - 34ms/step - loss: 5.4238 - val_loss: 5.3697 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "3/3 - 0s - 31ms/step - loss: 5.4507 - val_loss: 5.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "3/3 - 0s - 35ms/step - loss: 5.3562 - val_loss: 5.2936 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "3/3 - 0s - 27ms/step - loss: 5.3032 - val_loss: 5.2147 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.2567 - val_loss: 5.1757 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "3/3 - 0s - 27ms/step - loss: 5.2969 - val_loss: 5.1489 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "3/3 - 0s - 32ms/step - loss: 5.2496 - val_loss: 5.1198 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "3/3 - 0s - 29ms/step - loss: 5.1522 - val_loss: 5.0894 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "3/3 - 0s - 31ms/step - loss: 5.1469 - val_loss: 5.0436 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.0854 - val_loss: 5.0633 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "3/3 - 0s - 30ms/step - loss: 5.0716 - val_loss: 5.0128 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.0099 - val_loss: 4.9287 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "3/3 - 0s - 28ms/step - loss: 5.0128 - val_loss: 4.8720 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "3/3 - 0s - 36ms/step - loss: 4.9464 - val_loss: 4.8406 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.9336 - val_loss: 4.8106 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "3/3 - 0s - 34ms/step - loss: 4.8672 - val_loss: 4.7816 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.8630 - val_loss: 4.7421 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "3/3 - 0s - 35ms/step - loss: 4.8233 - val_loss: 4.7038 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "3/3 - 0s - 29ms/step - loss: 4.7672 - val_loss: 4.6729 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.7426 - val_loss: 4.6138 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.7248 - val_loss: 4.6069 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "3/3 - 0s - 35ms/step - loss: 4.6626 - val_loss: 4.6042 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "3/3 - 0s - 29ms/step - loss: 4.5944 - val_loss: 4.5680 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "3/3 - 0s - 44ms/step - loss: 4.6101 - val_loss: 4.5184 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "3/3 - 0s - 36ms/step - loss: 4.5942 - val_loss: 4.5157 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "3/3 - 0s - 36ms/step - loss: 4.5474 - val_loss: 4.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "3/3 - 0s - 27ms/step - loss: 4.4904 - val_loss: 4.4534 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "3/3 - 0s - 29ms/step - loss: 4.5095 - val_loss: 4.4373 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "3/3 - 0s - 32ms/step - loss: 4.4741 - val_loss: 4.3687 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "3/3 - 0s - 31ms/step - loss: 4.4307 - val_loss: 4.3163 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "3/3 - 0s - 31ms/step - loss: 4.4221 - val_loss: 4.2869 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.3779 - val_loss: 4.2579 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "3/3 - 0s - 30ms/step - loss: 4.3140 - val_loss: 4.2483 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "3/3 - 0s - 36ms/step - loss: 4.2776 - val_loss: 4.2590 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "3/3 - 0s - 30ms/step - loss: 4.2844 - val_loss: 4.2742 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "3/3 - 0s - 32ms/step - loss: 4.2679 - val_loss: 4.2395 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.2157 - val_loss: 4.1708 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.2004 - val_loss: 4.1613 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "3/3 - 0s - 27ms/step - loss: 4.1893 - val_loss: 4.1588 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "3/3 - 0s - 27ms/step - loss: 4.1382 - val_loss: 4.1212 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "3/3 - 0s - 30ms/step - loss: 4.0960 - val_loss: 4.0810 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "3/3 - 0s - 28ms/step - loss: 4.1257 - val_loss: 4.0240 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "3/3 - 0s - 33ms/step - loss: 4.0453 - val_loss: 4.0007 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "3/3 - 0s - 33ms/step - loss: 4.0486 - val_loss: 3.9712 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "3/3 - 0s - 40ms/step - loss: 4.0021 - val_loss: 3.9590 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "3/3 - 0s - 33ms/step - loss: 3.9816 - val_loss: 3.9272 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "3/3 - 0s - 37ms/step - loss: 3.9353 - val_loss: 3.8706 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "3/3 - 0s - 41ms/step - loss: 3.9168 - val_loss: 3.8433 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "3/3 - 0s - 29ms/step - loss: 3.9041 - val_loss: 3.8556 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "3/3 - 0s - 28ms/step - loss: 3.8724 - val_loss: 3.9070 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "3/3 - 0s - 29ms/step - loss: 3.8201 - val_loss: 3.8503 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "3/3 - 0s - 37ms/step - loss: 3.8305 - val_loss: 3.7813 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "3/3 - 0s - 29ms/step - loss: 3.7933 - val_loss: 3.7512 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "3/3 - 0s - 27ms/step - loss: 3.7498 - val_loss: 3.7601 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "3/3 - 0s - 32ms/step - loss: 3.7436 - val_loss: 3.7550 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "3/3 - 0s - 28ms/step - loss: 3.7238 - val_loss: 3.6884 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "3/3 - 0s - 39ms/step - loss: 3.6649 - val_loss: 3.6460 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "3/3 - 0s - 27ms/step - loss: 3.6948 - val_loss: 3.6313 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "3/3 - 0s - 28ms/step - loss: 3.6636 - val_loss: 3.6447 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "3/3 - 0s - 28ms/step - loss: 3.6292 - val_loss: 3.6158 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "3/3 - 0s - 28ms/step - loss: 3.6042 - val_loss: 3.5881 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "3/3 - 0s - 35ms/step - loss: 3.6295 - val_loss: 3.5687 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "3/3 - 0s - 35ms/step - loss: 3.5684 - val_loss: 3.5414 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "3/3 - 0s - 31ms/step - loss: 3.5150 - val_loss: 3.5156 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "3/3 - 0s - 27ms/step - loss: 3.5182 - val_loss: 3.5080 - learning_rate: 1.0000e-04\n",
      "Entrenando cluster numero: 6\n",
      "Epoch 1/100\n",
      "43/43 - 7s - 158ms/step - loss: 6.8301 - val_loss: 6.2699 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "43/43 - 2s - 46ms/step - loss: 6.0153 - val_loss: 5.5839 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "43/43 - 2s - 45ms/step - loss: 5.3838 - val_loss: 5.0130 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "43/43 - 2s - 44ms/step - loss: 4.8329 - val_loss: 4.6569 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "43/43 - 2s - 44ms/step - loss: 4.3610 - val_loss: 4.0950 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "43/43 - 2s - 44ms/step - loss: 3.9632 - val_loss: 3.7712 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "43/43 - 2s - 44ms/step - loss: 3.5968 - val_loss: 3.4729 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "43/43 - 2s - 44ms/step - loss: 3.2837 - val_loss: 3.2528 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "43/43 - 2s - 44ms/step - loss: 3.0122 - val_loss: 2.9989 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "43/43 - 2s - 44ms/step - loss: 2.7755 - val_loss: 2.8660 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "43/43 - 2s - 44ms/step - loss: 2.5664 - val_loss: 2.5455 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "43/43 - 2s - 44ms/step - loss: 2.3887 - val_loss: 2.5514 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "43/43 - 2s - 47ms/step - loss: 2.2163 - val_loss: 2.4105 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "43/43 - 2s - 47ms/step - loss: 2.1214 - val_loss: 2.1994 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.9999 - val_loss: 2.2464 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.8922 - val_loss: 2.1114 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.8028 - val_loss: 1.8588 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.7261 - val_loss: 2.0651 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.6499 - val_loss: 1.8825 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "43/43 - 2s - 45ms/step - loss: 1.5897 - val_loss: 1.8798 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.5195 - val_loss: 1.9411 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.4483 - val_loss: 2.5071 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.4255 - val_loss: 1.2126 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "43/43 - 2s - 45ms/step - loss: 1.3796 - val_loss: 1.5113 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.2975 - val_loss: 2.1875 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.2734 - val_loss: 1.6781 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.1902 - val_loss: 2.1930 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.1634 - val_loss: 1.3621 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "43/43 - 2s - 46ms/step - loss: 1.1364 - val_loss: 1.6330 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "43/43 - 2s - 49ms/step - loss: 1.0940 - val_loss: 1.8241 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "43/43 - 2s - 45ms/step - loss: 1.0574 - val_loss: 2.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "43/43 - 2s - 44ms/step - loss: 1.0208 - val_loss: 1.2194 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "43/43 - 2s - 44ms/step - loss: 1.0124 - val_loss: 1.5448 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9728 - val_loss: 2.2447 - learning_rate: 2.0000e-05\n",
      "Epoch 35/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.9645 - val_loss: 2.1201 - learning_rate: 2.0000e-05\n",
      "Epoch 36/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9579 - val_loss: 1.9758 - learning_rate: 2.0000e-05\n",
      "Epoch 37/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9542 - val_loss: 2.0472 - learning_rate: 2.0000e-05\n",
      "Epoch 38/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9559 - val_loss: 2.0472 - learning_rate: 2.0000e-05\n",
      "Epoch 39/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.9375 - val_loss: 2.2430 - learning_rate: 2.0000e-05\n",
      "Epoch 40/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9398 - val_loss: 1.9985 - learning_rate: 2.0000e-05\n",
      "Epoch 41/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9380 - val_loss: 1.9779 - learning_rate: 2.0000e-05\n",
      "Epoch 42/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.9271 - val_loss: 2.0961 - learning_rate: 2.0000e-05\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "43/43 - 2s - 44ms/step - loss: 0.9208 - val_loss: 1.8451 - learning_rate: 2.0000e-05\n",
      "Epoch 44/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.9195 - val_loss: 2.0214 - learning_rate: 1.0000e-05\n",
      "Epoch 45/100\n",
      "43/43 - 2s - 46ms/step - loss: 0.9085 - val_loss: 1.9077 - learning_rate: 1.0000e-05\n",
      "Epoch 46/100\n",
      "43/43 - 2s - 49ms/step - loss: 0.9113 - val_loss: 1.8453 - learning_rate: 1.0000e-05\n",
      "Epoch 47/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.9071 - val_loss: 2.1156 - learning_rate: 1.0000e-05\n",
      "Epoch 48/100\n",
      "43/43 - 2s - 46ms/step - loss: 0.9021 - val_loss: 2.0214 - learning_rate: 1.0000e-05\n",
      "Epoch 49/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8980 - val_loss: 1.9922 - learning_rate: 1.0000e-05\n",
      "Epoch 50/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8903 - val_loss: 2.3426 - learning_rate: 1.0000e-05\n",
      "Epoch 51/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8969 - val_loss: 2.1223 - learning_rate: 1.0000e-05\n",
      "Epoch 52/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8965 - val_loss: 1.9708 - learning_rate: 1.0000e-05\n",
      "Epoch 53/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8876 - val_loss: 2.0993 - learning_rate: 1.0000e-05\n",
      "Epoch 54/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8866 - val_loss: 1.9632 - learning_rate: 1.0000e-05\n",
      "Epoch 55/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8829 - val_loss: 1.9406 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8866 - val_loss: 2.0692 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8765 - val_loss: 2.0087 - learning_rate: 1.0000e-05\n",
      "Epoch 58/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8756 - val_loss: 2.1008 - learning_rate: 1.0000e-05\n",
      "Epoch 59/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8704 - val_loss: 2.1982 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8728 - val_loss: 2.1414 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8645 - val_loss: 2.0600 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8640 - val_loss: 2.2213 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8790 - val_loss: 1.9022 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "43/43 - 2s - 49ms/step - loss: 0.8605 - val_loss: 2.1290 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8548 - val_loss: 2.1096 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8462 - val_loss: 1.9947 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8471 - val_loss: 2.1035 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8507 - val_loss: 2.0344 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8451 - val_loss: 2.1452 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8461 - val_loss: 2.1166 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8366 - val_loss: 1.9843 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8360 - val_loss: 2.1839 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "43/43 - 2s - 46ms/step - loss: 0.8340 - val_loss: 2.1185 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "43/43 - 2s - 46ms/step - loss: 0.8329 - val_loss: 2.0785 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8282 - val_loss: 2.0998 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8270 - val_loss: 1.9513 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8272 - val_loss: 2.2212 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8217 - val_loss: 2.1330 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8184 - val_loss: 2.3001 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8150 - val_loss: 2.1719 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8135 - val_loss: 1.8199 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "43/43 - 2s - 48ms/step - loss: 0.8144 - val_loss: 2.0005 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.8049 - val_loss: 2.0593 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8031 - val_loss: 2.2836 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8045 - val_loss: 2.2074 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8022 - val_loss: 2.0972 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7955 - val_loss: 2.0516 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.7930 - val_loss: 2.3792 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.8003 - val_loss: 2.0241 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7930 - val_loss: 2.1075 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7898 - val_loss: 1.9377 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7838 - val_loss: 2.1181 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7802 - val_loss: 2.1260 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7796 - val_loss: 2.2230 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7753 - val_loss: 2.0636 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7739 - val_loss: 2.0924 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7643 - val_loss: 2.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "43/43 - 2s - 44ms/step - loss: 0.7603 - val_loss: 2.0587 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "43/43 - 2s - 45ms/step - loss: 0.7578 - val_loss: 2.0377 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "43/43 - 2s - 49ms/step - loss: 0.7529 - val_loss: 1.8324 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 7\n",
      "Epoch 1/100\n",
      "18/18 - 6s - 311ms/step - loss: 7.2590 - val_loss: 6.1505 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "18/18 - 1s - 46ms/step - loss: 6.6642 - val_loss: 6.2657 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "18/18 - 1s - 46ms/step - loss: 6.3124 - val_loss: 6.0161 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "18/18 - 1s - 48ms/step - loss: 6.0189 - val_loss: 5.6341 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "18/18 - 1s - 47ms/step - loss: 5.7250 - val_loss: 5.4543 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "18/18 - 1s - 47ms/step - loss: 5.4697 - val_loss: 5.2361 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "18/18 - 1s - 47ms/step - loss: 5.2147 - val_loss: 5.0316 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "18/18 - 1s - 49ms/step - loss: 4.9704 - val_loss: 4.8194 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "18/18 - 1s - 47ms/step - loss: 4.7771 - val_loss: 4.7822 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "18/18 - 1s - 48ms/step - loss: 4.5629 - val_loss: 4.3811 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "18/18 - 1s - 46ms/step - loss: 4.3837 - val_loss: 4.1891 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "18/18 - 1s - 48ms/step - loss: 4.1813 - val_loss: 4.2963 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "18/18 - 1s - 46ms/step - loss: 4.0089 - val_loss: 3.9560 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "18/18 - 1s - 47ms/step - loss: 3.8340 - val_loss: 3.8876 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "18/18 - 1s - 46ms/step - loss: 3.7044 - val_loss: 3.6426 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "18/18 - 1s - 47ms/step - loss: 3.5355 - val_loss: 3.7180 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "18/18 - 1s - 47ms/step - loss: 3.4224 - val_loss: 3.5602 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "18/18 - 1s - 47ms/step - loss: 3.2984 - val_loss: 3.2544 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "18/18 - 1s - 48ms/step - loss: 3.1759 - val_loss: 3.1117 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "18/18 - 1s - 46ms/step - loss: 3.0646 - val_loss: 3.1631 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.9501 - val_loss: 3.0710 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.8476 - val_loss: 2.8502 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "18/18 - 1s - 46ms/step - loss: 2.7676 - val_loss: 2.8414 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.6568 - val_loss: 2.8807 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "18/18 - 1s - 49ms/step - loss: 2.5777 - val_loss: 2.8038 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "18/18 - 1s - 50ms/step - loss: 2.4824 - val_loss: 2.7749 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "18/18 - 1s - 50ms/step - loss: 2.4229 - val_loss: 2.4025 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "18/18 - 1s - 51ms/step - loss: 2.3756 - val_loss: 2.3315 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "18/18 - 1s - 49ms/step - loss: 2.2880 - val_loss: 2.3350 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.2340 - val_loss: 2.3510 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "18/18 - 1s - 48ms/step - loss: 2.1608 - val_loss: 2.4756 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "18/18 - 1s - 46ms/step - loss: 2.0915 - val_loss: 2.3554 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.0415 - val_loss: 2.1894 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "18/18 - 1s - 47ms/step - loss: 2.0208 - val_loss: 2.2472 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.9625 - val_loss: 2.3394 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "18/18 - 1s - 49ms/step - loss: 1.9012 - val_loss: 2.6414 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.8976 - val_loss: 2.2304 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.8249 - val_loss: 2.2834 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.7760 - val_loss: 1.9534 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.7263 - val_loss: 2.1589 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.6978 - val_loss: 2.0468 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.6528 - val_loss: 1.9632 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.6403 - val_loss: 2.1974 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.5822 - val_loss: 1.8533 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.5626 - val_loss: 1.9771 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.5652 - val_loss: 2.1527 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.5348 - val_loss: 1.5345 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.4870 - val_loss: 2.0697 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.4719 - val_loss: 2.1846 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.4424 - val_loss: 2.1246 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.4262 - val_loss: 2.0903 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.3857 - val_loss: 1.9989 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.3833 - val_loss: 2.5428 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.4032 - val_loss: 2.1306 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.3721 - val_loss: 1.7670 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.3299 - val_loss: 1.7472 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "18/18 - 1s - 48ms/step - loss: 1.2919 - val_loss: 1.6011 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2876 - val_loss: 2.0394 - learning_rate: 2.0000e-05\n",
      "Epoch 59/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2620 - val_loss: 1.9922 - learning_rate: 2.0000e-05\n",
      "Epoch 60/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2577 - val_loss: 2.1547 - learning_rate: 2.0000e-05\n",
      "Epoch 61/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2532 - val_loss: 2.1203 - learning_rate: 2.0000e-05\n",
      "Epoch 62/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2565 - val_loss: 1.9691 - learning_rate: 2.0000e-05\n",
      "Epoch 63/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2604 - val_loss: 2.1016 - learning_rate: 2.0000e-05\n",
      "Epoch 64/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2571 - val_loss: 2.0430 - learning_rate: 2.0000e-05\n",
      "Epoch 65/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2484 - val_loss: 1.8375 - learning_rate: 2.0000e-05\n",
      "Epoch 66/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2537 - val_loss: 1.9767 - learning_rate: 2.0000e-05\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "18/18 - 1s - 47ms/step - loss: 1.2402 - val_loss: 2.2022 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2352 - val_loss: 1.9261 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2443 - val_loss: 1.9707 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2296 - val_loss: 1.9553 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2363 - val_loss: 2.0569 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2217 - val_loss: 2.0031 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2214 - val_loss: 2.1289 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2200 - val_loss: 2.0043 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2117 - val_loss: 2.0590 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2174 - val_loss: 2.1091 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2188 - val_loss: 2.0331 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2148 - val_loss: 2.1457 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.2046 - val_loss: 2.0216 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2118 - val_loss: 2.1197 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1955 - val_loss: 2.0324 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.2072 - val_loss: 2.0918 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2137 - val_loss: 1.9094 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2017 - val_loss: 1.9910 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.2031 - val_loss: 2.0552 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1941 - val_loss: 2.0085 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1898 - val_loss: 2.1357 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1953 - val_loss: 1.9823 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1944 - val_loss: 1.9809 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1993 - val_loss: 1.9938 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1996 - val_loss: 1.9578 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.1938 - val_loss: 2.0786 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1922 - val_loss: 2.0328 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1878 - val_loss: 1.9357 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.1761 - val_loss: 1.9525 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1845 - val_loss: 2.0659 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1831 - val_loss: 1.9971 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "18/18 - 1s - 46ms/step - loss: 1.1858 - val_loss: 2.0493 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "18/18 - 1s - 48ms/step - loss: 1.1735 - val_loss: 1.9992 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "18/18 - 1s - 47ms/step - loss: 1.1785 - val_loss: 2.0172 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 8\n",
      "Epoch 1/100\n",
      "19/19 - 5s - 287ms/step - loss: 7.1137 - val_loss: 6.3194 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "19/19 - 1s - 48ms/step - loss: 6.6143 - val_loss: 6.1905 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "19/19 - 1s - 46ms/step - loss: 6.2493 - val_loss: 5.9010 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "19/19 - 1s - 46ms/step - loss: 5.9516 - val_loss: 5.5668 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "19/19 - 1s - 47ms/step - loss: 5.6734 - val_loss: 5.2827 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "19/19 - 1s - 46ms/step - loss: 5.3913 - val_loss: 5.0181 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "19/19 - 1s - 47ms/step - loss: 5.1252 - val_loss: 4.7967 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "19/19 - 1s - 47ms/step - loss: 4.8896 - val_loss: 4.5872 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.6714 - val_loss: 4.3957 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "19/19 - 1s - 47ms/step - loss: 4.4487 - val_loss: 4.2891 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "19/19 - 1s - 46ms/step - loss: 4.2665 - val_loss: 3.9471 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "19/19 - 1s - 48ms/step - loss: 4.0750 - val_loss: 3.7480 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.9067 - val_loss: 3.5923 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.7275 - val_loss: 3.6214 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.5681 - val_loss: 3.3659 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "19/19 - 1s - 48ms/step - loss: 3.4268 - val_loss: 3.3386 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "19/19 - 1s - 48ms/step - loss: 3.2866 - val_loss: 3.0291 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "19/19 - 1s - 46ms/step - loss: 3.1522 - val_loss: 3.1309 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "19/19 - 1s - 47ms/step - loss: 3.0328 - val_loss: 2.8420 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.9042 - val_loss: 2.7083 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.8101 - val_loss: 2.7369 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.6983 - val_loss: 2.4877 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.5997 - val_loss: 2.4257 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.4949 - val_loss: 2.2950 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.4198 - val_loss: 2.2693 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.3297 - val_loss: 2.3010 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.2347 - val_loss: 2.2031 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.1575 - val_loss: 2.0283 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "19/19 - 1s - 47ms/step - loss: 2.0794 - val_loss: 1.8333 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "19/19 - 1s - 46ms/step - loss: 2.0032 - val_loss: 2.2009 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.9486 - val_loss: 2.8713 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.9199 - val_loss: 2.0727 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.8536 - val_loss: 1.7355 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.7617 - val_loss: 1.7633 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.7048 - val_loss: 2.2285 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.6647 - val_loss: 1.5343 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.6274 - val_loss: 2.1852 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.5693 - val_loss: 1.6102 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.5355 - val_loss: 1.6459 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.4926 - val_loss: 2.0988 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.4759 - val_loss: 2.3241 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.4360 - val_loss: 1.6619 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.3969 - val_loss: 2.2196 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.3651 - val_loss: 1.4420 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.4005 - val_loss: 1.2905 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.3239 - val_loss: 1.5700 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2900 - val_loss: 1.3668 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2711 - val_loss: 1.3494 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.2583 - val_loss: 1.4986 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2327 - val_loss: 1.3863 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.2008 - val_loss: 1.8192 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1835 - val_loss: 1.2834 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.1720 - val_loss: 1.4292 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1493 - val_loss: 1.2095 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1401 - val_loss: 1.2483 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.1083 - val_loss: 1.7206 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0938 - val_loss: 1.7720 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.0988 - val_loss: 1.7387 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0727 - val_loss: 1.3554 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0579 - val_loss: 1.3767 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "19/19 - 1s - 47ms/step - loss: 1.0388 - val_loss: 1.5496 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0374 - val_loss: 1.4703 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "19/19 - 1s - 48ms/step - loss: 1.0435 - val_loss: 0.8934 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "19/19 - 1s - 46ms/step - loss: 1.0336 - val_loss: 1.4470 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.9795 - val_loss: 1.4995 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.9853 - val_loss: 1.4035 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.9672 - val_loss: 1.5619 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.9614 - val_loss: 1.7467 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.9528 - val_loss: 1.3001 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "19/19 - 1s - 48ms/step - loss: 0.9444 - val_loss: 0.9872 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.9349 - val_loss: 0.9496 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.9295 - val_loss: 1.1856 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "19/19 - 1s - 46ms/step - loss: 0.9078 - val_loss: 1.5573 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8894 - val_loss: 1.3701 - learning_rate: 2.0000e-05\n",
      "Epoch 75/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8828 - val_loss: 1.4282 - learning_rate: 2.0000e-05\n",
      "Epoch 76/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8787 - val_loss: 1.3836 - learning_rate: 2.0000e-05\n",
      "Epoch 77/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8773 - val_loss: 1.2985 - learning_rate: 2.0000e-05\n",
      "Epoch 78/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8734 - val_loss: 1.3135 - learning_rate: 2.0000e-05\n",
      "Epoch 79/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8704 - val_loss: 1.4781 - learning_rate: 2.0000e-05\n",
      "Epoch 80/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8724 - val_loss: 1.4323 - learning_rate: 2.0000e-05\n",
      "Epoch 81/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8689 - val_loss: 1.4819 - learning_rate: 2.0000e-05\n",
      "Epoch 82/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8713 - val_loss: 1.3751 - learning_rate: 2.0000e-05\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 48ms/step - loss: 0.8644 - val_loss: 1.4806 - learning_rate: 2.0000e-05\n",
      "Epoch 84/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8634 - val_loss: 1.4296 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8610 - val_loss: 1.4730 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "19/19 - 1s - 48ms/step - loss: 0.8528 - val_loss: 1.4041 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "19/19 - 1s - 48ms/step - loss: 0.8621 - val_loss: 1.3662 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8527 - val_loss: 1.3805 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8624 - val_loss: 1.3082 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8612 - val_loss: 1.4214 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8550 - val_loss: 1.3076 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8548 - val_loss: 1.5171 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8481 - val_loss: 1.4525 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "19/19 - 1s - 48ms/step - loss: 0.8566 - val_loss: 1.5923 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8519 - val_loss: 1.4614 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8549 - val_loss: 1.4992 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8521 - val_loss: 1.4120 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8515 - val_loss: 1.4236 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "19/19 - 1s - 47ms/step - loss: 0.8409 - val_loss: 1.2869 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "19/19 - 1s - 46ms/step - loss: 0.8521 - val_loss: 1.3351 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 9\n",
      "Epoch 1/100\n",
      "15/15 - 5s - 350ms/step - loss: 7.0959 - val_loss: 6.3429 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "15/15 - 1s - 51ms/step - loss: 6.7183 - val_loss: 6.7332 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "15/15 - 1s - 46ms/step - loss: 6.4632 - val_loss: 6.1023 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "15/15 - 1s - 48ms/step - loss: 6.2081 - val_loss: 6.0222 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "15/15 - 1s - 46ms/step - loss: 5.9569 - val_loss: 5.7910 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "15/15 - 1s - 46ms/step - loss: 5.7155 - val_loss: 5.5568 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "15/15 - 1s - 47ms/step - loss: 5.4996 - val_loss: 5.4241 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "15/15 - 1s - 46ms/step - loss: 5.2890 - val_loss: 5.1471 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "15/15 - 1s - 46ms/step - loss: 5.0888 - val_loss: 5.0792 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "15/15 - 1s - 46ms/step - loss: 4.8956 - val_loss: 5.1386 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "15/15 - 1s - 48ms/step - loss: 4.7194 - val_loss: 4.8614 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "15/15 - 1s - 50ms/step - loss: 4.5566 - val_loss: 4.6474 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "15/15 - 1s - 46ms/step - loss: 4.3794 - val_loss: 4.5298 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "15/15 - 1s - 46ms/step - loss: 4.2167 - val_loss: 4.3069 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "15/15 - 1s - 48ms/step - loss: 4.0645 - val_loss: 4.3561 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "15/15 - 1s - 48ms/step - loss: 3.9195 - val_loss: 4.1794 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "15/15 - 1s - 48ms/step - loss: 3.7864 - val_loss: 4.0819 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "15/15 - 1s - 46ms/step - loss: 3.6672 - val_loss: 3.9231 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "15/15 - 1s - 46ms/step - loss: 3.5510 - val_loss: 3.7420 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "15/15 - 1s - 46ms/step - loss: 3.4229 - val_loss: 3.5731 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "15/15 - 1s - 46ms/step - loss: 3.3226 - val_loss: 3.6253 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "15/15 - 1s - 48ms/step - loss: 3.2080 - val_loss: 3.3030 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "15/15 - 1s - 46ms/step - loss: 3.1292 - val_loss: 3.3527 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "15/15 - 1s - 47ms/step - loss: 3.0196 - val_loss: 3.6763 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.9443 - val_loss: 3.3313 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.8386 - val_loss: 3.1151 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "15/15 - 1s - 48ms/step - loss: 2.7589 - val_loss: 3.1293 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "15/15 - 1s - 46ms/step - loss: 2.6782 - val_loss: 3.1686 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "15/15 - 1s - 46ms/step - loss: 2.6002 - val_loss: 3.1249 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.5440 - val_loss: 2.9547 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "15/15 - 1s - 46ms/step - loss: 2.4623 - val_loss: 3.0005 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "15/15 - 1s - 46ms/step - loss: 2.4096 - val_loss: 2.7529 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.3445 - val_loss: 2.9680 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.2794 - val_loss: 2.9213 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "15/15 - 1s - 48ms/step - loss: 2.2336 - val_loss: 2.7120 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "15/15 - 1s - 48ms/step - loss: 2.1890 - val_loss: 2.4488 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "15/15 - 1s - 48ms/step - loss: 2.1262 - val_loss: 2.7876 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "15/15 - 1s - 46ms/step - loss: 2.0774 - val_loss: 2.6156 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "15/15 - 1s - 47ms/step - loss: 2.0414 - val_loss: 2.2561 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "15/15 - 1s - 48ms/step - loss: 2.0199 - val_loss: 2.3681 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.9686 - val_loss: 2.6440 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.9211 - val_loss: 2.6178 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.8614 - val_loss: 2.5188 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.8371 - val_loss: 2.2792 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.8088 - val_loss: 2.5077 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.7788 - val_loss: 2.3214 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.7552 - val_loss: 2.3378 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.7204 - val_loss: 2.4095 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.6677 - val_loss: 2.2434 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.6543 - val_loss: 2.4071 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.6234 - val_loss: 2.4221 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.5822 - val_loss: 2.3220 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.5675 - val_loss: 2.2707 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.5361 - val_loss: 2.4151 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.5156 - val_loss: 2.2167 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.5007 - val_loss: 2.1579 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.4756 - val_loss: 1.9991 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.4594 - val_loss: 2.2142 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.4494 - val_loss: 2.3801 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.4408 - val_loss: 2.0044 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.4137 - val_loss: 2.2144 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "15/15 - 1s - 49ms/step - loss: 1.4047 - val_loss: 2.1250 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.4077 - val_loss: 2.1480 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.3663 - val_loss: 2.0928 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.3593 - val_loss: 2.0570 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "15/15 - 1s - 49ms/step - loss: 1.3209 - val_loss: 2.0904 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "15/15 - 1s - 46ms/step - loss: 1.3257 - val_loss: 2.0520 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2954 - val_loss: 2.1310 - learning_rate: 2.0000e-05\n",
      "Epoch 69/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2908 - val_loss: 2.1345 - learning_rate: 2.0000e-05\n",
      "Epoch 70/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2792 - val_loss: 2.1282 - learning_rate: 2.0000e-05\n",
      "Epoch 71/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2771 - val_loss: 2.1452 - learning_rate: 2.0000e-05\n",
      "Epoch 72/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2800 - val_loss: 2.1154 - learning_rate: 2.0000e-05\n",
      "Epoch 73/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2797 - val_loss: 2.1505 - learning_rate: 2.0000e-05\n",
      "Epoch 74/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2704 - val_loss: 2.1847 - learning_rate: 2.0000e-05\n",
      "Epoch 75/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2684 - val_loss: 2.1303 - learning_rate: 2.0000e-05\n",
      "Epoch 76/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2710 - val_loss: 2.1400 - learning_rate: 2.0000e-05\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "15/15 - 1s - 47ms/step - loss: 1.2642 - val_loss: 2.1135 - learning_rate: 2.0000e-05\n",
      "Epoch 78/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.2553 - val_loss: 2.1514 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2678 - val_loss: 2.1596 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2572 - val_loss: 2.1466 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2606 - val_loss: 2.1477 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2579 - val_loss: 2.1519 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2532 - val_loss: 2.1793 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2444 - val_loss: 2.1196 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2566 - val_loss: 2.1310 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2506 - val_loss: 2.1542 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2431 - val_loss: 2.1642 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2443 - val_loss: 2.1304 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.2525 - val_loss: 2.1538 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.2418 - val_loss: 2.1733 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.2418 - val_loss: 2.1458 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2408 - val_loss: 2.1392 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2426 - val_loss: 2.1389 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2410 - val_loss: 2.1499 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2361 - val_loss: 2.1398 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "15/15 - 1s - 49ms/step - loss: 1.2312 - val_loss: 2.1328 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2299 - val_loss: 2.1349 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "15/15 - 1s - 46ms/step - loss: 1.2336 - val_loss: 2.1484 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "15/15 - 1s - 48ms/step - loss: 1.2304 - val_loss: 2.1083 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "15/15 - 1s - 47ms/step - loss: 1.2298 - val_loss: 2.1180 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 10\n",
      "Epoch 1/100\n",
      "14/14 - 5s - 358ms/step - loss: 7.5266 - val_loss: 6.3020 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14/14 - 1s - 51ms/step - loss: 6.8753 - val_loss: 6.8490 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14/14 - 1s - 47ms/step - loss: 6.5990 - val_loss: 6.4052 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14/14 - 1s - 47ms/step - loss: 6.3588 - val_loss: 6.2025 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14/14 - 1s - 46ms/step - loss: 6.0919 - val_loss: 5.8988 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14/14 - 1s - 47ms/step - loss: 5.9151 - val_loss: 5.9550 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "14/14 - 1s - 47ms/step - loss: 5.7025 - val_loss: 5.6941 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "14/14 - 1s - 46ms/step - loss: 5.4887 - val_loss: 5.5043 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "14/14 - 1s - 47ms/step - loss: 5.3106 - val_loss: 5.3261 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "14/14 - 1s - 48ms/step - loss: 5.1335 - val_loss: 5.3681 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "14/14 - 1s - 45ms/step - loss: 4.9600 - val_loss: 5.0279 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "14/14 - 1s - 45ms/step - loss: 4.8029 - val_loss: 4.9415 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.6457 - val_loss: 4.8897 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "14/14 - 1s - 45ms/step - loss: 4.5045 - val_loss: 4.6492 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "14/14 - 1s - 46ms/step - loss: 4.3692 - val_loss: 4.5047 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.2435 - val_loss: 4.4536 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "14/14 - 1s - 46ms/step - loss: 4.1119 - val_loss: 4.3068 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "14/14 - 1s - 45ms/step - loss: 3.9902 - val_loss: 4.2398 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "14/14 - 1s - 45ms/step - loss: 3.8845 - val_loss: 4.1168 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.7499 - val_loss: 3.9318 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "14/14 - 1s - 46ms/step - loss: 3.6612 - val_loss: 3.8934 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "14/14 - 1s - 45ms/step - loss: 3.5601 - val_loss: 3.7726 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "14/14 - 1s - 46ms/step - loss: 3.4529 - val_loss: 3.6176 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "14/14 - 1s - 45ms/step - loss: 3.3649 - val_loss: 3.5156 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.2773 - val_loss: 3.3958 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.1908 - val_loss: 3.3908 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "14/14 - 1s - 46ms/step - loss: 3.1274 - val_loss: 3.3331 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "14/14 - 1s - 46ms/step - loss: 3.0459 - val_loss: 3.4230 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.9591 - val_loss: 3.3399 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.8873 - val_loss: 3.1750 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "14/14 - 1s - 45ms/step - loss: 2.8127 - val_loss: 3.1156 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "14/14 - 1s - 45ms/step - loss: 2.7620 - val_loss: 3.0720 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.7155 - val_loss: 3.0203 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.6447 - val_loss: 2.9556 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "14/14 - 1s - 45ms/step - loss: 2.5849 - val_loss: 2.9625 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "14/14 - 1s - 45ms/step - loss: 2.5331 - val_loss: 2.8232 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.4718 - val_loss: 2.7635 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.4260 - val_loss: 2.5846 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "14/14 - 1s - 45ms/step - loss: 2.3776 - val_loss: 2.7820 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.3286 - val_loss: 2.6631 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.2707 - val_loss: 2.5225 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.2426 - val_loss: 2.4637 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.1837 - val_loss: 2.7489 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.1657 - val_loss: 2.3773 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.1145 - val_loss: 2.3721 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.0804 - val_loss: 2.6719 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.0357 - val_loss: 2.6344 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.9997 - val_loss: 2.4138 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.9616 - val_loss: 2.3107 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.9396 - val_loss: 2.5927 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.8933 - val_loss: 1.9685 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.8554 - val_loss: 1.8062 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.8414 - val_loss: 1.5893 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.8608 - val_loss: 2.7238 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.7818 - val_loss: 1.9566 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.7477 - val_loss: 2.1670 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.7139 - val_loss: 1.8304 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.6898 - val_loss: 1.5988 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.6769 - val_loss: 1.7164 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.6491 - val_loss: 2.0660 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.6252 - val_loss: 2.0301 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5955 - val_loss: 2.2226 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "14/14 - 1s - 45ms/step - loss: 1.5628 - val_loss: 2.4291 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5738 - val_loss: 1.7998 - learning_rate: 2.0000e-05\n",
      "Epoch 65/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.5588 - val_loss: 1.6406 - learning_rate: 2.0000e-05\n",
      "Epoch 66/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.5404 - val_loss: 1.8653 - learning_rate: 2.0000e-05\n",
      "Epoch 67/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5425 - val_loss: 2.0867 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.5461 - val_loss: 1.8781 - learning_rate: 2.0000e-05\n",
      "Epoch 69/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5391 - val_loss: 1.8699 - learning_rate: 2.0000e-05\n",
      "Epoch 70/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.5207 - val_loss: 1.8176 - learning_rate: 2.0000e-05\n",
      "Epoch 71/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5154 - val_loss: 1.7516 - learning_rate: 2.0000e-05\n",
      "Epoch 72/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.5291 - val_loss: 2.0024 - learning_rate: 2.0000e-05\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 - 1s - 46ms/step - loss: 1.5196 - val_loss: 2.0315 - learning_rate: 2.0000e-05\n",
      "Epoch 74/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.5127 - val_loss: 1.7858 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5060 - val_loss: 1.7690 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.5027 - val_loss: 1.9069 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5076 - val_loss: 1.7366 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5119 - val_loss: 1.8622 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.5095 - val_loss: 1.8165 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5060 - val_loss: 1.7392 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4897 - val_loss: 1.9336 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.5103 - val_loss: 1.8803 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.4937 - val_loss: 1.8619 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.4934 - val_loss: 1.8850 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4898 - val_loss: 1.7775 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4933 - val_loss: 1.9338 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.4930 - val_loss: 1.6872 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4928 - val_loss: 1.9137 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4861 - val_loss: 1.7100 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4739 - val_loss: 1.8995 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4740 - val_loss: 1.8880 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.4739 - val_loss: 1.8817 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4824 - val_loss: 1.7268 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.4663 - val_loss: 1.9334 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4771 - val_loss: 1.6905 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4632 - val_loss: 2.0209 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4560 - val_loss: 1.7447 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "14/14 - 1s - 45ms/step - loss: 1.4712 - val_loss: 1.8106 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4589 - val_loss: 1.9186 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4579 - val_loss: 1.6844 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 11\n",
      "Epoch 1/100\n",
      "9/9 - 4s - 496ms/step - loss: 7.2069 - val_loss: 6.3921 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "9/9 - 0s - 48ms/step - loss: 6.9085 - val_loss: 6.5560 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "9/9 - 0s - 49ms/step - loss: 6.7022 - val_loss: 6.5151 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "9/9 - 0s - 50ms/step - loss: 6.5557 - val_loss: 6.2276 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "9/9 - 0s - 50ms/step - loss: 6.3626 - val_loss: 6.0747 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "9/9 - 0s - 53ms/step - loss: 6.2390 - val_loss: 5.9891 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "9/9 - 0s - 48ms/step - loss: 6.0926 - val_loss: 5.8467 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "9/9 - 0s - 48ms/step - loss: 5.9289 - val_loss: 5.6641 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "9/9 - 0s - 51ms/step - loss: 5.7981 - val_loss: 5.6377 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "9/9 - 0s - 48ms/step - loss: 5.6365 - val_loss: 5.2855 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "9/9 - 0s - 48ms/step - loss: 5.5142 - val_loss: 5.2678 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "9/9 - 0s - 48ms/step - loss: 5.3584 - val_loss: 5.1616 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "9/9 - 0s - 51ms/step - loss: 5.2349 - val_loss: 5.0367 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "9/9 - 0s - 49ms/step - loss: 5.1171 - val_loss: 4.9019 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "9/9 - 0s - 48ms/step - loss: 5.0207 - val_loss: 4.9264 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "9/9 - 0s - 49ms/step - loss: 4.8767 - val_loss: 4.8445 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "9/9 - 0s - 49ms/step - loss: 4.7570 - val_loss: 4.7243 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "9/9 - 0s - 51ms/step - loss: 4.6729 - val_loss: 4.6249 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "9/9 - 0s - 51ms/step - loss: 4.5707 - val_loss: 4.5277 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "9/9 - 0s - 51ms/step - loss: 4.4493 - val_loss: 4.3624 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "9/9 - 0s - 53ms/step - loss: 4.3465 - val_loss: 4.2131 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "9/9 - 0s - 48ms/step - loss: 4.2477 - val_loss: 4.1811 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "9/9 - 0s - 50ms/step - loss: 4.1698 - val_loss: 4.2200 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "9/9 - 0s - 50ms/step - loss: 4.0424 - val_loss: 3.9848 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "9/9 - 0s - 50ms/step - loss: 3.9770 - val_loss: 4.0586 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "9/9 - 0s - 50ms/step - loss: 3.8643 - val_loss: 4.0229 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "9/9 - 0s - 52ms/step - loss: 3.7866 - val_loss: 3.8002 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "9/9 - 0s - 49ms/step - loss: 3.7105 - val_loss: 3.7282 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "9/9 - 0s - 51ms/step - loss: 3.6228 - val_loss: 3.9003 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "9/9 - 0s - 51ms/step - loss: 3.5326 - val_loss: 3.5531 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "9/9 - 0s - 48ms/step - loss: 3.5016 - val_loss: 3.5225 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "9/9 - 0s - 49ms/step - loss: 3.3852 - val_loss: 3.7823 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "9/9 - 0s - 48ms/step - loss: 3.3027 - val_loss: 3.6374 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "9/9 - 0s - 48ms/step - loss: 3.2522 - val_loss: 3.4866 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "9/9 - 0s - 53ms/step - loss: 3.1587 - val_loss: 3.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "9/9 - 0s - 48ms/step - loss: 3.1198 - val_loss: 3.4136 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "9/9 - 0s - 50ms/step - loss: 3.0378 - val_loss: 3.4943 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.9885 - val_loss: 3.4002 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.9200 - val_loss: 3.4056 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.8993 - val_loss: 3.4830 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.8466 - val_loss: 3.3577 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.8011 - val_loss: 3.4310 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "9/9 - 0s - 51ms/step - loss: 2.7082 - val_loss: 3.0519 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.6859 - val_loss: 3.2805 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "9/9 - 0s - 49ms/step - loss: 2.6666 - val_loss: 3.4504 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.5776 - val_loss: 3.1887 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.5261 - val_loss: 2.7348 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.4681 - val_loss: 3.2067 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.4540 - val_loss: 2.7753 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.3973 - val_loss: 2.7470 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "9/9 - 0s - 51ms/step - loss: 2.3377 - val_loss: 3.3287 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.3073 - val_loss: 3.1898 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "9/9 - 0s - 49ms/step - loss: 2.2779 - val_loss: 3.1584 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "9/9 - 0s - 51ms/step - loss: 2.2442 - val_loss: 2.8346 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "9/9 - 0s - 49ms/step - loss: 2.1828 - val_loss: 2.9433 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.1448 - val_loss: 2.7689 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "9/9 - 0s - 48ms/step - loss: 2.1309 - val_loss: 2.8419 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.0881 - val_loss: 2.7120 - learning_rate: 2.0000e-05\n",
      "Epoch 59/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.0889 - val_loss: 2.7332 - learning_rate: 2.0000e-05\n",
      "Epoch 60/100\n",
      "9/9 - 0s - 49ms/step - loss: 2.0852 - val_loss: 2.8525 - learning_rate: 2.0000e-05\n",
      "Epoch 61/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.0750 - val_loss: 2.7832 - learning_rate: 2.0000e-05\n",
      "Epoch 62/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.0851 - val_loss: 2.7509 - learning_rate: 2.0000e-05\n",
      "Epoch 63/100\n",
      "9/9 - 0s - 51ms/step - loss: 2.0493 - val_loss: 2.7633 - learning_rate: 2.0000e-05\n",
      "Epoch 64/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.0523 - val_loss: 2.8322 - learning_rate: 2.0000e-05\n",
      "Epoch 65/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.0610 - val_loss: 2.7666 - learning_rate: 2.0000e-05\n",
      "Epoch 66/100\n",
      "9/9 - 0s - 49ms/step - loss: 2.0411 - val_loss: 2.7269 - learning_rate: 2.0000e-05\n",
      "Epoch 67/100\n",
      "9/9 - 0s - 51ms/step - loss: 2.0000 - val_loss: 2.7907 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 - 0s - 49ms/step - loss: 2.0253 - val_loss: 2.8193 - learning_rate: 2.0000e-05\n",
      "Epoch 69/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9970 - val_loss: 2.7825 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "9/9 - 0s - 47ms/step - loss: 1.9995 - val_loss: 2.7685 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "9/9 - 0s - 48ms/step - loss: 2.0103 - val_loss: 2.8808 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "9/9 - 0s - 50ms/step - loss: 2.0002 - val_loss: 2.8082 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9823 - val_loss: 2.8209 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "9/9 - 0s - 49ms/step - loss: 1.9967 - val_loss: 2.7848 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9845 - val_loss: 2.8240 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9845 - val_loss: 2.7563 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9835 - val_loss: 2.8032 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9771 - val_loss: 2.8206 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9641 - val_loss: 2.7064 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9797 - val_loss: 2.8276 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "9/9 - 0s - 51ms/step - loss: 1.9750 - val_loss: 2.7571 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9643 - val_loss: 2.7323 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9632 - val_loss: 2.8301 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "9/9 - 0s - 49ms/step - loss: 1.9663 - val_loss: 2.8000 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9428 - val_loss: 2.7614 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9493 - val_loss: 2.7660 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9426 - val_loss: 2.7956 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9282 - val_loss: 2.7454 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9457 - val_loss: 2.7845 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9356 - val_loss: 2.7486 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9374 - val_loss: 2.7757 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9347 - val_loss: 2.7733 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "9/9 - 0s - 54ms/step - loss: 1.9355 - val_loss: 2.8235 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9253 - val_loss: 2.7722 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "9/9 - 0s - 48ms/step - loss: 1.9143 - val_loss: 2.7643 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "9/9 - 0s - 50ms/step - loss: 1.9159 - val_loss: 2.7850 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "9/9 - 0s - 49ms/step - loss: 1.8967 - val_loss: 2.7777 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "9/9 - 0s - 49ms/step - loss: 1.9077 - val_loss: 2.7520 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "9/9 - 0s - 52ms/step - loss: 1.9013 - val_loss: 2.7397 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "9/9 - 0s - 52ms/step - loss: 1.8961 - val_loss: 2.8097 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 12\n",
      "Epoch 1/100\n",
      "30/30 - 6s - 199ms/step - loss: 7.0760 - val_loss: 6.5311 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "30/30 - 1s - 45ms/step - loss: 6.3698 - val_loss: 5.9835 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "30/30 - 1s - 45ms/step - loss: 5.8729 - val_loss: 5.3966 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "30/30 - 1s - 46ms/step - loss: 5.4644 - val_loss: 5.1887 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "30/30 - 1s - 46ms/step - loss: 5.0695 - val_loss: 4.8061 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "30/30 - 1s - 45ms/step - loss: 4.7156 - val_loss: 4.4405 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "30/30 - 1s - 46ms/step - loss: 4.3841 - val_loss: 4.1527 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "30/30 - 1s - 45ms/step - loss: 4.0945 - val_loss: 3.9302 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "30/30 - 1s - 47ms/step - loss: 3.8453 - val_loss: 3.7598 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "30/30 - 1s - 47ms/step - loss: 3.5980 - val_loss: 3.5579 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "30/30 - 1s - 48ms/step - loss: 3.3858 - val_loss: 3.3405 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "30/30 - 1s - 47ms/step - loss: 3.1933 - val_loss: 3.1644 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "30/30 - 1s - 45ms/step - loss: 3.0207 - val_loss: 3.2322 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "30/30 - 1s - 45ms/step - loss: 2.8542 - val_loss: 2.8907 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "30/30 - 1s - 45ms/step - loss: 2.7007 - val_loss: 2.7798 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "30/30 - 1s - 46ms/step - loss: 2.5696 - val_loss: 2.6586 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "30/30 - 1s - 45ms/step - loss: 2.4391 - val_loss: 2.4893 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "30/30 - 1s - 46ms/step - loss: 2.3337 - val_loss: 2.1857 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "30/30 - 1s - 45ms/step - loss: 2.2275 - val_loss: 2.6471 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "30/30 - 1s - 46ms/step - loss: 2.1385 - val_loss: 2.4655 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "30/30 - 1s - 45ms/step - loss: 2.0432 - val_loss: 2.1784 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.9582 - val_loss: 2.0522 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.8890 - val_loss: 1.8135 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "30/30 - 1s - 48ms/step - loss: 1.8138 - val_loss: 2.1144 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "30/30 - 1s - 48ms/step - loss: 1.7460 - val_loss: 1.9168 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.6849 - val_loss: 2.3867 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.6221 - val_loss: 1.7910 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.5831 - val_loss: 2.1107 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.5201 - val_loss: 2.0228 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.4789 - val_loss: 1.8261 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.4397 - val_loss: 1.9947 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.4088 - val_loss: 2.1237 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.3663 - val_loss: 1.5525 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.3306 - val_loss: 2.1803 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.3218 - val_loss: 1.9102 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.2889 - val_loss: 1.3889 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "30/30 - 2s - 50ms/step - loss: 1.2690 - val_loss: 1.7480 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "30/30 - 1s - 48ms/step - loss: 1.2280 - val_loss: 1.5485 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.1963 - val_loss: 1.7707 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.1668 - val_loss: 1.5445 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.1446 - val_loss: 1.3930 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.1311 - val_loss: 1.9068 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.1141 - val_loss: 1.9416 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.1012 - val_loss: 1.3089 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.0680 - val_loss: 1.1011 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.0650 - val_loss: 1.6999 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.0531 - val_loss: 1.7520 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "30/30 - 1s - 45ms/step - loss: 1.0495 - val_loss: 1.3836 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "30/30 - 1s - 46ms/step - loss: 1.0109 - val_loss: 1.6587 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.9850 - val_loss: 1.8684 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.9881 - val_loss: 1.0027 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.9685 - val_loss: 1.2486 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.9662 - val_loss: 1.9153 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.9684 - val_loss: 1.1195 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.9418 - val_loss: 1.2735 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.9223 - val_loss: 0.8338 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.9301 - val_loss: 1.0985 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.9063 - val_loss: 1.5181 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.8799 - val_loss: 1.0618 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.8852 - val_loss: 1.8317 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.8479 - val_loss: 1.3912 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.8451 - val_loss: 1.1633 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.8275 - val_loss: 1.7262 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "30/30 - 1s - 47ms/step - loss: 0.8239 - val_loss: 2.1054 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "30/30 - 1s - 48ms/step - loss: 0.8202 - val_loss: 1.5125 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "30/30 - 1s - 47ms/step - loss: 0.7853 - val_loss: 1.5115 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7791 - val_loss: 1.8196 - learning_rate: 2.0000e-05\n",
      "Epoch 68/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7724 - val_loss: 1.6053 - learning_rate: 2.0000e-05\n",
      "Epoch 69/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7539 - val_loss: 1.8154 - learning_rate: 2.0000e-05\n",
      "Epoch 70/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7627 - val_loss: 1.3368 - learning_rate: 2.0000e-05\n",
      "Epoch 71/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7539 - val_loss: 1.6316 - learning_rate: 2.0000e-05\n",
      "Epoch 72/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7480 - val_loss: 1.5607 - learning_rate: 2.0000e-05\n",
      "Epoch 73/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7458 - val_loss: 1.7660 - learning_rate: 2.0000e-05\n",
      "Epoch 74/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7474 - val_loss: 1.5327 - learning_rate: 2.0000e-05\n",
      "Epoch 75/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7466 - val_loss: 1.6534 - learning_rate: 2.0000e-05\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "30/30 - 1s - 46ms/step - loss: 0.7405 - val_loss: 1.5159 - learning_rate: 2.0000e-05\n",
      "Epoch 77/100\n",
      "30/30 - 1s - 47ms/step - loss: 0.7371 - val_loss: 1.5104 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "30/30 - 1s - 48ms/step - loss: 0.7338 - val_loss: 1.7384 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7403 - val_loss: 1.7220 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7371 - val_loss: 1.5649 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7326 - val_loss: 1.5174 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7278 - val_loss: 1.5735 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7297 - val_loss: 1.6736 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7263 - val_loss: 1.5602 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7245 - val_loss: 1.5103 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7300 - val_loss: 1.4248 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7273 - val_loss: 1.3937 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7267 - val_loss: 1.5335 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7172 - val_loss: 1.8990 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.7157 - val_loss: 1.5095 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "30/30 - 1s - 48ms/step - loss: 0.7148 - val_loss: 1.5348 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7206 - val_loss: 1.5368 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "30/30 - 1s - 45ms/step - loss: 0.7107 - val_loss: 1.7059 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "30/30 - 1s - 46ms/step - loss: 0.7117 - val_loss: 1.5798 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.7072 - val_loss: 1.6400 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.7045 - val_loss: 1.5226 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.7102 - val_loss: 1.6637 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "30/30 - 2s - 50ms/step - loss: 0.7078 - val_loss: 1.5212 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "30/30 - 1s - 49ms/step - loss: 0.7036 - val_loss: 1.8251 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "30/30 - 1s - 48ms/step - loss: 0.6995 - val_loss: 1.5298 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 13\n",
      "Epoch 1/100\n",
      "10/10 - 5s - 482ms/step - loss: 8.1875 - val_loss: 6.9695 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "10/10 - 1s - 53ms/step - loss: 7.0626 - val_loss: 6.1517 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "10/10 - 0s - 49ms/step - loss: 6.8756 - val_loss: 6.3101 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "10/10 - 1s - 51ms/step - loss: 6.6469 - val_loss: 6.4494 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "10/10 - 1s - 50ms/step - loss: 6.4885 - val_loss: 6.1851 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "10/10 - 1s - 50ms/step - loss: 6.2784 - val_loss: 5.9384 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "10/10 - 0s - 48ms/step - loss: 6.1233 - val_loss: 5.7938 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "10/10 - 1s - 53ms/step - loss: 6.0352 - val_loss: 5.6940 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "10/10 - 0s - 48ms/step - loss: 5.8694 - val_loss: 5.6130 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "10/10 - 0s - 48ms/step - loss: 5.7425 - val_loss: 5.4372 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "10/10 - 0s - 48ms/step - loss: 5.5752 - val_loss: 5.3457 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "10/10 - 0s - 50ms/step - loss: 5.4694 - val_loss: 5.2360 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "10/10 - 0s - 50ms/step - loss: 5.3540 - val_loss: 5.0535 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "10/10 - 1s - 52ms/step - loss: 5.2191 - val_loss: 4.9621 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "10/10 - 1s - 50ms/step - loss: 5.0741 - val_loss: 4.8736 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "10/10 - 0s - 48ms/step - loss: 4.9793 - val_loss: 4.7981 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "10/10 - 0s - 48ms/step - loss: 4.8768 - val_loss: 4.6533 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "10/10 - 1s - 51ms/step - loss: 4.7612 - val_loss: 4.5974 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "10/10 - 0s - 49ms/step - loss: 4.6721 - val_loss: 4.6081 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "10/10 - 1s - 51ms/step - loss: 4.5636 - val_loss: 4.3470 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "10/10 - 0s - 50ms/step - loss: 4.4434 - val_loss: 4.3546 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "10/10 - 1s - 51ms/step - loss: 4.3496 - val_loss: 4.1670 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "10/10 - 1s - 51ms/step - loss: 4.2732 - val_loss: 4.1674 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "10/10 - 0s - 48ms/step - loss: 4.2072 - val_loss: 4.0591 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "10/10 - 0s - 49ms/step - loss: 4.1132 - val_loss: 4.0254 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "10/10 - 0s - 50ms/step - loss: 4.0261 - val_loss: 3.9240 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "10/10 - 0s - 49ms/step - loss: 3.9220 - val_loss: 3.9032 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "10/10 - 1s - 51ms/step - loss: 3.8596 - val_loss: 3.8237 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "10/10 - 1s - 52ms/step - loss: 3.8001 - val_loss: 3.7006 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "10/10 - 0s - 49ms/step - loss: 3.7225 - val_loss: 3.5646 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "10/10 - 0s - 48ms/step - loss: 3.6564 - val_loss: 3.6487 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "10/10 - 1s - 51ms/step - loss: 3.5808 - val_loss: 3.4709 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "10/10 - 1s - 52ms/step - loss: 3.5183 - val_loss: 3.3696 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "10/10 - 1s - 50ms/step - loss: 3.4464 - val_loss: 3.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "10/10 - 1s - 51ms/step - loss: 3.3885 - val_loss: 3.3497 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "10/10 - 0s - 49ms/step - loss: 3.3262 - val_loss: 3.1353 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "10/10 - 0s - 49ms/step - loss: 3.2751 - val_loss: 3.2342 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "10/10 - 0s - 49ms/step - loss: 3.2129 - val_loss: 3.2196 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "10/10 - 1s - 50ms/step - loss: 3.1443 - val_loss: 3.1662 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "10/10 - 0s - 48ms/step - loss: 3.1020 - val_loss: 2.9499 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "10/10 - 0s - 48ms/step - loss: 3.0565 - val_loss: 3.1036 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "10/10 - 1s - 52ms/step - loss: 2.9998 - val_loss: 2.9678 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.9535 - val_loss: 3.0189 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.9060 - val_loss: 2.8735 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "10/10 - 1s - 51ms/step - loss: 2.8737 - val_loss: 2.8275 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "10/10 - 0s - 49ms/step - loss: 2.8105 - val_loss: 2.8081 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.7896 - val_loss: 3.0093 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "10/10 - 0s - 49ms/step - loss: 2.7522 - val_loss: 2.6905 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "10/10 - 1s - 51ms/step - loss: 2.6981 - val_loss: 2.5223 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "10/10 - 0s - 49ms/step - loss: 2.6530 - val_loss: 2.7232 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.6209 - val_loss: 2.6658 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "10/10 - 0s - 49ms/step - loss: 2.5879 - val_loss: 2.5089 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.5363 - val_loss: 2.7515 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.4902 - val_loss: 2.4481 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "10/10 - 1s - 51ms/step - loss: 2.4991 - val_loss: 2.5414 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.4332 - val_loss: 2.8574 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.4160 - val_loss: 2.4994 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "10/10 - 0s - 50ms/step - loss: 2.3714 - val_loss: 2.6113 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.3441 - val_loss: 2.4350 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.2797 - val_loss: 2.8883 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.2605 - val_loss: 2.5095 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.2334 - val_loss: 2.2881 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.2033 - val_loss: 2.3074 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "10/10 - 0s - 50ms/step - loss: 2.1917 - val_loss: 3.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.2217 - val_loss: 2.6815 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.1343 - val_loss: 2.3270 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.0958 - val_loss: 2.6411 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.0727 - val_loss: 2.2680 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.0354 - val_loss: 2.2955 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "10/10 - 1s - 50ms/step - loss: 2.0269 - val_loss: 2.6085 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "10/10 - 0s - 48ms/step - loss: 2.0010 - val_loss: 2.7708 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "10/10 - 0s - 48ms/step - loss: 1.9859 - val_loss: 2.6477 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "10/10 - 1s - 55ms/step - loss: 1.9706 - val_loss: 1.9506 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.9818 - val_loss: 2.3632 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "10/10 - 1s - 53ms/step - loss: 1.9521 - val_loss: 1.9788 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "10/10 - 1s - 54ms/step - loss: 1.8964 - val_loss: 2.3823 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.8854 - val_loss: 2.0985 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "10/10 - 1s - 62ms/step - loss: 1.8689 - val_loss: 2.7138 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "10/10 - 1s - 53ms/step - loss: 1.8233 - val_loss: 2.6424 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "10/10 - 1s - 55ms/step - loss: 1.8180 - val_loss: 2.0984 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "10/10 - 1s - 54ms/step - loss: 1.8218 - val_loss: 3.2912 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "10/10 - 1s - 55ms/step - loss: 1.8054 - val_loss: 3.0421 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "10/10 - 0s - 50ms/step - loss: 1.8087 - val_loss: 2.0232 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.7691 - val_loss: 2.6305 - learning_rate: 2.0000e-05\n",
      "Epoch 85/100\n",
      "10/10 - 0s - 48ms/step - loss: 1.7483 - val_loss: 2.1538 - learning_rate: 2.0000e-05\n",
      "Epoch 86/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.7468 - val_loss: 2.7391 - learning_rate: 2.0000e-05\n",
      "Epoch 87/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.7524 - val_loss: 2.3054 - learning_rate: 2.0000e-05\n",
      "Epoch 88/100\n",
      "10/10 - 0s - 48ms/step - loss: 1.7406 - val_loss: 2.3406 - learning_rate: 2.0000e-05\n",
      "Epoch 89/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.7333 - val_loss: 2.6072 - learning_rate: 2.0000e-05\n",
      "Epoch 90/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.7257 - val_loss: 2.2054 - learning_rate: 2.0000e-05\n",
      "Epoch 91/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.7246 - val_loss: 2.4486 - learning_rate: 2.0000e-05\n",
      "Epoch 92/100\n",
      "10/10 - 1s - 52ms/step - loss: 1.7281 - val_loss: 2.6586 - learning_rate: 2.0000e-05\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "10/10 - 0s - 49ms/step - loss: 1.7124 - val_loss: 2.3748 - learning_rate: 2.0000e-05\n",
      "Epoch 94/100\n",
      "10/10 - 1s - 53ms/step - loss: 1.7288 - val_loss: 2.4662 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "10/10 - 1s - 53ms/step - loss: 1.7187 - val_loss: 2.4370 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.7029 - val_loss: 2.4234 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.7175 - val_loss: 2.5016 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "10/10 - 1s - 51ms/step - loss: 1.7116 - val_loss: 2.4468 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.7037 - val_loss: 2.5123 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "10/10 - 0s - 49ms/step - loss: 1.6977 - val_loss: 2.4021 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 14\n",
      "Epoch 1/100\n",
      "14/14 - 5s - 387ms/step - loss: 7.1336 - val_loss: 6.3191 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14/14 - 1s - 51ms/step - loss: 6.8050 - val_loss: 6.5458 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14/14 - 1s - 47ms/step - loss: 6.5152 - val_loss: 6.0827 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14/14 - 1s - 47ms/step - loss: 6.2925 - val_loss: 5.9759 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14/14 - 1s - 48ms/step - loss: 6.0591 - val_loss: 5.6592 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14/14 - 1s - 47ms/step - loss: 5.8291 - val_loss: 5.4801 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "14/14 - 1s - 48ms/step - loss: 5.6204 - val_loss: 5.3974 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "14/14 - 1s - 46ms/step - loss: 5.4176 - val_loss: 5.1622 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "14/14 - 1s - 49ms/step - loss: 5.2173 - val_loss: 4.9601 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "14/14 - 1s - 48ms/step - loss: 5.0497 - val_loss: 4.8144 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.8670 - val_loss: 4.6004 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "14/14 - 1s - 48ms/step - loss: 4.7007 - val_loss: 4.6183 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.5434 - val_loss: 4.4120 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.4015 - val_loss: 4.1336 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.2468 - val_loss: 4.0485 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "14/14 - 1s - 47ms/step - loss: 4.1057 - val_loss: 3.9112 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "14/14 - 1s - 48ms/step - loss: 3.9887 - val_loss: 3.7976 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "14/14 - 1s - 49ms/step - loss: 3.8505 - val_loss: 3.6952 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "14/14 - 1s - 49ms/step - loss: 3.7339 - val_loss: 3.6015 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "14/14 - 1s - 48ms/step - loss: 3.6189 - val_loss: 3.3745 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "14/14 - 1s - 48ms/step - loss: 3.5197 - val_loss: 3.4395 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.4148 - val_loss: 3.3276 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.3122 - val_loss: 3.1709 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "14/14 - 1s - 48ms/step - loss: 3.2148 - val_loss: 3.1608 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "14/14 - 1s - 49ms/step - loss: 3.1250 - val_loss: 2.9968 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "14/14 - 1s - 47ms/step - loss: 3.0401 - val_loss: 2.9489 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.9553 - val_loss: 2.8198 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "14/14 - 1s - 49ms/step - loss: 2.8785 - val_loss: 2.7498 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "14/14 - 1s - 49ms/step - loss: 2.7957 - val_loss: 2.6952 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.7201 - val_loss: 2.6692 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "14/14 - 1s - 49ms/step - loss: 2.6643 - val_loss: 2.5556 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.5910 - val_loss: 2.5064 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.5173 - val_loss: 2.4961 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.4743 - val_loss: 2.5075 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "14/14 - 1s - 49ms/step - loss: 2.4137 - val_loss: 2.4026 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.3499 - val_loss: 2.2666 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.3089 - val_loss: 2.1810 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "14/14 - 1s - 51ms/step - loss: 2.2373 - val_loss: 2.2310 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.1919 - val_loss: 2.1899 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "14/14 - 1s - 46ms/step - loss: 2.1570 - val_loss: 2.0869 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.1026 - val_loss: 2.1616 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "14/14 - 1s - 47ms/step - loss: 2.0533 - val_loss: 1.9787 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "14/14 - 1s - 48ms/step - loss: 2.0108 - val_loss: 2.0707 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.9858 - val_loss: 2.0792 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.9432 - val_loss: 1.9062 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.8882 - val_loss: 1.9762 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.8645 - val_loss: 2.0297 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.8276 - val_loss: 2.0488 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.7814 - val_loss: 1.7768 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.7477 - val_loss: 2.1862 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.7292 - val_loss: 1.7005 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.7261 - val_loss: 1.9252 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.6762 - val_loss: 1.8248 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.6512 - val_loss: 1.7953 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.6323 - val_loss: 1.7251 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.6236 - val_loss: 1.6564 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5776 - val_loss: 1.8828 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.5600 - val_loss: 1.5512 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.5354 - val_loss: 1.9011 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.5253 - val_loss: 1.8841 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.4882 - val_loss: 1.8455 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.4933 - val_loss: 1.8605 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4587 - val_loss: 1.7285 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.4370 - val_loss: 1.5829 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.4178 - val_loss: 1.5823 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.3918 - val_loss: 1.5541 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.3905 - val_loss: 1.7470 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.3545 - val_loss: 1.5494 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.3363 - val_loss: 1.3920 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "14/14 - 1s - 53ms/step - loss: 1.3391 - val_loss: 2.1377 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "14/14 - 1s - 54ms/step - loss: 1.3354 - val_loss: 1.4057 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "14/14 - 1s - 50ms/step - loss: 1.2883 - val_loss: 1.6160 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "14/14 - 1s - 53ms/step - loss: 1.2778 - val_loss: 1.7560 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.2729 - val_loss: 1.5179 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.2544 - val_loss: 1.3481 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.2784 - val_loss: 1.6691 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.2766 - val_loss: 1.1600 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.2676 - val_loss: 1.9333 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.2257 - val_loss: 1.3104 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.2180 - val_loss: 1.6387 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.1896 - val_loss: 1.6054 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.1730 - val_loss: 1.4336 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.1529 - val_loss: 1.4320 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.1552 - val_loss: 1.4739 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.1543 - val_loss: 1.3569 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.1376 - val_loss: 1.5735 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "14/14 - 1s - 47ms/step - loss: 1.1232 - val_loss: 1.4068 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.1105 - val_loss: 1.5496 - learning_rate: 2.0000e-05\n",
      "Epoch 89/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.1093 - val_loss: 1.7201 - learning_rate: 2.0000e-05\n",
      "Epoch 90/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.1133 - val_loss: 1.4581 - learning_rate: 2.0000e-05\n",
      "Epoch 91/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.1041 - val_loss: 1.6089 - learning_rate: 2.0000e-05\n",
      "Epoch 92/100\n",
      "14/14 - 1s - 49ms/step - loss: 1.1057 - val_loss: 1.5085 - learning_rate: 2.0000e-05\n",
      "Epoch 93/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.1033 - val_loss: 1.7860 - learning_rate: 2.0000e-05\n",
      "Epoch 94/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.0993 - val_loss: 1.6333 - learning_rate: 2.0000e-05\n",
      "Epoch 95/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.0826 - val_loss: 1.4652 - learning_rate: 2.0000e-05\n",
      "Epoch 96/100\n",
      "14/14 - 1s - 46ms/step - loss: 1.0974 - val_loss: 1.5793 - learning_rate: 2.0000e-05\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 - 1s - 47ms/step - loss: 1.0819 - val_loss: 1.8207 - learning_rate: 2.0000e-05\n",
      "Epoch 98/100\n",
      "14/14 - 1s - 50ms/step - loss: 1.0987 - val_loss: 1.4277 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "14/14 - 1s - 47ms/step - loss: 1.0832 - val_loss: 1.6519 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "14/14 - 1s - 48ms/step - loss: 1.0863 - val_loss: 1.6806 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Preparar los datos por cluster\n",
    "for cluster in range(n_clusters):\n",
    "    print(f'Entrenando cluster numero: {cluster}')\n",
    "    cluster_data = grouped_dff[grouped_dff['cluster'] == cluster].copy()\n",
    "    cluster_data.sort_values(by='periodo', inplace=True)\n",
    "    \n",
    "    # Preparar los datos para LSTM\n",
    "    X, y = [], []\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn_scaled','suma_total_2019_scaled','suma_total_2018_scaled','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']].values\n",
    "        if len(series) > 2:  # Asegurarse de que haya suficientes datos\n",
    "            X.append(series[:-2])  # Todos los datos excepto los últimos 2\n",
    "            y.append(series[-1, 0])  # Selecciona solo tn_scaled como objetivo\n",
    "\n",
    "    # Pad y reshape de X\n",
    "    max_len = max(len(seq) for seq in X)\n",
    "    X_padded = np.array([np.pad(seq, ((max_len - len(seq), 0), (0, 0)), mode='constant') for seq in X]).astype(np.float32)\n",
    "    y = np.array(y).astype(np.float32)\n",
    "    \n",
    "    if len(X_padded) == 0 or len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_model((X_padded.shape[1], X_padded.shape[2]))\n",
    "    model.fit(X_padded, y, epochs=200, verbose=2, batch_size=128, validation_split=0.2, callbacks=[reduce_lr])\n",
    "    models[cluster] = model\n",
    "\n",
    "    # Hacer predicciones\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn_scaled','suma_total_2019_scaled','suma_total_2018_scaled','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']].values\n",
    "        \n",
    "        if len(series) > 2:\n",
    "            max_len = len(series) - 1\n",
    "            X_pred = np.pad(series[1:], ((max_len - len(series[1:]), 0), (0, 0)), mode='constant').astype(np.float32)\n",
    "            X_pred = np.reshape(X_pred, (1, X_pred.shape[0], X_pred.shape[1]))\n",
    "            pred = model.predict(X_pred, verbose=0)\n",
    "            predictions.append([key[0], key[1], key[2], key[3], pred[0][0], key[4]])\n",
    "\n",
    "    # Guardar las predicciones en un DataFrame y exportar a CSV\n",
    "    pred_dff_temp = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediccion', 'customer_id'])\n",
    "    pred_dff_temp.to_csv(f\"predicciones_temporales_cluster_{cluster}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>prediccion</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.002434</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>10106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.002435</td>\n",
       "      <td>10144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1.002436</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  prediccion  customer_id\n",
       "0     0     0     4     22    1.002434        10001\n",
       "1     0     0     4     22    1.002435        10032\n",
       "2     0     0     4     22    1.002435        10106\n",
       "3     0     0     4     22    1.002435        10144\n",
       "4     0     0     4     22    1.002436        10225"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dff_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dff_temp.to_csv(\"C:/Users/Usuario/desktop/vero2/pred_df_temp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>prediccion</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.409166</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.409168</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.409173</td>\n",
       "      <td>10106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.409173</td>\n",
       "      <td>10144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.409177</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  prediccion  customer_id\n",
       "0     0     0     4     22    7.409166        10001\n",
       "1     0     0     4     22    7.409168        10032\n",
       "2     0     0     4     22    7.409173        10106\n",
       "3     0     0     4     22    7.409173        10144\n",
       "4     0     0     4     22    7.409177        10225"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dff = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediccion', 'customer_id'])\n",
    "# Suponiendo que `scaler` es el objeto StandardScaler que usaste para escalar `tn` originalmente\n",
    "\n",
    "# Desescalar las predicciones\n",
    "predicciones_desescaladas = scaler.inverse_transform(pred_dff['prediccion'].values.reshape(-1, 1))\n",
    "\n",
    "# Reemplazar las predicciones escaladas con las desescaladas en el DataFrame\n",
    "pred_dff['prediccion'] = predicciones_desescaladas.flatten()\n",
    "\n",
    "# Ahora pred_df tiene las predicciones desescaladas en la columna 'prediccion'\n",
    "pred_dff.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat1  cat2  cat3  brand  prediccion  customer_id  product_id\n",
      "0     0     0     4     22    7.409166        10001       20609\n",
      "1     0     0     4     22    7.409168        10032       20609\n",
      "2     0     0     4     22    7.409173        10106       20609\n",
      "3     0     0     4     22    7.409173        10144       20609\n",
      "4     0     0     4     22    7.409177        10225       20609\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Obtener combinaciones únicas de 'product_id', 'cat1', 'cat2', 'cat3', 'brand' del DataFrame original\n",
    "distinct_combinations = df[['product_id', 'cat1', 'cat2', 'cat3', 'brand']].drop_duplicates()\n",
    "\n",
    "# Paso 2: Realizar un join con pred_df en las columnas correspondientes\n",
    "pred_dff = pred_dff.merge(distinct_combinations, on=['cat1', 'cat2', 'cat3', 'brand'], how='left')\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones y las nuevas columnas agregadas\n",
    "print(pred_dff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat1  cat2  cat3  brand  prediccion  customer_id  product_id  \\\n",
      "0     0     0     4     22    7.409166        10001       20609   \n",
      "1     0     0     4     22    7.409168        10032       20609   \n",
      "2     0     0     4     22    7.409173        10106       20609   \n",
      "3     0     0     4     22    7.409173        10144       20609   \n",
      "4     0     0     4     22    7.409177        10225       20609   \n",
      "\n",
      "   prediccion_ajustada  \n",
      "0             7.409166  \n",
      "1             7.409168  \n",
      "2             7.409173  \n",
      "3             7.409173  \n",
      "4             7.409177  \n"
     ]
    }
   ],
   "source": [
    "# Inicializar una lista para almacenar las predicciones ajustadas\n",
    "predictions_adjusted = []\n",
    "\n",
    "# Iterar sobre las filas de pred_df\n",
    "for index, row in pred_dff.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'], row['product_id'])\n",
    "    \n",
    "    # Buscar el ratio correspondiente en ratio_dict utilizando la clave correcta\n",
    "    if key in ratio_dict:\n",
    "        ratio = ratio_dict[key]\n",
    "        # Calcular la predicción ajustada\n",
    "        prediccion_ajustada = row['prediccion'] * ratio\n",
    "        predictions_adjusted.append(prediccion_ajustada)\n",
    "    else:\n",
    "        predictions_adjusted.append(row['prediccion'])  # Mantener la predicción original si no hay ratio definido\n",
    "\n",
    "# Agregar las predicciones ajustadas al DataFrame pred_df\n",
    "pred_dff['prediccion_ajustada'] = predictions_adjusted\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones ajustadas\n",
    "print(pred_dff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product_id  prediccion_ajustada\n",
      "0         20001          2499.465930\n",
      "1         20002          2510.942963\n",
      "2         20003          3151.594747\n",
      "3         20004          3151.594747\n",
      "4         20005          3151.594747\n",
      "..          ...                  ...\n",
      "775       21263           449.055525\n",
      "776       21265           892.007104\n",
      "777       21266           892.007104\n",
      "778       21267           179.697158\n",
      "779       21276           179.697158\n",
      "\n",
      "[780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sumarizar las predicciones ajustadas por product_id\n",
    "summarized_predictions = pred_dff.groupby('product_id')['prediccion_ajustada'].sum().reset_index()\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones sumarizadas\n",
    "print(summarized_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las predicciones finales en un archivo CSV\n",
    "summarized_predictions.to_csv(\"C:/Users/Usuario/desktop/vero2/modelodtw2002.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
