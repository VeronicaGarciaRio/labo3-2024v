{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en esta version cambio el modelo para evitar sobreajuste, incorporo a los clusters y a la red 4 variables nuevas: dif entre dic y feb, totales 2019 y 2018. mas capas al lstm. 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "import joblib\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense, Dropout, Activation\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Importa EarlyStopping desde callbacks\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>439.90647</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>65.92436</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>144.78714</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>33.63991</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>111.51691</td>\n",
       "      <td>109.05244</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>178.49426</td>\n",
       "      <td>176.02980</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>240.59870</td>\n",
       "      <td>236.65556</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    customer_id  product_id  periodo  plan_precios_cuidados  cust_request_qty  \\\n",
       "0         10001       20001   201812                    0.0              20.0   \n",
       "1         10001       20001   201901                    0.0              53.0   \n",
       "2         10001       20001   201902                    0.0              39.0   \n",
       "3         10001       20001   201903                    0.0              23.0   \n",
       "4         10001       20001   201904                    0.0              33.0   \n",
       "5         10001       20001   201905                    0.0              31.0   \n",
       "6         10001       20001   201906                    0.0               7.0   \n",
       "7         10001       20001   201907                    0.0              14.0   \n",
       "8         10001       20001   201908                    0.0               9.0   \n",
       "9         10001       20001   201909                    0.0              18.0   \n",
       "10        10001       20001   201910                    0.0              21.0   \n",
       "11        10001       20001   201911                    0.0              21.0   \n",
       "\n",
       "    cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0         254.62373  254.62373   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1         393.26092  386.60688   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2         309.90610  309.90610   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3         142.87158  130.54927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4         364.37071  364.37071   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "5         439.90647  439.90647   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "6          65.92436   65.92436   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "7         144.78714  144.78714   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "8          33.63991   33.63991   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "9         111.51691  109.05244   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "10        178.49426  176.02980   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "11        240.59870  236.65556   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "   descripcion quarter  month  close_quarter   age mes_inicial  \n",
       "0       genoma      Q4     12            1.0  23.0  2018-12-01  \n",
       "1       genoma      Q1      1            0.0  24.0  2018-12-01  \n",
       "2       genoma      Q1      2            0.0  25.0  2018-12-01  \n",
       "3       genoma      Q1      3            1.0  26.0  2018-12-01  \n",
       "4       genoma      Q2      4            0.0  27.0  2018-12-01  \n",
       "5       genoma      Q2      5            0.0  28.0  2018-12-01  \n",
       "6       genoma      Q2      6            1.0  29.0  2018-12-01  \n",
       "7       genoma      Q3      7            0.0  30.0  2018-12-01  \n",
       "8       genoma      Q3      8            0.0  31.0  2018-12-01  \n",
       "9       genoma      Q3      9            1.0  32.0  2018-12-01  \n",
       "10      genoma      Q4     10            0.0  33.0  2018-12-01  \n",
       "11      genoma      Q4     11            0.0  34.0  2018-12-01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar y preprocesar los datos\n",
    "file_path = \"C:/Users/Usuario/desktop/vero2/final_dataset_completo_con_ceros.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head(12)\n",
    "\n",
    "# filtered_df = df[df['product_id'].between(20001, 20012)]\n",
    "\n",
    "# Opcional: Ver las primeras filas del DataFrame filtrado\n",
    "# print(filtered_df.head())\n",
    "\n",
    "# df=filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'product_id', 'periodo', 'plan_precios_cuidados',\n",
       "       'cust_request_qty', 'cust_request_tn', 'tn', 'cat1', 'cat2', 'cat3',\n",
       "       'brand', 'sku_size', 'descripcion', 'quarter', 'month', 'close_quarter',\n",
       "       'age', 'mes_inicial'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], var_name='periodo', value_name='tn')\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas categóricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Agrupar las ventas por periodo, cat1, cat2, cat3, brand y customer_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m', errors='coerce')\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({'tn': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "# Paso 2: Calcular los ratios incluyendo customer_id\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 12)]\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['tn'].sum().reset_index()\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index()\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['ratio'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.87535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10002</td>\n",
       "      <td>0.27780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10003</td>\n",
       "      <td>0.27256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10004</td>\n",
       "      <td>0.13628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10005</td>\n",
       "      <td>0.06290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     periodo  cat1  cat2  cat3  brand  customer_id       tn\n",
       "0 2018-12-01     0     0     4     22        10001  0.87535\n",
       "1 2018-12-01     0     0     4     22        10002  0.27780\n",
       "2 2018-12-01     0     0     4     22        10003  0.27256\n",
       "3 2018-12-01     0     0     4     22        10004  0.13628\n",
       "4 2018-12-01     0     0     4     22        10005  0.06290"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434443</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434444</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434445</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434446</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434447</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434448 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0          0     0     4     22        10001                           0.0   \n",
       "1          0     0     4     22        10001                           0.0   \n",
       "2          0     0     4     22        10001                           0.0   \n",
       "3          0     0     4     22        10001                           0.0   \n",
       "4          0     0     4     22        10001                           0.0   \n",
       "...      ...   ...   ...    ...          ...                           ...   \n",
       "434443     3    13    82     32        10552                           0.0   \n",
       "434444     3    13    82     32        10552                           0.0   \n",
       "434445     3    13    82     32        10552                           0.0   \n",
       "434446     3    13    82     32        10552                           0.0   \n",
       "434447     3    13    82     32        10552                           0.0   \n",
       "\n",
       "        variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019  \\\n",
       "0                           -0.60803          0.87535          3.28124   \n",
       "1                           -0.60803          0.87535          3.28124   \n",
       "2                           -0.60803          0.87535          3.28124   \n",
       "3                           -0.60803          0.87535          3.28124   \n",
       "4                           -0.60803          0.87535          3.28124   \n",
       "...                              ...              ...              ...   \n",
       "434443                       0.00000          0.00000          0.00146   \n",
       "434444                       0.00000          0.00000          0.00146   \n",
       "434445                       0.00000          0.00000          0.00146   \n",
       "434446                       0.00000          0.00000          0.00146   \n",
       "434447                       0.00000          0.00000          0.00146   \n",
       "\n",
       "             tn    periodo  \n",
       "0       0.87535 2018-12-01  \n",
       "1       0.30925 2019-01-01  \n",
       "2       0.26732 2019-02-01  \n",
       "3       0.38264 2019-03-01  \n",
       "4       0.06290 2019-04-01  \n",
       "...         ...        ...  \n",
       "434443  0.00000 2019-08-01  \n",
       "434444  0.00000 2019-09-01  \n",
       "434445  0.00000 2019-10-01  \n",
       "434446  0.00000 2019-11-01  \n",
       "434447  0.00000 2019-12-01  \n",
       "\n",
       "[434448 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificar que la columna 'periodo' exista en el DataFrame\n",
    "if 'periodo' in grouped_df.columns:\n",
    "    # Asegúrate de que la columna 'periodo' esté en formato de fecha\n",
    "    grouped_df['periodo'] = pd.to_datetime(grouped_df['periodo'])\n",
    "\n",
    "    # Filtrar los datos necesarios para los cálculos\n",
    "    df_dic2017 = grouped_df[grouped_df['periodo'] == '2017-12-01']\n",
    "    df_feb2018 = grouped_df[grouped_df['periodo'] == '2018-02-01']\n",
    "    df_dic2018 = grouped_df[grouped_df['periodo'] == '2018-12-01']\n",
    "    df_feb2019 = grouped_df[grouped_df['periodo'] == '2019-02-01']\n",
    "    df_2018 = grouped_df[grouped_df['periodo'].dt.year == 2018]\n",
    "    df_2019 = grouped_df[grouped_df['periodo'].dt.year == 2019]\n",
    "\n",
    "    # Merge para obtener las variaciones\n",
    "    merged_2018 = pd.merge(df_feb2018, df_dic2017, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('_feb2018', '_dic2017'))\n",
    "    merged_2019 = pd.merge(df_feb2019, df_dic2018, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('_feb2019', '_dic2018'))\n",
    "\n",
    "    # Calcular las variaciones\n",
    "    merged_2018['variacion_feb2018_vs_dic2017'] = merged_2018['tn_feb2018'] - merged_2018['tn_dic2017']\n",
    "    merged_2019['variacion_feb2019_vs_dic2018'] = merged_2019['tn_feb2019'] - merged_2019['tn_dic2018']\n",
    "\n",
    "    # Agrupar y sumar para obtener las sumas totales de tn para 2018 y 2019\n",
    "    sum_2018 = df_2018.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index().rename(columns={'tn': 'suma_total_2018'})\n",
    "    sum_2019 = df_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index().rename(columns={'tn': 'suma_total_2019'})\n",
    "\n",
    "    # Unir todos los resultados en un solo DataFrame\n",
    "    final_df = merged_2018[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2018_vs_dic2017']].merge(\n",
    "        merged_2019[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2019_vs_dic2018']],\n",
    "        on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    "    ).merge(\n",
    "        sum_2018, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    "    ).merge(\n",
    "        sum_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='outer'\n",
    "    )\n",
    "\n",
    "    # Rellenar NaNs con 0\n",
    "    final_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Agregar las columnas 'tn' y 'periodo' originales\n",
    "    final_df = final_df.merge(grouped_df[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn', 'periodo']], on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], how='left')\n",
    "\n",
    "    # Mostrar el DataFrame final\n",
    "    display(final_df)\n",
    "else:\n",
    "    print(\"La columna 'periodo' no existe en el DataFrame 'grouped_df'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0     0     0     4     22        10001                           0.0   \n",
       "1     0     0     4     22        10001                           0.0   \n",
       "2     0     0     4     22        10001                           0.0   \n",
       "3     0     0     4     22        10001                           0.0   \n",
       "4     0     0     4     22        10001                           0.0   \n",
       "\n",
       "   variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019       tn  \\\n",
       "0                      -0.60803          0.87535          3.28124  0.87535   \n",
       "1                      -0.60803          0.87535          3.28124  0.30925   \n",
       "2                      -0.60803          0.87535          3.28124  0.26732   \n",
       "3                      -0.60803          0.87535          3.28124  0.38264   \n",
       "4                      -0.60803          0.87535          3.28124  0.06290   \n",
       "\n",
       "     periodo  \n",
       "0 2018-12-01  \n",
       "1 2019-01-01  \n",
       "2 2019-02-01  \n",
       "3 2019-03-01  \n",
       "4 2019-04-01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer las semillas para numpy, random y tensorflow/keras\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>2018-12-01</th>\n",
       "      <th>2019-01-01</th>\n",
       "      <th>2019-02-01</th>\n",
       "      <th>2019-03-01</th>\n",
       "      <th>2019-04-01</th>\n",
       "      <th>2019-05-01</th>\n",
       "      <th>2019-06-01</th>\n",
       "      <th>2019-07-01</th>\n",
       "      <th>2019-08-01</th>\n",
       "      <th>2019-09-01</th>\n",
       "      <th>2019-10-01</th>\n",
       "      <th>2019-11-01</th>\n",
       "      <th>2019-12-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">22</th>\n",
       "      <th>10001</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.60803</th>\n",
       "      <th>0.87535</th>\n",
       "      <th>3.281240</th>\n",
       "      <td>2.180914</td>\n",
       "      <td>-0.041167</td>\n",
       "      <td>-0.205752</td>\n",
       "      <td>0.246907</td>\n",
       "      <td>-1.008151</td>\n",
       "      <td>0.102890</td>\n",
       "      <td>-1.255048</td>\n",
       "      <td>1.275635</td>\n",
       "      <td>0.802407</td>\n",
       "      <td>0.329180</td>\n",
       "      <td>0.082282</td>\n",
       "      <td>-1.255048</td>\n",
       "      <td>-1.255048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.27780</th>\n",
       "      <th>0.27780</th>\n",
       "      <th>1.323515</th>\n",
       "      <td>1.119645</td>\n",
       "      <td>-0.398540</td>\n",
       "      <td>-0.891954</td>\n",
       "      <td>-0.891954</td>\n",
       "      <td>-0.891954</td>\n",
       "      <td>1.195605</td>\n",
       "      <td>-0.891954</td>\n",
       "      <td>1.613131</td>\n",
       "      <td>0.569351</td>\n",
       "      <td>-0.474428</td>\n",
       "      <td>1.575115</td>\n",
       "      <td>-0.740107</td>\n",
       "      <td>-0.891954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.27256</th>\n",
       "      <th>0.27256</th>\n",
       "      <th>2.521215</th>\n",
       "      <td>0.258112</td>\n",
       "      <td>-0.046944</td>\n",
       "      <td>-0.962110</td>\n",
       "      <td>0.868222</td>\n",
       "      <td>1.478378</td>\n",
       "      <td>1.478378</td>\n",
       "      <td>-0.962110</td>\n",
       "      <td>-0.962110</td>\n",
       "      <td>0.258134</td>\n",
       "      <td>1.478378</td>\n",
       "      <td>-0.962110</td>\n",
       "      <td>-0.962110</td>\n",
       "      <td>-0.962110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.40885</th>\n",
       "      <th>0.13628</th>\n",
       "      <th>5.417185</th>\n",
       "      <td>-1.155997</td>\n",
       "      <td>-0.614457</td>\n",
       "      <td>0.468663</td>\n",
       "      <td>0.739433</td>\n",
       "      <td>-0.343686</td>\n",
       "      <td>-1.155997</td>\n",
       "      <td>1.551744</td>\n",
       "      <td>-0.614457</td>\n",
       "      <td>0.333258</td>\n",
       "      <td>1.280973</td>\n",
       "      <td>1.280973</td>\n",
       "      <td>-0.072916</td>\n",
       "      <td>-1.697537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.07338</th>\n",
       "      <th>0.06290</th>\n",
       "      <th>0.815065</th>\n",
       "      <td>-0.128474</td>\n",
       "      <td>-0.564409</td>\n",
       "      <td>1.905152</td>\n",
       "      <td>-0.418913</td>\n",
       "      <td>-0.418913</td>\n",
       "      <td>0.161965</td>\n",
       "      <td>0.452681</td>\n",
       "      <td>-0.564409</td>\n",
       "      <td>0.670371</td>\n",
       "      <td>1.905152</td>\n",
       "      <td>-0.273693</td>\n",
       "      <td>-1.000068</td>\n",
       "      <td>-1.726442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">13</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">82</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">32</th>\n",
       "      <th>10363</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.000000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10367</th>\n",
       "      <th>0.0</th>\n",
       "      <th>-0.00146</th>\n",
       "      <th>0.00146</th>\n",
       "      <th>0.001460</th>\n",
       "      <td>2.345208</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>2.345208</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10482</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.001460</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10513</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.000730</th>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10552</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.00000</th>\n",
       "      <th>0.001460</th>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>2.345208</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>2.345208</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "      <td>-0.426401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40383 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "periodo                                                                                                                     2018-12-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           2.180914   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515           1.119645   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           0.258112   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -1.155997   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.128474   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460           2.345208   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-01-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -0.041167   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.398540   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.046944   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -0.614457   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.564409   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-02-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -0.205752   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.891954   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           0.468663   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065           1.905152   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-03-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           0.246907   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.891954   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           0.868222   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           0.739433   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.418913   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-04-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -1.008151   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.891954   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           1.478378   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -0.343686   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.418913   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460           2.345208   \n",
       "\n",
       "periodo                                                                                                                     2019-05-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           0.102890   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515           1.195605   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           1.478378   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -1.155997   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065           0.161965   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460           3.464102   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-06-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -1.255048   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.891954   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           1.551744   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065           0.452681   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460           2.345208   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460           2.345208   \n",
       "\n",
       "periodo                                                                                                                     2019-07-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           1.275635   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515           1.613131   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -0.614457   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.564409   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-08-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           0.802407   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515           0.569351   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           0.258134   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           0.333258   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065           0.670371   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-09-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           0.329180   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.474428   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215           1.478378   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           1.280973   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065           1.905152   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730           3.464102   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-10-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240           0.082282   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515           1.575115   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185           1.280973   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -0.273693   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-11-01  \\\n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019               \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -1.255048   \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.740107   \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110   \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -0.072916   \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -1.000068   \n",
       "...                                                                                                                                ...   \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000   \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401   \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675   \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675   \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401   \n",
       "\n",
       "periodo                                                                                                                     2019-12-01  \n",
       "cat1 cat2 cat3 brand customer_id variacion_feb2018_vs_dic2017 variacion_feb2019_vs_dic2018 suma_total_2018 suma_total_2019              \n",
       "0    0    4    22    10001       0.0                          -0.60803                     0.87535         3.281240          -1.255048  \n",
       "                     10002       0.0                          -0.27780                     0.27780         1.323515          -0.891954  \n",
       "                     10003       0.0                          -0.27256                     0.27256         2.521215          -0.962110  \n",
       "                     10004       0.0                           0.40885                     0.13628         5.417185          -1.697537  \n",
       "                     10005       0.0                           0.07338                     0.06290         0.815065          -1.726442  \n",
       "...                                                                                                                                ...  \n",
       "3    13   82   32    10363       0.0                           0.00000                     0.00000         0.000000           0.000000  \n",
       "                     10367       0.0                          -0.00146                     0.00146         0.001460          -0.426401  \n",
       "                     10482       0.0                           0.00000                     0.00000         0.001460          -0.288675  \n",
       "                     10513       0.0                           0.00000                     0.00000         0.000730          -0.288675  \n",
       "                     10552       0.0                           0.00000                     0.00000         0.001460          -0.426401  \n",
       "\n",
       "[40383 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import joblib\n",
    "\n",
    "# Pivotear el DataFrame\n",
    "pivoted_df = final_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'variacion_feb2018_vs_dic2017', 'variacion_feb2019_vs_dic2018', 'suma_total_2018', 'suma_total_2019'], columns='periodo', values='tn').fillna(0)\n",
    "\n",
    "# Escalar las series temporales\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)  # normalizar las series temporales\n",
    "scaled_series = scaler.fit_transform(pivoted_df.values)\n",
    "\n",
    "# Guardar el scaler para su uso posterior\n",
    "joblib.dump(scaler, 'timeseries_scaler.pkl')\n",
    "\n",
    "# Aplanar las series escaladas para crear un DataFrame\n",
    "n_samples, n_timesteps, n_features = scaled_series.shape\n",
    "flat_scaled_series = scaled_series.reshape((n_samples, n_timesteps * n_features))\n",
    "\n",
    "# Crear un DataFrame escalado para mostrar\n",
    "scaled_df = pd.DataFrame(flat_scaled_series, index=pivoted_df.index, columns=pivoted_df.columns)\n",
    "\n",
    "# Mostrar el DataFrame final escalado\n",
    "display(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434443</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434444</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434445</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434446</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434447</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>10552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434448 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0          0     0     4     22        10001                           0.0   \n",
       "1          0     0     4     22        10001                           0.0   \n",
       "2          0     0     4     22        10001                           0.0   \n",
       "3          0     0     4     22        10001                           0.0   \n",
       "4          0     0     4     22        10001                           0.0   \n",
       "...      ...   ...   ...    ...          ...                           ...   \n",
       "434443     3    13    82     32        10552                           0.0   \n",
       "434444     3    13    82     32        10552                           0.0   \n",
       "434445     3    13    82     32        10552                           0.0   \n",
       "434446     3    13    82     32        10552                           0.0   \n",
       "434447     3    13    82     32        10552                           0.0   \n",
       "\n",
       "        variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019  \\\n",
       "0                           -0.60803          0.87535          3.28124   \n",
       "1                           -0.60803          0.87535          3.28124   \n",
       "2                           -0.60803          0.87535          3.28124   \n",
       "3                           -0.60803          0.87535          3.28124   \n",
       "4                           -0.60803          0.87535          3.28124   \n",
       "...                              ...              ...              ...   \n",
       "434443                       0.00000          0.00000          0.00146   \n",
       "434444                       0.00000          0.00000          0.00146   \n",
       "434445                       0.00000          0.00000          0.00146   \n",
       "434446                       0.00000          0.00000          0.00146   \n",
       "434447                       0.00000          0.00000          0.00146   \n",
       "\n",
       "             tn    periodo  cluster  \n",
       "0       0.87535 2018-12-01        0  \n",
       "1       0.30925 2019-01-01        0  \n",
       "2       0.26732 2019-02-01        0  \n",
       "3       0.38264 2019-03-01        0  \n",
       "4       0.06290 2019-04-01        0  \n",
       "...         ...        ...      ...  \n",
       "434443  0.00000 2019-08-01       12  \n",
       "434444  0.00000 2019-09-01       12  \n",
       "434445  0.00000 2019-10-01       12  \n",
       "434446  0.00000 2019-11-01       12  \n",
       "434447  0.00000 2019-12-01       12  \n",
       "\n",
       "[434448 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import joblib\n",
    "\n",
    "# Supongamos que `grouped_df` es tu DataFrame original con las columnas necesarias\n",
    "\n",
    "# Procesamiento previo similar al anterior código...\n",
    "# (Se omiten las líneas anteriores para brevedad)\n",
    "\n",
    "# Crear el modelo TimeSeriesKMeans\n",
    "n_clusters = 15\n",
    "model = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "\n",
    "# Ajustar el modelo usando las series temporales escaladas\n",
    "model.fit(scaled_df)\n",
    "\n",
    "# Obtener los clusters asignados a cada serie temporal\n",
    "clusters = model.labels_\n",
    "\n",
    "# Agregar los clusters al DataFrame pivoteado\n",
    "pivoted_df['cluster'] = clusters\n",
    "\n",
    "# Reiniciar el índice para que `pivoted_df` tenga los índices como columnas normales\n",
    "pivoted_df.reset_index(inplace=True)\n",
    "\n",
    "# Fusionar `final_df` con `pivoted_df` usando `left_on` y `right_on`\n",
    "grouped_dff = final_df.merge(pivoted_df[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'cluster']], \n",
    "                             on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], \n",
    "                             how='left')\n",
    "\n",
    "# Mostrar el DataFrame final con los números de grupo\n",
    "display(grouped_dff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dff.to_csv(\"C:/Users/Usuario/desktop/vero2/grouped_dff.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
      "0         0     0     4     22        10001                           0.0   \n",
      "1         0     0     4     22        10002                           0.0   \n",
      "2         0     0     4     22        10003                           0.0   \n",
      "3         0     0     4     22        10004                           0.0   \n",
      "4         0     0     4     22        10005                           0.0   \n",
      "...     ...   ...   ...    ...          ...                           ...   \n",
      "40378     3    13    82     32        10363                           0.0   \n",
      "40379     3    13    82     32        10367                           0.0   \n",
      "40380     3    13    82     32        10482                           0.0   \n",
      "40381     3    13    82     32        10513                           0.0   \n",
      "40382     3    13    82     32        10552                           0.0   \n",
      "\n",
      "       variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019  cluster  \n",
      "0                          -0.60803          0.87535         3.281240        0  \n",
      "1                          -0.27780          0.27780         1.323515       11  \n",
      "2                          -0.27256          0.27256         2.521215       11  \n",
      "3                           0.40885          0.13628         5.417185        7  \n",
      "4                           0.07338          0.06290         0.815065       12  \n",
      "...                             ...              ...              ...      ...  \n",
      "40378                       0.00000          0.00000         0.000000        5  \n",
      "40379                      -0.00146          0.00146         0.001460       14  \n",
      "40380                       0.00000          0.00000         0.001460        6  \n",
      "40381                       0.00000          0.00000         0.000730        6  \n",
      "40382                       0.00000          0.00000         0.001460       12  \n",
      "\n",
      "[40383 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#control\n",
    "# Dropear la columna 'periodo'\n",
    "check = grouped_dff.drop(columns=['periodo'])\n",
    "\n",
    "# Agrupar por 'cat1', 'cat2', 'cat3', 'brand', 'customer_id' y obtener el cluster asignado\n",
    "result_df = check.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018','suma_total_2018','suma_total_2019']).agg({\n",
    "    'cluster': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [cat1, cat2, cat3, brand, customer_id, variacion_feb2018_vs_dic2017, variacion_feb2019_vs_dic2018, suma_total_2018, suma_total_2019, cluster]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'cat1', 'cat2', 'cat3', 'brand', 'customer_id' y contar clusters únicos\n",
    "cluster_counts = check.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018','suma_total_2018','suma_total_2019']).agg({\n",
    "    'cluster': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "# Filtrar combinaciones con más de un cluster\n",
    "multiple_clusters = cluster_counts[cluster_counts['cluster'] > 1]\n",
    "\n",
    "# Mostrar combinaciones con más de un cluster\n",
    "print(multiple_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hasta aca ok con grouped_dff guardado en vero2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para acumular las filas de resultados\n",
    "resultados_por_producto = []\n",
    "\n",
    "# Elegir un número de pasos de tiempo\n",
    "n_steps = 13  # Ventana de tiempo de 18 meses\n",
    "n_features = 10 # Cambia esto si tienes más características\n",
    "step_ahead= 2\n",
    "\n",
    "def crear_secuencias(datos, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(datos) - n_steps - step_ahead + 1):\n",
    "        end_ix = i + n_steps\n",
    "        out_end_ix = end_ix + step_ahead - 1\n",
    "        seq_x, seq_y = datos[i:end_ix], datos[out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=256, return_sequences=True, kernel_regularizer=l2(0.001)), input_shape=input_shape))\n",
    "    model.add(Dropout(0.4))  # Adjusted dropout rate\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "    model.add(Dropout(0.4))  # Adjusted dropout rate\n",
    "    model.add(Bidirectional(LSTM(units=64, kernel_regularizer=l2(0.001))))\n",
    "    model.add(Dropout(0.4))  # Adjusted dropout rate\n",
    "    model.add(Dense(64, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')  # Adjusted learning rate\n",
    "    return model\n",
    "\n",
    "# Use the updated early_stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reescalar 'tn' en grouped_df\n",
    "scaler = StandardScaler()\n",
    "grouped_dff['tn_scaled'] = scaler.fit_transform(grouped_dff[['tn']])\n",
    "grouped_dff['suma_total_2019_scaled'] = scaler.fit_transform(grouped_dff[['suma_total_2019']])\n",
    "grouped_dff['suma_total_2018_scaled'] = scaler.fit_transform(grouped_dff[['suma_total_2018']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora tengo grouped_dff con escalado en tn: se agrega columna tn_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>variacion_feb2018_vs_dic2017</th>\n",
       "      <th>variacion_feb2019_vs_dic2018</th>\n",
       "      <th>suma_total_2018</th>\n",
       "      <th>suma_total_2019</th>\n",
       "      <th>tn</th>\n",
       "      <th>periodo</th>\n",
       "      <th>cluster</th>\n",
       "      <th>tn_scaled</th>\n",
       "      <th>suma_total_2019_scaled</th>\n",
       "      <th>suma_total_2018_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.078741</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.26732</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.084338</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.38264</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.068945</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.60803</td>\n",
       "      <td>0.87535</td>\n",
       "      <td>3.28124</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.111625</td>\n",
       "      <td>-0.093435</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  customer_id  variacion_feb2018_vs_dic2017  \\\n",
       "0     0     0     4     22        10001                           0.0   \n",
       "1     0     0     4     22        10001                           0.0   \n",
       "2     0     0     4     22        10001                           0.0   \n",
       "3     0     0     4     22        10001                           0.0   \n",
       "4     0     0     4     22        10001                           0.0   \n",
       "\n",
       "   variacion_feb2019_vs_dic2018  suma_total_2018  suma_total_2019       tn  \\\n",
       "0                      -0.60803          0.87535          3.28124  0.87535   \n",
       "1                      -0.60803          0.87535          3.28124  0.30925   \n",
       "2                      -0.60803          0.87535          3.28124  0.26732   \n",
       "3                      -0.60803          0.87535          3.28124  0.38264   \n",
       "4                      -0.60803          0.87535          3.28124  0.06290   \n",
       "\n",
       "     periodo  cluster  tn_scaled  suma_total_2019_scaled  \\\n",
       "0 2018-12-01        0  -0.003176               -0.093435   \n",
       "1 2019-01-01        0  -0.078741               -0.093435   \n",
       "2 2019-02-01        0  -0.084338               -0.093435   \n",
       "3 2019-03-01        0  -0.068945               -0.093435   \n",
       "4 2019-04-01        0  -0.111625               -0.093435   \n",
       "\n",
       "   suma_total_2018_scaled  \n",
       "0                0.023427  \n",
       "1                0.023427  \n",
       "2                0.023427  \n",
       "3                0.023427  \n",
       "4                0.023427  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver de dejar solo la fecha y tn y cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          periodo  tn_scaled  suma_total_2019_scaled  suma_total_2018_scaled  \\\n",
      "0      2018-12-01  -0.003176               -0.093435                0.023427   \n",
      "1      2019-01-01  -0.078741               -0.093435                0.023427   \n",
      "2      2019-02-01  -0.084338               -0.093435                0.023427   \n",
      "3      2019-03-01  -0.068945               -0.093435                0.023427   \n",
      "4      2019-04-01  -0.111625               -0.093435                0.023427   \n",
      "...           ...        ...                     ...                     ...   \n",
      "434443 2019-08-01  -0.120021               -0.134593               -0.107733   \n",
      "434444 2019-09-01  -0.120021               -0.134593               -0.107733   \n",
      "434445 2019-10-01  -0.120021               -0.134593               -0.107733   \n",
      "434446 2019-11-01  -0.120021               -0.134593               -0.107733   \n",
      "434447 2019-12-01  -0.120021               -0.134593               -0.107733   \n",
      "\n",
      "        cluster  variacion_feb2018_vs_dic2017  variacion_feb2019_vs_dic2018  \n",
      "0             0                           0.0                      -0.60803  \n",
      "1             0                           0.0                      -0.60803  \n",
      "2             0                           0.0                      -0.60803  \n",
      "3             0                           0.0                      -0.60803  \n",
      "4             0                           0.0                      -0.60803  \n",
      "...         ...                           ...                           ...  \n",
      "434443       12                           0.0                       0.00000  \n",
      "434444       12                           0.0                       0.00000  \n",
      "434445       12                           0.0                       0.00000  \n",
      "434446       12                           0.0                       0.00000  \n",
      "434447       12                           0.0                       0.00000  \n",
      "\n",
      "[434448 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar solo las columnas 'periodo', 'tn' y 'cluster'\n",
    "time_series_dff = grouped_dff[['periodo', 'tn_scaled', 'suma_total_2019_scaled','suma_total_2018_scaled','cluster','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']]\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(time_series_dff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OJO DEJE EPOCHS EN 200!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir dimensiones de X_padded y y antes de entrenar\n",
    "print(f'Dimensiones de X_padded: {X_padded.shape}')\n",
    "print(f'Dimensiones de y: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando cluster numero: 0\n",
      "Epoch 1/200\n",
      "10/10 - 7s - 739ms/step - loss: 2.5726 - val_loss: 1.9833 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "10/10 - 1s - 73ms/step - loss: 2.0145 - val_loss: 2.0489 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.8115 - val_loss: 2.0488 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "10/10 - 1s - 80ms/step - loss: 1.6444 - val_loss: 1.9707 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "10/10 - 1s - 87ms/step - loss: 1.5508 - val_loss: 1.7991 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "10/10 - 1s - 80ms/step - loss: 1.5012 - val_loss: 1.6695 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.4391 - val_loss: 1.9061 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.3780 - val_loss: 1.8661 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.3728 - val_loss: 1.6041 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.3453 - val_loss: 1.4213 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.3065 - val_loss: 1.4209 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.2944 - val_loss: 1.9787 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.2810 - val_loss: 1.6715 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.2606 - val_loss: 2.4094 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.3152 - val_loss: 1.1163 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.2355 - val_loss: 1.9007 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.2043 - val_loss: 1.4601 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.1862 - val_loss: 1.7247 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.1733 - val_loss: 1.3250 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.1506 - val_loss: 1.6191 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.1495 - val_loss: 1.5023 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.1243 - val_loss: 1.8910 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.1535 - val_loss: 1.3843 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.1066 - val_loss: 1.5191 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "10/10 - 1s - 82ms/step - loss: 1.0916 - val_loss: 1.3386 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "10/10 - 1s - 85ms/step - loss: 1.0933 - val_loss: 1.3526 - learning_rate: 2.0000e-04\n",
      "Epoch 27/200\n",
      "10/10 - 1s - 83ms/step - loss: 1.0793 - val_loss: 1.4386 - learning_rate: 2.0000e-04\n",
      "Epoch 28/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0793 - val_loss: 1.6209 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0745 - val_loss: 1.6067 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0754 - val_loss: 1.6216 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0820 - val_loss: 1.7076 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0778 - val_loss: 1.6477 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0776 - val_loss: 1.5096 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0545 - val_loss: 1.4554 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "10/10 - 1s - 74ms/step - loss: 1.0607 - val_loss: 1.4552 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0553 - val_loss: 1.4452 - learning_rate: 4.0000e-05\n",
      "Epoch 37/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0511 - val_loss: 1.4720 - learning_rate: 4.0000e-05\n",
      "Epoch 38/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0526 - val_loss: 1.5136 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0573 - val_loss: 1.5129 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0581 - val_loss: 1.5157 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0483 - val_loss: 1.5244 - learning_rate: 4.0000e-05\n",
      "Epoch 42/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0513 - val_loss: 1.5179 - learning_rate: 4.0000e-05\n",
      "Epoch 43/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0588 - val_loss: 1.4909 - learning_rate: 4.0000e-05\n",
      "Epoch 44/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0551 - val_loss: 1.4793 - learning_rate: 4.0000e-05\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "10/10 - 1s - 71ms/step - loss: 1.0553 - val_loss: 1.5259 - learning_rate: 4.0000e-05\n",
      "Epoch 46/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0516 - val_loss: 1.5254 - learning_rate: 1.0000e-05\n",
      "Epoch 47/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0497 - val_loss: 1.5225 - learning_rate: 1.0000e-05\n",
      "Epoch 48/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0534 - val_loss: 1.5199 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0524 - val_loss: 1.5065 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0550 - val_loss: 1.5040 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0504 - val_loss: 1.5038 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0535 - val_loss: 1.5137 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0434 - val_loss: 1.5057 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0479 - val_loss: 1.4899 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0454 - val_loss: 1.4884 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0474 - val_loss: 1.4846 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0504 - val_loss: 1.4905 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0476 - val_loss: 1.4996 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0481 - val_loss: 1.5057 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0446 - val_loss: 1.5090 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0438 - val_loss: 1.5039 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0488 - val_loss: 1.5066 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0458 - val_loss: 1.5170 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0490 - val_loss: 1.5166 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "10/10 - 1s - 69ms/step - loss: 1.0463 - val_loss: 1.5226 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0482 - val_loss: 1.5187 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0450 - val_loss: 1.5157 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0460 - val_loss: 1.5192 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0470 - val_loss: 1.5273 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0412 - val_loss: 1.5254 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0477 - val_loss: 1.5097 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0443 - val_loss: 1.5011 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0513 - val_loss: 1.4981 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0385 - val_loss: 1.4937 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0472 - val_loss: 1.5000 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0423 - val_loss: 1.5033 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0459 - val_loss: 1.5103 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0406 - val_loss: 1.5125 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0465 - val_loss: 1.5134 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0484 - val_loss: 1.5104 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0425 - val_loss: 1.5082 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0516 - val_loss: 1.5073 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0454 - val_loss: 1.5161 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0492 - val_loss: 1.5208 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0448 - val_loss: 1.5288 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0430 - val_loss: 1.5300 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0430 - val_loss: 1.5380 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0426 - val_loss: 1.5229 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0431 - val_loss: 1.5067 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0446 - val_loss: 1.5025 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0436 - val_loss: 1.5071 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0446 - val_loss: 1.5082 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0449 - val_loss: 1.5042 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0382 - val_loss: 1.4932 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0400 - val_loss: 1.4892 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0448 - val_loss: 1.4836 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0372 - val_loss: 1.4851 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0405 - val_loss: 1.4752 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0470 - val_loss: 1.4676 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0442 - val_loss: 1.4613 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0474 - val_loss: 1.4606 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0442 - val_loss: 1.4669 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0353 - val_loss: 1.4715 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0368 - val_loss: 1.4774 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0317 - val_loss: 1.4841 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0357 - val_loss: 1.4887 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0377 - val_loss: 1.5021 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0413 - val_loss: 1.5077 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0412 - val_loss: 1.4987 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0383 - val_loss: 1.4813 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0388 - val_loss: 1.4719 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0400 - val_loss: 1.4813 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0398 - val_loss: 1.4820 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0382 - val_loss: 1.4703 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0394 - val_loss: 1.4632 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0368 - val_loss: 1.4578 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0352 - val_loss: 1.4542 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0381 - val_loss: 1.4589 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0374 - val_loss: 1.4609 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0359 - val_loss: 1.4555 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0382 - val_loss: 1.4609 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0333 - val_loss: 1.4661 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0358 - val_loss: 1.4629 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0400 - val_loss: 1.4582 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0316 - val_loss: 1.4617 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0403 - val_loss: 1.4752 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0293 - val_loss: 1.4863 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0298 - val_loss: 1.4992 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0367 - val_loss: 1.5028 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0375 - val_loss: 1.5069 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0308 - val_loss: 1.5091 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0324 - val_loss: 1.4994 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0328 - val_loss: 1.4869 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0369 - val_loss: 1.4968 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0314 - val_loss: 1.5040 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0347 - val_loss: 1.5068 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0311 - val_loss: 1.5107 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0332 - val_loss: 1.5095 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0322 - val_loss: 1.5076 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0288 - val_loss: 1.4997 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0255 - val_loss: 1.4861 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0315 - val_loss: 1.4819 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0330 - val_loss: 1.4806 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0288 - val_loss: 1.4679 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0338 - val_loss: 1.4705 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0247 - val_loss: 1.4874 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0309 - val_loss: 1.4911 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0265 - val_loss: 1.4902 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0306 - val_loss: 1.4916 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0296 - val_loss: 1.4824 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0328 - val_loss: 1.4808 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0330 - val_loss: 1.4771 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0292 - val_loss: 1.4828 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0285 - val_loss: 1.4889 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0261 - val_loss: 1.4849 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0257 - val_loss: 1.4789 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0298 - val_loss: 1.4899 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "10/10 - 1s - 70ms/step - loss: 1.0287 - val_loss: 1.4899 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0243 - val_loss: 1.4868 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0261 - val_loss: 1.4845 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "10/10 - 1s - 79ms/step - loss: 1.0247 - val_loss: 1.4896 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0298 - val_loss: 1.4844 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0272 - val_loss: 1.4850 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0260 - val_loss: 1.4734 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0252 - val_loss: 1.4609 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0213 - val_loss: 1.4667 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0193 - val_loss: 1.4649 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0302 - val_loss: 1.4775 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0236 - val_loss: 1.4866 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0256 - val_loss: 1.4984 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0230 - val_loss: 1.5020 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0246 - val_loss: 1.4963 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0200 - val_loss: 1.4796 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0293 - val_loss: 1.4633 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0181 - val_loss: 1.4450 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0228 - val_loss: 1.4439 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0187 - val_loss: 1.4412 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0209 - val_loss: 1.4467 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0232 - val_loss: 1.4493 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0213 - val_loss: 1.4526 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0243 - val_loss: 1.4530 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0181 - val_loss: 1.4572 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0194 - val_loss: 1.4631 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0226 - val_loss: 1.4606 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0203 - val_loss: 1.4651 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0187 - val_loss: 1.4602 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0218 - val_loss: 1.4538 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0199 - val_loss: 1.4579 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0250 - val_loss: 1.4611 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "10/10 - 1s - 74ms/step - loss: 1.0158 - val_loss: 1.4595 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0169 - val_loss: 1.4561 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "10/10 - 1s - 73ms/step - loss: 1.0183 - val_loss: 1.4491 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0219 - val_loss: 1.4416 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0155 - val_loss: 1.4570 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0206 - val_loss: 1.4696 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0179 - val_loss: 1.4693 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0142 - val_loss: 1.4621 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0182 - val_loss: 1.4606 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "10/10 - 1s - 72ms/step - loss: 1.0161 - val_loss: 1.4684 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "10/10 - 1s - 71ms/step - loss: 1.0212 - val_loss: 1.4760 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 1\n",
      "Epoch 1/200\n",
      "22/22 - 8s - 364ms/step - loss: 2.1759 - val_loss: 1.7277 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "22/22 - 2s - 75ms/step - loss: 1.6423 - val_loss: 1.5089 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "22/22 - 2s - 77ms/step - loss: 1.3946 - val_loss: 1.5015 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "22/22 - 2s - 76ms/step - loss: 1.2507 - val_loss: 1.0886 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "22/22 - 2s - 76ms/step - loss: 1.1731 - val_loss: 1.2765 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "22/22 - 2s - 75ms/step - loss: 1.1082 - val_loss: 1.0603 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "22/22 - 2s - 75ms/step - loss: 1.0570 - val_loss: 1.3199 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "22/22 - 2s - 76ms/step - loss: 1.0121 - val_loss: 1.1998 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.9797 - val_loss: 1.0699 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.9496 - val_loss: 0.8970 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.9202 - val_loss: 1.1481 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.9022 - val_loss: 1.0459 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.8752 - val_loss: 1.0125 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.8621 - val_loss: 1.0120 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.8465 - val_loss: 0.9294 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.8291 - val_loss: 0.7219 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.8243 - val_loss: 0.9877 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.8049 - val_loss: 0.9199 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7983 - val_loss: 1.0180 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7851 - val_loss: 0.8037 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7811 - val_loss: 0.9009 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7711 - val_loss: 1.0848 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7635 - val_loss: 0.8459 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7566 - val_loss: 1.0090 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7521 - val_loss: 0.7616 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "22/22 - 2s - 75ms/step - loss: 0.7452 - val_loss: 1.0031 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7459 - val_loss: 0.8179 - learning_rate: 2.0000e-04\n",
      "Epoch 28/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7382 - val_loss: 0.8814 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7363 - val_loss: 0.8740 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7353 - val_loss: 0.8841 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "22/22 - 2s - 85ms/step - loss: 0.7352 - val_loss: 0.8192 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "22/22 - 2s - 80ms/step - loss: 0.7333 - val_loss: 0.9271 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7305 - val_loss: 0.8812 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7312 - val_loss: 0.8744 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7282 - val_loss: 0.8731 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "22/22 - 2s - 81ms/step - loss: 0.7264 - val_loss: 0.8311 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "22/22 - 2s - 80ms/step - loss: 0.7264 - val_loss: 0.8343 - learning_rate: 4.0000e-05\n",
      "Epoch 38/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7255 - val_loss: 0.8634 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7252 - val_loss: 0.8665 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7263 - val_loss: 0.8821 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7251 - val_loss: 0.8998 - learning_rate: 4.0000e-05\n",
      "Epoch 42/200\n",
      "22/22 - 2s - 81ms/step - loss: 0.7261 - val_loss: 0.8691 - learning_rate: 4.0000e-05\n",
      "Epoch 43/200\n",
      "22/22 - 2s - 80ms/step - loss: 0.7236 - val_loss: 0.8734 - learning_rate: 4.0000e-05\n",
      "Epoch 44/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7230 - val_loss: 0.8699 - learning_rate: 4.0000e-05\n",
      "Epoch 45/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7256 - val_loss: 0.8554 - learning_rate: 4.0000e-05\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "22/22 - 2s - 78ms/step - loss: 0.7240 - val_loss: 0.8660 - learning_rate: 4.0000e-05\n",
      "Epoch 47/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7229 - val_loss: 0.8667 - learning_rate: 1.0000e-05\n",
      "Epoch 48/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7242 - val_loss: 0.8629 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7245 - val_loss: 0.8649 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7230 - val_loss: 0.8669 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "22/22 - 2s - 81ms/step - loss: 0.7222 - val_loss: 0.8706 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7227 - val_loss: 0.8714 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7250 - val_loss: 0.8726 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7235 - val_loss: 0.8734 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "22/22 - 2s - 81ms/step - loss: 0.7216 - val_loss: 0.8719 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7218 - val_loss: 0.8713 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7225 - val_loss: 0.8674 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7242 - val_loss: 0.8690 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7246 - val_loss: 0.8648 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7235 - val_loss: 0.8632 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7223 - val_loss: 0.8677 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7253 - val_loss: 0.8785 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7241 - val_loss: 0.8808 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7232 - val_loss: 0.8711 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7210 - val_loss: 0.8571 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7219 - val_loss: 0.8592 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7216 - val_loss: 0.8561 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7232 - val_loss: 0.8627 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7205 - val_loss: 0.8587 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7220 - val_loss: 0.8621 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7232 - val_loss: 0.8694 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7216 - val_loss: 0.8747 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7210 - val_loss: 0.8758 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7211 - val_loss: 0.8723 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7196 - val_loss: 0.8652 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7196 - val_loss: 0.8624 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7210 - val_loss: 0.8554 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7194 - val_loss: 0.8602 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7212 - val_loss: 0.8668 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7204 - val_loss: 0.8599 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7200 - val_loss: 0.8655 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7204 - val_loss: 0.8700 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7213 - val_loss: 0.8795 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7205 - val_loss: 0.8778 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7201 - val_loss: 0.8625 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7197 - val_loss: 0.8640 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7199 - val_loss: 0.8686 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7177 - val_loss: 0.8627 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7205 - val_loss: 0.8697 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7204 - val_loss: 0.8756 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7166 - val_loss: 0.8787 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7201 - val_loss: 0.8724 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7183 - val_loss: 0.8733 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7167 - val_loss: 0.8712 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7202 - val_loss: 0.8676 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7169 - val_loss: 0.8758 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7170 - val_loss: 0.8809 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7173 - val_loss: 0.8776 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7165 - val_loss: 0.8801 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7182 - val_loss: 0.8713 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7158 - val_loss: 0.8870 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7173 - val_loss: 0.8893 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7148 - val_loss: 0.8884 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.7151 - val_loss: 0.8813 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7126 - val_loss: 0.8813 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7116 - val_loss: 0.8891 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7114 - val_loss: 0.8731 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.7087 - val_loss: 0.9290 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.7059 - val_loss: 0.9840 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.7054 - val_loss: 0.8356 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.7036 - val_loss: 0.9001 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6975 - val_loss: 0.9528 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6966 - val_loss: 1.0214 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.6920 - val_loss: 0.8045 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6876 - val_loss: 0.8785 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6808 - val_loss: 0.8838 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6809 - val_loss: 0.9262 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6732 - val_loss: 1.1394 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6809 - val_loss: 0.9371 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6701 - val_loss: 0.7014 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6696 - val_loss: 0.8502 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6631 - val_loss: 0.7741 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.6650 - val_loss: 0.8387 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6572 - val_loss: 0.7309 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6545 - val_loss: 0.8831 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.6471 - val_loss: 0.9351 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6463 - val_loss: 1.0661 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6389 - val_loss: 0.8603 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6411 - val_loss: 0.9191 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6341 - val_loss: 1.0767 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6292 - val_loss: 0.9542 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6278 - val_loss: 1.1509 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6201 - val_loss: 1.1321 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6175 - val_loss: 0.8000 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6324 - val_loss: 0.9897 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.6131 - val_loss: 1.0223 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.6026 - val_loss: 1.0834 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6003 - val_loss: 1.3377 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.6066 - val_loss: 0.9727 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5919 - val_loss: 0.9567 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5884 - val_loss: 1.0646 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5793 - val_loss: 1.1014 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5937 - val_loss: 1.2533 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5722 - val_loss: 0.9178 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5769 - val_loss: 1.0079 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5657 - val_loss: 0.9387 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5589 - val_loss: 0.7881 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5587 - val_loss: 1.2541 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5532 - val_loss: 0.7907 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5643 - val_loss: 0.9931 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5517 - val_loss: 1.0219 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5398 - val_loss: 1.2715 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5586 - val_loss: 1.1820 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5553 - val_loss: 1.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5344 - val_loss: 0.7473 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5344 - val_loss: 0.6641 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5591 - val_loss: 0.6794 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5444 - val_loss: 0.9264 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5365 - val_loss: 1.1689 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5285 - val_loss: 1.0429 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5337 - val_loss: 0.9327 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5285 - val_loss: 1.1637 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "22/22 - 2s - 79ms/step - loss: 0.5219 - val_loss: 0.9994 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5214 - val_loss: 1.0973 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5161 - val_loss: 1.3554 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5289 - val_loss: 0.8624 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5231 - val_loss: 0.9294 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5249 - val_loss: 1.0670 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5130 - val_loss: 0.9510 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5158 - val_loss: 0.9572 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "22/22 - 2s - 74ms/step - loss: 0.5160 - val_loss: 1.2439 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5159 - val_loss: 0.9384 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5121 - val_loss: 0.6347 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5207 - val_loss: 0.7838 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5128 - val_loss: 1.3773 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "22/22 - 2s - 78ms/step - loss: 0.5141 - val_loss: 1.3756 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5195 - val_loss: 1.1370 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5058 - val_loss: 0.9608 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5175 - val_loss: 1.0891 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5070 - val_loss: 1.1083 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5099 - val_loss: 0.9823 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "22/22 - 2s - 74ms/step - loss: 0.5085 - val_loss: 1.0801 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4981 - val_loss: 1.0949 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4976 - val_loss: 0.9742 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5036 - val_loss: 0.6748 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4971 - val_loss: 1.0319 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5114 - val_loss: 1.2099 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "22/22 - 2s - 77ms/step - loss: 0.5011 - val_loss: 1.0484 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4850 - val_loss: 0.9536 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4841 - val_loss: 1.0741 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4878 - val_loss: 1.2033 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5049 - val_loss: 1.4126 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.5258 - val_loss: 0.9316 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "22/22 - 2s - 74ms/step - loss: 0.4890 - val_loss: 0.7912 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "22/22 - 2s - 74ms/step - loss: 0.4896 - val_loss: 1.1684 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.5065 - val_loss: 1.0430 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "22/22 - 2s - 75ms/step - loss: 0.4822 - val_loss: 1.0845 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.4892 - val_loss: 1.0181 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.4846 - val_loss: 0.9735 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "22/22 - 2s - 76ms/step - loss: 0.4749 - val_loss: 1.0204 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 2\n",
      "Epoch 1/200\n",
      "19/19 - 7s - 391ms/step - loss: 2.1734 - val_loss: 1.7011 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.6526 - val_loss: 1.5364 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.4203 - val_loss: 1.4293 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.2821 - val_loss: 1.4651 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.1903 - val_loss: 1.1738 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.1101 - val_loss: 1.4883 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.0678 - val_loss: 0.9649 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.0106 - val_loss: 0.9255 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.9928 - val_loss: 1.0931 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.9491 - val_loss: 0.8385 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.9194 - val_loss: 0.9405 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.8917 - val_loss: 0.9065 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.8683 - val_loss: 0.7697 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.8493 - val_loss: 0.9194 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.8351 - val_loss: 0.7117 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.8073 - val_loss: 0.8468 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7956 - val_loss: 0.8206 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7787 - val_loss: 1.0548 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7791 - val_loss: 0.8033 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7568 - val_loss: 0.7031 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7474 - val_loss: 0.8515 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7357 - val_loss: 0.8618 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7318 - val_loss: 0.6021 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7222 - val_loss: 0.8900 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7069 - val_loss: 0.6728 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.7085 - val_loss: 0.7125 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7031 - val_loss: 0.8543 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6939 - val_loss: 0.7610 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "19/19 - 1s - 65ms/step - loss: 0.6813 - val_loss: 0.7369 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6849 - val_loss: 0.7612 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6756 - val_loss: 0.9313 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6711 - val_loss: 0.7025 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "19/19 - 1s - 66ms/step - loss: 0.6623 - val_loss: 0.6166 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6632 - val_loss: 0.7942 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6585 - val_loss: 0.7402 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6552 - val_loss: 0.7412 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6559 - val_loss: 0.7555 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6554 - val_loss: 0.7318 - learning_rate: 2.0000e-04\n",
      "Epoch 39/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6553 - val_loss: 0.7882 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6521 - val_loss: 0.7074 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6516 - val_loss: 0.7235 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6508 - val_loss: 0.7667 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "19/19 - 1s - 69ms/step - loss: 0.6510 - val_loss: 0.7045 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6480 - val_loss: 0.7209 - learning_rate: 4.0000e-05\n",
      "Epoch 45/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6499 - val_loss: 0.7498 - learning_rate: 4.0000e-05\n",
      "Epoch 46/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6486 - val_loss: 0.7563 - learning_rate: 4.0000e-05\n",
      "Epoch 47/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6469 - val_loss: 0.7486 - learning_rate: 4.0000e-05\n",
      "Epoch 48/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6461 - val_loss: 0.7484 - learning_rate: 4.0000e-05\n",
      "Epoch 49/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6475 - val_loss: 0.7461 - learning_rate: 4.0000e-05\n",
      "Epoch 50/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6466 - val_loss: 0.7383 - learning_rate: 4.0000e-05\n",
      "Epoch 51/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6452 - val_loss: 0.7541 - learning_rate: 4.0000e-05\n",
      "Epoch 52/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6472 - val_loss: 0.7382 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 66ms/step - loss: 0.6468 - val_loss: 0.7470 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6464 - val_loss: 0.7468 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "19/19 - 1s - 65ms/step - loss: 0.6449 - val_loss: 0.7467 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6447 - val_loss: 0.7431 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6436 - val_loss: 0.7419 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6462 - val_loss: 0.7396 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6480 - val_loss: 0.7369 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6428 - val_loss: 0.7424 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6458 - val_loss: 0.7538 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6439 - val_loss: 0.7560 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6443 - val_loss: 0.7480 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6439 - val_loss: 0.7472 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6469 - val_loss: 0.7445 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6426 - val_loss: 0.7459 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6458 - val_loss: 0.7408 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6433 - val_loss: 0.7402 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6411 - val_loss: 0.7460 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6465 - val_loss: 0.7451 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6435 - val_loss: 0.7482 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6459 - val_loss: 0.7481 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6441 - val_loss: 0.7479 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6448 - val_loss: 0.7503 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6444 - val_loss: 0.7463 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6453 - val_loss: 0.7532 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6447 - val_loss: 0.7447 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6475 - val_loss: 0.7468 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6429 - val_loss: 0.7473 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6454 - val_loss: 0.7559 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6442 - val_loss: 0.7482 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6412 - val_loss: 0.7401 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6428 - val_loss: 0.7399 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6435 - val_loss: 0.7496 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6432 - val_loss: 0.7464 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6435 - val_loss: 0.7456 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6428 - val_loss: 0.7444 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6430 - val_loss: 0.7467 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6431 - val_loss: 0.7430 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6409 - val_loss: 0.7455 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6448 - val_loss: 0.7426 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6437 - val_loss: 0.7510 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6389 - val_loss: 0.7463 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6425 - val_loss: 0.7497 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6444 - val_loss: 0.7465 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6423 - val_loss: 0.7424 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6400 - val_loss: 0.7419 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6415 - val_loss: 0.7475 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6440 - val_loss: 0.7447 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6430 - val_loss: 0.7380 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6397 - val_loss: 0.7454 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6408 - val_loss: 0.7533 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6405 - val_loss: 0.7553 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6391 - val_loss: 0.7413 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6402 - val_loss: 0.7410 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6391 - val_loss: 0.7635 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6395 - val_loss: 0.7627 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6385 - val_loss: 0.7584 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6362 - val_loss: 0.7563 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6370 - val_loss: 0.7550 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6340 - val_loss: 0.7629 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6369 - val_loss: 0.7639 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6373 - val_loss: 0.7726 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6356 - val_loss: 0.7725 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6376 - val_loss: 0.7557 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6355 - val_loss: 0.7744 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6337 - val_loss: 0.7716 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6361 - val_loss: 0.7661 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6352 - val_loss: 0.8309 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6343 - val_loss: 0.7656 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6318 - val_loss: 0.8030 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6267 - val_loss: 0.8168 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6268 - val_loss: 0.7530 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6317 - val_loss: 0.8487 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6238 - val_loss: 0.8450 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6202 - val_loss: 0.8218 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6214 - val_loss: 0.8262 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6206 - val_loss: 0.7403 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6146 - val_loss: 0.7564 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6185 - val_loss: 0.8013 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6083 - val_loss: 0.8527 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6082 - val_loss: 0.7743 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6110 - val_loss: 0.8020 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6116 - val_loss: 0.9069 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6010 - val_loss: 0.8570 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6031 - val_loss: 0.8659 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6017 - val_loss: 0.8756 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5965 - val_loss: 0.8476 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5946 - val_loss: 0.8522 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5929 - val_loss: 0.9315 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5897 - val_loss: 0.7367 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5852 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5887 - val_loss: 0.9830 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5956 - val_loss: 0.8642 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5869 - val_loss: 0.8540 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5861 - val_loss: 0.8223 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5855 - val_loss: 0.9095 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5784 - val_loss: 0.9933 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5775 - val_loss: 0.8732 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5763 - val_loss: 0.8261 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5812 - val_loss: 0.8673 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5739 - val_loss: 0.9131 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5727 - val_loss: 0.9551 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5730 - val_loss: 0.8257 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5689 - val_loss: 0.8696 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5648 - val_loss: 0.9493 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5570 - val_loss: 0.8922 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5663 - val_loss: 0.9338 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5627 - val_loss: 0.9156 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5597 - val_loss: 0.8534 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5601 - val_loss: 0.9963 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5587 - val_loss: 0.9595 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5585 - val_loss: 0.8237 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5635 - val_loss: 0.9319 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5488 - val_loss: 0.8605 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5539 - val_loss: 0.8584 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.5583 - val_loss: 0.9025 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5557 - val_loss: 0.8493 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5444 - val_loss: 0.9959 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5424 - val_loss: 0.9658 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5417 - val_loss: 0.9938 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5440 - val_loss: 0.8104 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5468 - val_loss: 1.0141 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5411 - val_loss: 0.9213 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5373 - val_loss: 0.8661 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5379 - val_loss: 0.9006 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5485 - val_loss: 0.9091 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5291 - val_loss: 0.9652 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5305 - val_loss: 0.9915 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.5336 - val_loss: 0.9112 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5397 - val_loss: 0.9974 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.5274 - val_loss: 0.9678 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.5267 - val_loss: 0.9562 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5231 - val_loss: 0.9946 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5254 - val_loss: 0.9657 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5239 - val_loss: 0.9431 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5231 - val_loss: 0.8665 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5229 - val_loss: 0.9329 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5165 - val_loss: 0.9738 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5153 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.5092 - val_loss: 1.0142 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5050 - val_loss: 1.0610 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5104 - val_loss: 0.7784 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5119 - val_loss: 0.9938 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5134 - val_loss: 1.0094 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5032 - val_loss: 0.7956 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5050 - val_loss: 0.8699 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5068 - val_loss: 0.8644 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.4944 - val_loss: 0.9287 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.4999 - val_loss: 0.8269 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 3\n",
      "Epoch 1/200\n",
      "19/19 - 8s - 417ms/step - loss: 2.2403 - val_loss: 2.2143 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.6939 - val_loss: 1.7153 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.4647 - val_loss: 1.8941 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.3282 - val_loss: 1.7447 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "19/19 - 1s - 66ms/step - loss: 1.2383 - val_loss: 1.6062 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.1754 - val_loss: 1.6182 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.1244 - val_loss: 1.5812 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.0740 - val_loss: 1.3819 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.0416 - val_loss: 1.3608 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.0075 - val_loss: 1.6362 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.9801 - val_loss: 1.3968 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.9532 - val_loss: 1.3440 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.9319 - val_loss: 1.4265 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.9098 - val_loss: 1.3804 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.8906 - val_loss: 1.2899 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.8667 - val_loss: 1.8301 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.8656 - val_loss: 1.1316 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.8455 - val_loss: 1.3396 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.8337 - val_loss: 1.3655 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.8114 - val_loss: 1.3311 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.8072 - val_loss: 1.2884 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.7943 - val_loss: 1.2216 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.7808 - val_loss: 1.2696 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.7753 - val_loss: 1.1776 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "19/19 - 1s - 76ms/step - loss: 0.7647 - val_loss: 1.2598 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.7597 - val_loss: 1.1011 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.7525 - val_loss: 1.4735 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.7436 - val_loss: 1.0676 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.7370 - val_loss: 1.3129 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.7329 - val_loss: 1.2345 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.7290 - val_loss: 1.3648 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.7268 - val_loss: 1.0234 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.7152 - val_loss: 1.1970 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.7137 - val_loss: 1.1105 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.7071 - val_loss: 0.9405 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.7133 - val_loss: 1.3781 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6992 - val_loss: 1.1808 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6922 - val_loss: 1.1228 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6951 - val_loss: 1.1912 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6936 - val_loss: 0.9800 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6868 - val_loss: 1.2922 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6802 - val_loss: 1.2900 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.6775 - val_loss: 1.1985 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.6759 - val_loss: 1.0969 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "19/19 - 1s - 75ms/step - loss: 0.6740 - val_loss: 1.0725 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6710 - val_loss: 1.1545 - learning_rate: 2.0000e-04\n",
      "Epoch 47/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6676 - val_loss: 1.1435 - learning_rate: 2.0000e-04\n",
      "Epoch 48/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6670 - val_loss: 1.1632 - learning_rate: 2.0000e-04\n",
      "Epoch 49/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6662 - val_loss: 1.1080 - learning_rate: 2.0000e-04\n",
      "Epoch 50/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6647 - val_loss: 1.1926 - learning_rate: 2.0000e-04\n",
      "Epoch 51/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6653 - val_loss: 1.1356 - learning_rate: 2.0000e-04\n",
      "Epoch 52/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6639 - val_loss: 1.1494 - learning_rate: 2.0000e-04\n",
      "Epoch 53/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6644 - val_loss: 1.2134 - learning_rate: 2.0000e-04\n",
      "Epoch 54/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6646 - val_loss: 1.1115 - learning_rate: 2.0000e-04\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "19/19 - 1s - 75ms/step - loss: 0.6617 - val_loss: 1.1511 - learning_rate: 2.0000e-04\n",
      "Epoch 56/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6606 - val_loss: 1.1252 - learning_rate: 4.0000e-05\n",
      "Epoch 57/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6622 - val_loss: 1.1411 - learning_rate: 4.0000e-05\n",
      "Epoch 58/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6611 - val_loss: 1.1692 - learning_rate: 4.0000e-05\n",
      "Epoch 59/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6577 - val_loss: 1.1413 - learning_rate: 4.0000e-05\n",
      "Epoch 60/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6597 - val_loss: 1.1589 - learning_rate: 4.0000e-05\n",
      "Epoch 61/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6627 - val_loss: 1.1546 - learning_rate: 4.0000e-05\n",
      "Epoch 62/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6617 - val_loss: 1.1526 - learning_rate: 4.0000e-05\n",
      "Epoch 63/200\n",
      "19/19 - 1s - 75ms/step - loss: 0.6601 - val_loss: 1.1528 - learning_rate: 4.0000e-05\n",
      "Epoch 64/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.6588 - val_loss: 1.1511 - learning_rate: 4.0000e-05\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 67ms/step - loss: 0.6594 - val_loss: 1.1396 - learning_rate: 4.0000e-05\n",
      "Epoch 66/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6592 - val_loss: 1.1375 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6591 - val_loss: 1.1486 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6599 - val_loss: 1.1495 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6582 - val_loss: 1.1474 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6587 - val_loss: 1.1571 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6585 - val_loss: 1.1508 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6578 - val_loss: 1.1486 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6588 - val_loss: 1.1464 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6586 - val_loss: 1.1511 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6605 - val_loss: 1.1537 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6592 - val_loss: 1.1434 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6605 - val_loss: 1.1540 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6588 - val_loss: 1.1437 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6594 - val_loss: 1.1491 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6597 - val_loss: 1.1495 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6596 - val_loss: 1.1460 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6612 - val_loss: 1.1557 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6607 - val_loss: 1.1557 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6580 - val_loss: 1.1496 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6587 - val_loss: 1.1512 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6628 - val_loss: 1.1538 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6595 - val_loss: 1.1601 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6584 - val_loss: 1.1581 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6596 - val_loss: 1.1462 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6604 - val_loss: 1.1527 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6576 - val_loss: 1.1527 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6578 - val_loss: 1.1481 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6564 - val_loss: 1.1506 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6598 - val_loss: 1.1502 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6571 - val_loss: 1.1524 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6595 - val_loss: 1.1544 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6564 - val_loss: 1.1589 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6583 - val_loss: 1.1443 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6575 - val_loss: 1.1581 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6581 - val_loss: 1.1515 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6575 - val_loss: 1.1527 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6598 - val_loss: 1.1521 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6585 - val_loss: 1.1446 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6560 - val_loss: 1.1514 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6591 - val_loss: 1.1427 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6559 - val_loss: 1.1500 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6548 - val_loss: 1.1494 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6583 - val_loss: 1.1599 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6552 - val_loss: 1.1552 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6575 - val_loss: 1.1483 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6569 - val_loss: 1.1529 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6561 - val_loss: 1.1481 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6578 - val_loss: 1.1460 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6561 - val_loss: 1.1533 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6553 - val_loss: 1.1564 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6573 - val_loss: 1.1487 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6568 - val_loss: 1.1512 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6564 - val_loss: 1.1565 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6570 - val_loss: 1.1545 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6571 - val_loss: 1.1605 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6579 - val_loss: 1.1537 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6578 - val_loss: 1.1525 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6559 - val_loss: 1.1649 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6558 - val_loss: 1.1532 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6560 - val_loss: 1.1436 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6543 - val_loss: 1.1495 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6558 - val_loss: 1.1602 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6544 - val_loss: 1.1626 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6548 - val_loss: 1.1518 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6575 - val_loss: 1.1530 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6552 - val_loss: 1.1616 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6552 - val_loss: 1.1457 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6532 - val_loss: 1.1484 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6550 - val_loss: 1.1474 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6511 - val_loss: 1.1557 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6528 - val_loss: 1.1530 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6538 - val_loss: 1.1462 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6540 - val_loss: 1.1502 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6528 - val_loss: 1.1491 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6513 - val_loss: 1.1504 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6522 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6521 - val_loss: 1.1474 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6497 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6537 - val_loss: 1.1762 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6527 - val_loss: 1.1563 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6516 - val_loss: 1.1704 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6504 - val_loss: 1.1652 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6515 - val_loss: 1.1747 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6540 - val_loss: 1.1567 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6485 - val_loss: 1.1700 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6492 - val_loss: 1.1752 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6473 - val_loss: 1.1257 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6492 - val_loss: 1.1144 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6474 - val_loss: 1.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6476 - val_loss: 1.0899 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6465 - val_loss: 1.2195 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6441 - val_loss: 1.1111 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6414 - val_loss: 1.2291 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6378 - val_loss: 1.2636 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6408 - val_loss: 1.2967 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6445 - val_loss: 1.2558 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6342 - val_loss: 1.2168 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6321 - val_loss: 1.2630 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6306 - val_loss: 1.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6329 - val_loss: 1.0377 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6333 - val_loss: 1.0763 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6223 - val_loss: 1.1949 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6172 - val_loss: 1.1183 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6110 - val_loss: 1.2456 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6019 - val_loss: 1.3785 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6015 - val_loss: 1.0819 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5959 - val_loss: 1.0563 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5903 - val_loss: 1.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5922 - val_loss: 1.0561 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5825 - val_loss: 1.1588 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5752 - val_loss: 1.1957 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5781 - val_loss: 1.2578 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5716 - val_loss: 1.1769 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5681 - val_loss: 1.1784 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.5683 - val_loss: 0.8767 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5633 - val_loss: 0.8457 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5508 - val_loss: 1.0066 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5508 - val_loss: 0.8227 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5485 - val_loss: 0.9564 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5453 - val_loss: 0.8704 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5432 - val_loss: 0.8707 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5376 - val_loss: 1.0154 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "19/19 - 1s - 74ms/step - loss: 0.5367 - val_loss: 1.0763 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "19/19 - 1s - 73ms/step - loss: 0.5398 - val_loss: 0.8053 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.5494 - val_loss: 0.7298 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5367 - val_loss: 0.9599 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5327 - val_loss: 0.9418 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5230 - val_loss: 0.9395 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5288 - val_loss: 0.7569 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5227 - val_loss: 0.8722 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5137 - val_loss: 1.0459 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5130 - val_loss: 0.9925 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5117 - val_loss: 0.7771 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5135 - val_loss: 0.8249 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5163 - val_loss: 0.9443 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 4\n",
      "Epoch 1/200\n",
      "11/11 - 7s - 665ms/step - loss: 2.4807 - val_loss: 1.7374 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.9606 - val_loss: 1.6578 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.7679 - val_loss: 1.5468 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.6088 - val_loss: 1.7594 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.4944 - val_loss: 1.5811 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "11/11 - 1s - 64ms/step - loss: 1.4326 - val_loss: 1.5261 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "11/11 - 1s - 67ms/step - loss: 1.3642 - val_loss: 1.3247 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.3027 - val_loss: 1.3144 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "11/11 - 1s - 64ms/step - loss: 1.2625 - val_loss: 1.1393 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.2319 - val_loss: 1.0318 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.1926 - val_loss: 1.2285 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.1601 - val_loss: 1.0747 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.1404 - val_loss: 1.1473 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.1095 - val_loss: 1.1540 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.0780 - val_loss: 0.8326 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.0819 - val_loss: 1.2070 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.0431 - val_loss: 1.1631 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "11/11 - 1s - 67ms/step - loss: 1.0354 - val_loss: 1.0980 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "11/11 - 1s - 65ms/step - loss: 1.0159 - val_loss: 0.9036 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.0008 - val_loss: 0.9148 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.9877 - val_loss: 0.7831 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "11/11 - 1s - 66ms/step - loss: 1.0055 - val_loss: 0.9584 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.9831 - val_loss: 1.3837 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.9543 - val_loss: 0.8086 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.9406 - val_loss: 0.8752 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.9300 - val_loss: 1.0697 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.9210 - val_loss: 1.1162 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.9036 - val_loss: 0.8506 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8968 - val_loss: 0.9112 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8828 - val_loss: 0.9619 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8774 - val_loss: 0.6950 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.8762 - val_loss: 1.0779 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8593 - val_loss: 0.9649 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.8601 - val_loss: 0.7752 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8543 - val_loss: 0.8790 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8477 - val_loss: 1.0246 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8386 - val_loss: 0.6640 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.8397 - val_loss: 0.9714 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8275 - val_loss: 0.9264 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8185 - val_loss: 0.7919 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "11/11 - 1s - 64ms/step - loss: 0.8129 - val_loss: 1.2091 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8366 - val_loss: 0.5751 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8278 - val_loss: 0.9939 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8219 - val_loss: 1.0831 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.8165 - val_loss: 0.7233 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7931 - val_loss: 1.0061 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7847 - val_loss: 0.8512 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7892 - val_loss: 0.7354 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7927 - val_loss: 1.0245 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7854 - val_loss: 0.7001 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7812 - val_loss: 0.9832 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "11/11 - 1s - 66ms/step - loss: 0.7740 - val_loss: 0.8117 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7640 - val_loss: 0.8374 - learning_rate: 2.0000e-04\n",
      "Epoch 54/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7655 - val_loss: 0.8118 - learning_rate: 2.0000e-04\n",
      "Epoch 55/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7687 - val_loss: 0.8108 - learning_rate: 2.0000e-04\n",
      "Epoch 56/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7597 - val_loss: 0.8327 - learning_rate: 2.0000e-04\n",
      "Epoch 57/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7609 - val_loss: 0.8719 - learning_rate: 2.0000e-04\n",
      "Epoch 58/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7612 - val_loss: 0.8727 - learning_rate: 2.0000e-04\n",
      "Epoch 59/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7596 - val_loss: 0.8235 - learning_rate: 2.0000e-04\n",
      "Epoch 60/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7574 - val_loss: 0.8702 - learning_rate: 2.0000e-04\n",
      "Epoch 61/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7586 - val_loss: 0.9104 - learning_rate: 2.0000e-04\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "11/11 - 1s - 66ms/step - loss: 0.7613 - val_loss: 0.8564 - learning_rate: 2.0000e-04\n",
      "Epoch 63/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7580 - val_loss: 0.8822 - learning_rate: 4.0000e-05\n",
      "Epoch 64/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7592 - val_loss: 0.8768 - learning_rate: 4.0000e-05\n",
      "Epoch 65/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7561 - val_loss: 0.8570 - learning_rate: 4.0000e-05\n",
      "Epoch 66/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7587 - val_loss: 0.8597 - learning_rate: 4.0000e-05\n",
      "Epoch 67/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7594 - val_loss: 0.8345 - learning_rate: 4.0000e-05\n",
      "Epoch 68/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7543 - val_loss: 0.8441 - learning_rate: 4.0000e-05\n",
      "Epoch 69/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7567 - val_loss: 0.8739 - learning_rate: 4.0000e-05\n",
      "Epoch 70/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7566 - val_loss: 0.8910 - learning_rate: 4.0000e-05\n",
      "Epoch 71/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7556 - val_loss: 0.8699 - learning_rate: 4.0000e-05\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "11/11 - 1s - 65ms/step - loss: 0.7517 - val_loss: 0.8230 - learning_rate: 4.0000e-05\n",
      "Epoch 73/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7570 - val_loss: 0.8198 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7554 - val_loss: 0.8237 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7586 - val_loss: 0.8270 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7553 - val_loss: 0.8357 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7557 - val_loss: 0.8395 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7543 - val_loss: 0.8379 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7538 - val_loss: 0.8367 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7561 - val_loss: 0.8390 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7553 - val_loss: 0.8419 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7546 - val_loss: 0.8392 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7541 - val_loss: 0.8390 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7543 - val_loss: 0.8357 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7589 - val_loss: 0.8347 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7536 - val_loss: 0.8409 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7539 - val_loss: 0.8392 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7555 - val_loss: 0.8401 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7581 - val_loss: 0.8405 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7530 - val_loss: 0.8455 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7492 - val_loss: 0.8445 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7527 - val_loss: 0.8438 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7532 - val_loss: 0.8450 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "11/11 - 1s - 64ms/step - loss: 0.7554 - val_loss: 0.8415 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7562 - val_loss: 0.8414 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7560 - val_loss: 0.8472 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7535 - val_loss: 0.8523 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7569 - val_loss: 0.8539 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7552 - val_loss: 0.8512 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7552 - val_loss: 0.8545 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7556 - val_loss: 0.8523 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7521 - val_loss: 0.8504 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7558 - val_loss: 0.8467 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7553 - val_loss: 0.8460 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7537 - val_loss: 0.8492 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7547 - val_loss: 0.8468 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7566 - val_loss: 0.8503 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7496 - val_loss: 0.8484 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7557 - val_loss: 0.8509 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7552 - val_loss: 0.8470 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7509 - val_loss: 0.8398 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7556 - val_loss: 0.8438 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7561 - val_loss: 0.8492 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7565 - val_loss: 0.8521 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7540 - val_loss: 0.8475 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7536 - val_loss: 0.8406 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7585 - val_loss: 0.8338 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7549 - val_loss: 0.8261 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7536 - val_loss: 0.8217 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7547 - val_loss: 0.8215 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7485 - val_loss: 0.8280 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7562 - val_loss: 0.8388 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7512 - val_loss: 0.8485 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7524 - val_loss: 0.8521 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7528 - val_loss: 0.8515 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7519 - val_loss: 0.8470 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7551 - val_loss: 0.8462 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7520 - val_loss: 0.8483 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7527 - val_loss: 0.8492 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7521 - val_loss: 0.8514 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7550 - val_loss: 0.8531 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7540 - val_loss: 0.8549 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7507 - val_loss: 0.8611 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7548 - val_loss: 0.8614 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7508 - val_loss: 0.8678 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7529 - val_loss: 0.8643 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7520 - val_loss: 0.8600 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7537 - val_loss: 0.8570 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7520 - val_loss: 0.8526 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7480 - val_loss: 0.8490 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7546 - val_loss: 0.8474 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7493 - val_loss: 0.8433 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7527 - val_loss: 0.8407 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7503 - val_loss: 0.8360 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7541 - val_loss: 0.8394 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7527 - val_loss: 0.8391 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7496 - val_loss: 0.8409 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7523 - val_loss: 0.8493 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7524 - val_loss: 0.8543 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7532 - val_loss: 0.8523 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7513 - val_loss: 0.8485 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7523 - val_loss: 0.8504 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7530 - val_loss: 0.8463 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7509 - val_loss: 0.8425 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7482 - val_loss: 0.8386 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "11/11 - 1s - 65ms/step - loss: 0.7502 - val_loss: 0.8399 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7516 - val_loss: 0.8467 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7491 - val_loss: 0.8444 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "11/11 - 1s - 70ms/step - loss: 0.7513 - val_loss: 0.8436 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7498 - val_loss: 0.8410 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7536 - val_loss: 0.8452 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7488 - val_loss: 0.8538 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7476 - val_loss: 0.8580 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7497 - val_loss: 0.8533 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7504 - val_loss: 0.8554 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7504 - val_loss: 0.8544 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7507 - val_loss: 0.8536 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7522 - val_loss: 0.8518 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7518 - val_loss: 0.8611 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7573 - val_loss: 0.8636 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7493 - val_loss: 0.8613 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7516 - val_loss: 0.8553 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7486 - val_loss: 0.8536 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7500 - val_loss: 0.8579 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7515 - val_loss: 0.8584 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7511 - val_loss: 0.8550 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7498 - val_loss: 0.8536 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7491 - val_loss: 0.8490 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7492 - val_loss: 0.8444 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7479 - val_loss: 0.8430 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7545 - val_loss: 0.8457 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7493 - val_loss: 0.8490 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7494 - val_loss: 0.8514 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7465 - val_loss: 0.8478 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7477 - val_loss: 0.8527 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7504 - val_loss: 0.8479 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7500 - val_loss: 0.8410 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7435 - val_loss: 0.8299 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7505 - val_loss: 0.8254 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7499 - val_loss: 0.8220 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "11/11 - 1s - 66ms/step - loss: 0.7508 - val_loss: 0.8245 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "11/11 - 1s - 68ms/step - loss: 0.7485 - val_loss: 0.8342 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7469 - val_loss: 0.8386 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7456 - val_loss: 0.8367 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7492 - val_loss: 0.8410 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7507 - val_loss: 0.8477 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7482 - val_loss: 0.8526 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "11/11 - 1s - 67ms/step - loss: 0.7501 - val_loss: 0.8588 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "11/11 - 1s - 69ms/step - loss: 0.7478 - val_loss: 0.8569 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "11/11 - 1s - 70ms/step - loss: 0.7525 - val_loss: 0.8663 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 5\n",
      "Epoch 1/200\n",
      "3/3 - 7s - 2s/step - loss: 3.1130 - val_loss: 1.8199 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "3/3 - 0s - 39ms/step - loss: 2.4004 - val_loss: 2.2257 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "3/3 - 0s - 38ms/step - loss: 2.1187 - val_loss: 2.5904 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "3/3 - 0s - 36ms/step - loss: 2.0607 - val_loss: 2.3111 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.9505 - val_loss: 1.9966 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.8537 - val_loss: 1.9530 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.7680 - val_loss: 1.9825 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.7149 - val_loss: 1.9339 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.6568 - val_loss: 1.8260 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.6153 - val_loss: 1.7277 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.5459 - val_loss: 1.8488 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.4953 - val_loss: 1.6959 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.4662 - val_loss: 1.5334 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.4423 - val_loss: 1.5764 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.4054 - val_loss: 1.7544 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.3606 - val_loss: 1.6228 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.3313 - val_loss: 1.3538 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.3156 - val_loss: 1.5693 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.2952 - val_loss: 1.6201 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.2655 - val_loss: 1.4810 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.2562 - val_loss: 1.4538 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.2232 - val_loss: 1.4936 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.2089 - val_loss: 1.4486 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.1884 - val_loss: 1.4366 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.1519 - val_loss: 1.3806 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.1554 - val_loss: 1.3846 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3/3 - 0s - 35ms/step - loss: 1.1658 - val_loss: 1.4509 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.1406 - val_loss: 1.4181 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.1347 - val_loss: 1.3817 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.1369 - val_loss: 1.3674 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.1294 - val_loss: 1.3610 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.1404 - val_loss: 1.4054 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.1260 - val_loss: 1.4128 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.1163 - val_loss: 1.4002 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.1077 - val_loss: 1.3868 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.1165 - val_loss: 1.3335 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.1111 - val_loss: 1.3292 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.1087 - val_loss: 1.3465 - learning_rate: 2.0000e-04\n",
      "Epoch 39/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0898 - val_loss: 1.4003 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.1128 - val_loss: 1.3940 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0987 - val_loss: 1.3777 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0971 - val_loss: 1.3681 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.1161 - val_loss: 1.3232 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "3/3 - 0s - 31ms/step - loss: 1.1215 - val_loss: 1.3399 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.1038 - val_loss: 1.3874 - learning_rate: 2.0000e-04\n",
      "Epoch 46/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.1140 - val_loss: 1.3926 - learning_rate: 2.0000e-04\n",
      "Epoch 47/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0999 - val_loss: 1.3953 - learning_rate: 2.0000e-04\n",
      "Epoch 48/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0945 - val_loss: 1.3829 - learning_rate: 2.0000e-04\n",
      "Epoch 49/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0850 - val_loss: 1.3609 - learning_rate: 2.0000e-04\n",
      "Epoch 50/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.1042 - val_loss: 1.3877 - learning_rate: 2.0000e-04\n",
      "Epoch 51/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.1069 - val_loss: 1.3576 - learning_rate: 2.0000e-04\n",
      "Epoch 52/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0908 - val_loss: 1.3431 - learning_rate: 2.0000e-04\n",
      "Epoch 53/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0778 - val_loss: 1.3187 - learning_rate: 2.0000e-04\n",
      "Epoch 54/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0826 - val_loss: 1.3169 - learning_rate: 2.0000e-04\n",
      "Epoch 55/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0832 - val_loss: 1.3372 - learning_rate: 2.0000e-04\n",
      "Epoch 56/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0775 - val_loss: 1.3599 - learning_rate: 2.0000e-04\n",
      "Epoch 57/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0793 - val_loss: 1.3739 - learning_rate: 2.0000e-04\n",
      "Epoch 58/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0737 - val_loss: 1.3682 - learning_rate: 2.0000e-04\n",
      "Epoch 59/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0630 - val_loss: 1.3626 - learning_rate: 2.0000e-04\n",
      "Epoch 60/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0693 - val_loss: 1.3501 - learning_rate: 2.0000e-04\n",
      "Epoch 61/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0681 - val_loss: 1.3664 - learning_rate: 2.0000e-04\n",
      "Epoch 62/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0737 - val_loss: 1.3854 - learning_rate: 2.0000e-04\n",
      "Epoch 63/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0566 - val_loss: 1.3825 - learning_rate: 2.0000e-04\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3/3 - 0s - 39ms/step - loss: 1.0687 - val_loss: 1.3816 - learning_rate: 2.0000e-04\n",
      "Epoch 65/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0447 - val_loss: 1.3738 - learning_rate: 4.0000e-05\n",
      "Epoch 66/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0628 - val_loss: 1.3711 - learning_rate: 4.0000e-05\n",
      "Epoch 67/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0690 - val_loss: 1.3701 - learning_rate: 4.0000e-05\n",
      "Epoch 68/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0706 - val_loss: 1.3743 - learning_rate: 4.0000e-05\n",
      "Epoch 69/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0600 - val_loss: 1.3771 - learning_rate: 4.0000e-05\n",
      "Epoch 70/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0538 - val_loss: 1.3759 - learning_rate: 4.0000e-05\n",
      "Epoch 71/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0613 - val_loss: 1.3712 - learning_rate: 4.0000e-05\n",
      "Epoch 72/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0478 - val_loss: 1.3708 - learning_rate: 4.0000e-05\n",
      "Epoch 73/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0558 - val_loss: 1.3685 - learning_rate: 4.0000e-05\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "3/3 - 0s - 38ms/step - loss: 1.0540 - val_loss: 1.3596 - learning_rate: 4.0000e-05\n",
      "Epoch 75/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0704 - val_loss: 1.3587 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0557 - val_loss: 1.3582 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0490 - val_loss: 1.3565 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0471 - val_loss: 1.3558 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0569 - val_loss: 1.3545 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0389 - val_loss: 1.3525 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0567 - val_loss: 1.3506 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0730 - val_loss: 1.3502 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0527 - val_loss: 1.3506 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0594 - val_loss: 1.3520 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0653 - val_loss: 1.3522 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0468 - val_loss: 1.3526 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0640 - val_loss: 1.3529 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0519 - val_loss: 1.3513 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0544 - val_loss: 1.3514 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0537 - val_loss: 1.3510 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0555 - val_loss: 1.3509 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0559 - val_loss: 1.3500 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "3/3 - 0s - 41ms/step - loss: 1.0565 - val_loss: 1.3492 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0506 - val_loss: 1.3486 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0637 - val_loss: 1.3483 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "3/3 - 0s - 42ms/step - loss: 1.0504 - val_loss: 1.3488 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0557 - val_loss: 1.3491 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "3/3 - 0s - 31ms/step - loss: 1.0596 - val_loss: 1.3507 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0493 - val_loss: 1.3497 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0760 - val_loss: 1.3501 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0410 - val_loss: 1.3490 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0585 - val_loss: 1.3484 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0694 - val_loss: 1.3482 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0600 - val_loss: 1.3478 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0529 - val_loss: 1.3463 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0548 - val_loss: 1.3447 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0395 - val_loss: 1.3431 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0626 - val_loss: 1.3418 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0499 - val_loss: 1.3404 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0490 - val_loss: 1.3392 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0537 - val_loss: 1.3379 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0448 - val_loss: 1.3374 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0461 - val_loss: 1.3381 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0432 - val_loss: 1.3400 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "3/3 - 0s - 31ms/step - loss: 1.0583 - val_loss: 1.3402 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0556 - val_loss: 1.3414 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0607 - val_loss: 1.3428 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0394 - val_loss: 1.3456 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0409 - val_loss: 1.3473 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0515 - val_loss: 1.3476 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0615 - val_loss: 1.3476 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "3/3 - 0s - 49ms/step - loss: 1.0488 - val_loss: 1.3473 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0578 - val_loss: 1.3462 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0626 - val_loss: 1.3454 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0465 - val_loss: 1.3446 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0591 - val_loss: 1.3448 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0497 - val_loss: 1.3447 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0448 - val_loss: 1.3453 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0648 - val_loss: 1.3465 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0507 - val_loss: 1.3447 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0515 - val_loss: 1.3427 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0395 - val_loss: 1.3427 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0569 - val_loss: 1.3418 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0791 - val_loss: 1.3430 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0410 - val_loss: 1.3433 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0623 - val_loss: 1.3426 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "3/3 - 0s - 43ms/step - loss: 1.0610 - val_loss: 1.3415 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0585 - val_loss: 1.3415 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0578 - val_loss: 1.3417 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0568 - val_loss: 1.3419 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0384 - val_loss: 1.3427 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0447 - val_loss: 1.3439 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0420 - val_loss: 1.3449 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0488 - val_loss: 1.3472 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0617 - val_loss: 1.3496 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0435 - val_loss: 1.3519 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0509 - val_loss: 1.3542 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0426 - val_loss: 1.3555 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0408 - val_loss: 1.3549 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0538 - val_loss: 1.3540 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "3/3 - 0s - 31ms/step - loss: 1.0483 - val_loss: 1.3528 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0420 - val_loss: 1.3522 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0538 - val_loss: 1.3512 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0576 - val_loss: 1.3499 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0553 - val_loss: 1.3494 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0488 - val_loss: 1.3492 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0260 - val_loss: 1.3468 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0528 - val_loss: 1.3449 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0470 - val_loss: 1.3432 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0410 - val_loss: 1.3427 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0380 - val_loss: 1.3417 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0393 - val_loss: 1.3413 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0488 - val_loss: 1.3414 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0455 - val_loss: 1.3416 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0455 - val_loss: 1.3410 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0482 - val_loss: 1.3450 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0526 - val_loss: 1.3513 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0442 - val_loss: 1.3488 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0487 - val_loss: 1.3470 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0570 - val_loss: 1.3497 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0478 - val_loss: 1.3480 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0470 - val_loss: 1.3482 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0450 - val_loss: 1.3489 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0631 - val_loss: 1.3435 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0388 - val_loss: 1.3399 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0568 - val_loss: 1.3392 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0503 - val_loss: 1.3414 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0493 - val_loss: 1.3418 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "3/3 - 0s - 31ms/step - loss: 1.0512 - val_loss: 1.3427 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0435 - val_loss: 1.3456 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0609 - val_loss: 1.3462 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0382 - val_loss: 1.3440 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0486 - val_loss: 1.3414 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0368 - val_loss: 1.3436 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0574 - val_loss: 1.3466 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "3/3 - 0s - 34ms/step - loss: 1.0603 - val_loss: 1.3497 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0365 - val_loss: 1.3499 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0475 - val_loss: 1.3481 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "3/3 - 0s - 36ms/step - loss: 1.0432 - val_loss: 1.3481 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0382 - val_loss: 1.3474 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "3/3 - 0s - 37ms/step - loss: 1.0632 - val_loss: 1.3444 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0661 - val_loss: 1.3453 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0364 - val_loss: 1.3472 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "3/3 - 0s - 33ms/step - loss: 1.0502 - val_loss: 1.3502 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "3/3 - 0s - 35ms/step - loss: 1.0456 - val_loss: 1.3520 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "3/3 - 0s - 38ms/step - loss: 1.0334 - val_loss: 1.3531 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0381 - val_loss: 1.3530 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "3/3 - 0s - 40ms/step - loss: 1.0402 - val_loss: 1.3489 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "3/3 - 0s - 39ms/step - loss: 1.0386 - val_loss: 1.3415 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "3/3 - 0s - 32ms/step - loss: 1.0395 - val_loss: 1.3361 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 6\n",
      "Epoch 1/200\n",
      "43/43 - 9s - 208ms/step - loss: 1.9405 - val_loss: 1.7975 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "43/43 - 3s - 64ms/step - loss: 1.3681 - val_loss: 1.3017 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "43/43 - 3s - 64ms/step - loss: 1.1717 - val_loss: 1.4364 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "43/43 - 3s - 65ms/step - loss: 1.0599 - val_loss: 1.4871 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.9797 - val_loss: 0.8973 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.9260 - val_loss: 1.1073 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.8798 - val_loss: 0.9480 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.8446 - val_loss: 1.3290 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.8180 - val_loss: 0.9524 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7932 - val_loss: 0.8873 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7764 - val_loss: 1.0688 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7584 - val_loss: 0.9447 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7575 - val_loss: 0.8680 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7356 - val_loss: 1.0016 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.7249 - val_loss: 0.8971 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7154 - val_loss: 1.0485 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7128 - val_loss: 1.1679 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.7013 - val_loss: 0.9759 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6952 - val_loss: 0.8996 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6870 - val_loss: 0.9002 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6847 - val_loss: 1.0686 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6783 - val_loss: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "43/43 - 3s - 65ms/step - loss: 0.6739 - val_loss: 1.0255 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6668 - val_loss: 0.9865 - learning_rate: 2.0000e-04\n",
      "Epoch 25/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6653 - val_loss: 0.9508 - learning_rate: 2.0000e-04\n",
      "Epoch 26/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6662 - val_loss: 0.9566 - learning_rate: 2.0000e-04\n",
      "Epoch 27/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6646 - val_loss: 0.9289 - learning_rate: 2.0000e-04\n",
      "Epoch 28/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6619 - val_loss: 1.0020 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6628 - val_loss: 0.9578 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6620 - val_loss: 1.0038 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6615 - val_loss: 0.9511 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6606 - val_loss: 1.0862 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "43/43 - 3s - 65ms/step - loss: 0.6604 - val_loss: 0.9768 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6578 - val_loss: 0.9776 - learning_rate: 4.0000e-05\n",
      "Epoch 35/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6569 - val_loss: 0.9854 - learning_rate: 4.0000e-05\n",
      "Epoch 36/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6571 - val_loss: 0.9811 - learning_rate: 4.0000e-05\n",
      "Epoch 37/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6573 - val_loss: 0.9630 - learning_rate: 4.0000e-05\n",
      "Epoch 38/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6561 - val_loss: 0.9836 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6568 - val_loss: 0.9736 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6566 - val_loss: 0.9545 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "43/43 - 3s - 64ms/step - loss: 0.6566 - val_loss: 0.9666 - learning_rate: 4.0000e-05\n",
      "Epoch 42/200\n",
      "43/43 - 3s - 64ms/step - loss: 0.6567 - val_loss: 0.9743 - learning_rate: 4.0000e-05\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "43/43 - 3s - 65ms/step - loss: 0.6560 - val_loss: 0.9830 - learning_rate: 4.0000e-05\n",
      "Epoch 44/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6557 - val_loss: 0.9837 - learning_rate: 1.0000e-05\n",
      "Epoch 45/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6551 - val_loss: 0.9867 - learning_rate: 1.0000e-05\n",
      "Epoch 46/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6552 - val_loss: 0.9803 - learning_rate: 1.0000e-05\n",
      "Epoch 47/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6546 - val_loss: 0.9725 - learning_rate: 1.0000e-05\n",
      "Epoch 48/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6556 - val_loss: 0.9678 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6551 - val_loss: 0.9618 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6559 - val_loss: 0.9769 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "43/43 - 3s - 67ms/step - loss: 0.6553 - val_loss: 0.9680 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6547 - val_loss: 0.9662 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6563 - val_loss: 0.9652 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6536 - val_loss: 0.9747 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6548 - val_loss: 0.9840 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6551 - val_loss: 0.9786 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6552 - val_loss: 0.9718 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6540 - val_loss: 0.9646 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6549 - val_loss: 0.9752 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6540 - val_loss: 0.9830 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6546 - val_loss: 0.9745 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6536 - val_loss: 0.9833 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6548 - val_loss: 0.9853 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6543 - val_loss: 0.9752 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6533 - val_loss: 0.9748 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6533 - val_loss: 0.9742 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6551 - val_loss: 0.9700 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6535 - val_loss: 0.9698 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6541 - val_loss: 0.9782 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6538 - val_loss: 0.9804 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6546 - val_loss: 0.9851 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6531 - val_loss: 0.9751 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6520 - val_loss: 0.9730 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6531 - val_loss: 0.9693 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6530 - val_loss: 0.9769 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6541 - val_loss: 0.9765 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6527 - val_loss: 0.9692 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6534 - val_loss: 0.9819 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6533 - val_loss: 0.9737 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6521 - val_loss: 0.9885 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6538 - val_loss: 0.9832 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6520 - val_loss: 0.9791 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6519 - val_loss: 0.9736 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6519 - val_loss: 0.9719 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6523 - val_loss: 0.9706 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6515 - val_loss: 0.9784 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6522 - val_loss: 0.9864 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6515 - val_loss: 0.9888 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6522 - val_loss: 0.9856 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6510 - val_loss: 0.9834 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6522 - val_loss: 0.9744 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6518 - val_loss: 0.9708 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6519 - val_loss: 0.9744 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6509 - val_loss: 0.9768 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6518 - val_loss: 0.9697 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6515 - val_loss: 0.9703 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6510 - val_loss: 0.9692 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6502 - val_loss: 0.9755 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6498 - val_loss: 0.9817 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "43/43 - 3s - 67ms/step - loss: 0.6500 - val_loss: 0.9748 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "43/43 - 3s - 67ms/step - loss: 0.6512 - val_loss: 0.9664 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6507 - val_loss: 0.9779 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6499 - val_loss: 0.9767 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6501 - val_loss: 0.9798 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6500 - val_loss: 0.9811 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6500 - val_loss: 0.9605 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6499 - val_loss: 0.9780 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6488 - val_loss: 0.9887 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6492 - val_loss: 0.9689 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6492 - val_loss: 0.9648 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6502 - val_loss: 0.9664 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6490 - val_loss: 0.9683 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6500 - val_loss: 0.9628 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6492 - val_loss: 0.9730 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6494 - val_loss: 0.9733 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6477 - val_loss: 0.9701 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6479 - val_loss: 0.9735 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6487 - val_loss: 0.9568 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6476 - val_loss: 0.9672 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6472 - val_loss: 0.9859 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6466 - val_loss: 0.9598 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6474 - val_loss: 0.9853 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.6468 - val_loss: 0.9974 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6461 - val_loss: 1.0163 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6438 - val_loss: 0.9870 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6372 - val_loss: 1.1471 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.6121 - val_loss: 1.0619 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.5753 - val_loss: 0.9253 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.5461 - val_loss: 0.9398 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.5345 - val_loss: 0.8853 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.5191 - val_loss: 0.9333 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4956 - val_loss: 1.1532 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4923 - val_loss: 1.7381 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4996 - val_loss: 1.4052 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.4780 - val_loss: 1.5954 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4803 - val_loss: 1.2280 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4697 - val_loss: 1.0414 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4640 - val_loss: 1.1466 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4536 - val_loss: 1.3316 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4534 - val_loss: 0.9732 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.4598 - val_loss: 1.1246 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4482 - val_loss: 1.4107 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4430 - val_loss: 1.7651 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4655 - val_loss: 1.4438 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4329 - val_loss: 1.6577 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4342 - val_loss: 1.2451 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.4247 - val_loss: 1.3227 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4300 - val_loss: 0.9060 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4259 - val_loss: 1.5856 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4311 - val_loss: 1.1675 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4115 - val_loss: 1.3341 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4181 - val_loss: 1.8680 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.4125 - val_loss: 1.6047 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.4058 - val_loss: 0.9814 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "43/43 - 3s - 64ms/step - loss: 0.4091 - val_loss: 1.3638 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3968 - val_loss: 1.5918 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3928 - val_loss: 1.2195 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3809 - val_loss: 1.2885 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.3753 - val_loss: 1.7322 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3803 - val_loss: 0.8483 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3668 - val_loss: 1.3776 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3702 - val_loss: 0.7232 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3958 - val_loss: 1.4817 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3575 - val_loss: 1.7883 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.3592 - val_loss: 1.7644 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3495 - val_loss: 1.3092 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3232 - val_loss: 0.8831 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3240 - val_loss: 1.3749 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.3156 - val_loss: 1.1204 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2964 - val_loss: 1.3305 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.2926 - val_loss: 1.1275 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.2931 - val_loss: 0.9572 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2799 - val_loss: 1.7485 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2902 - val_loss: 0.7063 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2764 - val_loss: 1.6403 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2608 - val_loss: 0.5932 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.2741 - val_loss: 0.9079 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2425 - val_loss: 1.2581 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2370 - val_loss: 0.6554 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2403 - val_loss: 0.7103 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2315 - val_loss: 1.3588 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2338 - val_loss: 0.8725 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "43/43 - 3s - 67ms/step - loss: 0.2358 - val_loss: 0.6738 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.2084 - val_loss: 0.3497 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.2096 - val_loss: 0.3073 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1931 - val_loss: 0.2948 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "43/43 - 3s - 67ms/step - loss: 0.1939 - val_loss: 0.3102 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.1817 - val_loss: 0.1758 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.1682 - val_loss: 0.2120 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1785 - val_loss: 0.2063 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1554 - val_loss: 0.2100 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1506 - val_loss: 0.1507 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1505 - val_loss: 0.2472 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1324 - val_loss: 0.1567 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "43/43 - 3s - 66ms/step - loss: 0.1497 - val_loss: 0.1541 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1277 - val_loss: 0.2687 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1287 - val_loss: 0.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1140 - val_loss: 0.1564 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1199 - val_loss: 0.1595 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "43/43 - 3s - 65ms/step - loss: 0.1060 - val_loss: 0.1442 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 7\n",
      "Epoch 1/200\n",
      "18/18 - 7s - 416ms/step - loss: 2.2365 - val_loss: 2.1350 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "18/18 - 1s - 68ms/step - loss: 1.7004 - val_loss: 1.7269 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "18/18 - 1s - 67ms/step - loss: 1.4616 - val_loss: 1.2897 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "18/18 - 1s - 68ms/step - loss: 1.3161 - val_loss: 1.4302 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "18/18 - 1s - 68ms/step - loss: 1.2289 - val_loss: 1.2189 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "18/18 - 1s - 70ms/step - loss: 1.1638 - val_loss: 1.4716 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "18/18 - 1s - 67ms/step - loss: 1.1121 - val_loss: 1.2539 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "18/18 - 1s - 68ms/step - loss: 1.0693 - val_loss: 1.0923 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "18/18 - 1s - 70ms/step - loss: 1.0342 - val_loss: 1.1406 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "18/18 - 1s - 68ms/step - loss: 1.0058 - val_loss: 1.1223 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.9755 - val_loss: 1.1126 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.9523 - val_loss: 1.1686 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.9338 - val_loss: 1.0520 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.9086 - val_loss: 0.9413 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.8898 - val_loss: 1.0411 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.8720 - val_loss: 1.0888 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.8628 - val_loss: 0.7064 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.8661 - val_loss: 1.0861 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.8400 - val_loss: 0.9273 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.8257 - val_loss: 1.1127 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.8191 - val_loss: 1.0779 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.8134 - val_loss: 0.8855 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7977 - val_loss: 0.9610 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7891 - val_loss: 0.8816 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7849 - val_loss: 1.0548 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7741 - val_loss: 1.1747 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "18/18 - 1s - 69ms/step - loss: 0.7813 - val_loss: 0.7945 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7654 - val_loss: 1.0713 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7622 - val_loss: 0.8655 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7603 - val_loss: 0.9721 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7597 - val_loss: 0.9198 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7549 - val_loss: 0.9408 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "18/18 - 1s - 75ms/step - loss: 0.7524 - val_loss: 0.9457 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "18/18 - 1s - 79ms/step - loss: 0.7501 - val_loss: 0.9057 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7532 - val_loss: 0.8986 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7524 - val_loss: 0.9980 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "18/18 - 1s - 69ms/step - loss: 0.7483 - val_loss: 0.9190 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7469 - val_loss: 0.9430 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7472 - val_loss: 0.9433 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7484 - val_loss: 0.9528 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7467 - val_loss: 0.9376 - learning_rate: 4.0000e-05\n",
      "Epoch 42/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7464 - val_loss: 0.9368 - learning_rate: 4.0000e-05\n",
      "Epoch 43/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7456 - val_loss: 0.9302 - learning_rate: 4.0000e-05\n",
      "Epoch 44/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7434 - val_loss: 0.9439 - learning_rate: 4.0000e-05\n",
      "Epoch 45/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7435 - val_loss: 0.9253 - learning_rate: 4.0000e-05\n",
      "Epoch 46/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7416 - val_loss: 0.9257 - learning_rate: 4.0000e-05\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "18/18 - 1s - 71ms/step - loss: 0.7447 - val_loss: 0.9390 - learning_rate: 4.0000e-05\n",
      "Epoch 48/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7450 - val_loss: 0.9388 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7438 - val_loss: 0.9389 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7407 - val_loss: 0.9391 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7440 - val_loss: 0.9349 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7431 - val_loss: 0.9410 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7436 - val_loss: 0.9429 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7425 - val_loss: 0.9308 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7419 - val_loss: 0.9325 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7425 - val_loss: 0.9364 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7438 - val_loss: 0.9407 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7414 - val_loss: 0.9352 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7435 - val_loss: 0.9333 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7435 - val_loss: 0.9402 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7428 - val_loss: 0.9368 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7408 - val_loss: 0.9359 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7414 - val_loss: 0.9338 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7401 - val_loss: 0.9358 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7429 - val_loss: 0.9322 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7423 - val_loss: 0.9373 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7401 - val_loss: 0.9351 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7394 - val_loss: 0.9327 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7396 - val_loss: 0.9341 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7417 - val_loss: 0.9338 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7403 - val_loss: 0.9343 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7417 - val_loss: 0.9407 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7408 - val_loss: 0.9393 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7415 - val_loss: 0.9372 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7417 - val_loss: 0.9407 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7424 - val_loss: 0.9370 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "18/18 - 1s - 72ms/step - loss: 0.7394 - val_loss: 0.9300 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7407 - val_loss: 0.9303 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7420 - val_loss: 0.9310 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7378 - val_loss: 0.9358 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7394 - val_loss: 0.9407 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7378 - val_loss: 0.9303 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7428 - val_loss: 0.9396 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7374 - val_loss: 0.9357 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7411 - val_loss: 0.9328 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7409 - val_loss: 0.9293 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7399 - val_loss: 0.9297 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7384 - val_loss: 0.9325 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7379 - val_loss: 0.9441 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7397 - val_loss: 0.9240 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7343 - val_loss: 0.9317 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7342 - val_loss: 0.9417 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7350 - val_loss: 0.9351 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7375 - val_loss: 0.9366 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7347 - val_loss: 0.9410 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7357 - val_loss: 0.9216 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7367 - val_loss: 0.9272 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7349 - val_loss: 0.9381 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7355 - val_loss: 0.9476 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7324 - val_loss: 0.9198 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7354 - val_loss: 0.9438 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7346 - val_loss: 0.9434 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7319 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7334 - val_loss: 0.9433 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7331 - val_loss: 0.9177 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7352 - val_loss: 0.9545 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7292 - val_loss: 0.9333 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7265 - val_loss: 0.9683 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7308 - val_loss: 0.9229 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7294 - val_loss: 0.9338 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7272 - val_loss: 0.9520 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7283 - val_loss: 0.9724 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.7277 - val_loss: 1.0148 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7241 - val_loss: 0.9201 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7229 - val_loss: 0.9531 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7203 - val_loss: 1.0277 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7209 - val_loss: 0.9847 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7169 - val_loss: 0.8964 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7207 - val_loss: 1.0659 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7156 - val_loss: 0.8766 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7106 - val_loss: 1.0629 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.7130 - val_loss: 0.9557 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.7067 - val_loss: 1.0393 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7020 - val_loss: 1.0550 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.7037 - val_loss: 0.9814 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7034 - val_loss: 0.8840 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.7013 - val_loss: 1.0807 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.6994 - val_loss: 1.0863 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6979 - val_loss: 1.0829 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6943 - val_loss: 0.9479 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6896 - val_loss: 1.0508 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6874 - val_loss: 1.0071 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.6859 - val_loss: 1.0511 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6825 - val_loss: 1.0681 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6821 - val_loss: 0.9032 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6788 - val_loss: 1.0178 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6699 - val_loss: 1.0080 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6683 - val_loss: 1.0690 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6731 - val_loss: 0.9924 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6670 - val_loss: 0.9801 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6656 - val_loss: 1.1478 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6668 - val_loss: 0.8918 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6664 - val_loss: 1.2708 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.6646 - val_loss: 0.8785 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6658 - val_loss: 0.9437 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6568 - val_loss: 1.1449 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6438 - val_loss: 0.9188 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6438 - val_loss: 0.9185 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.6500 - val_loss: 1.0242 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6515 - val_loss: 1.2636 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6428 - val_loss: 0.9613 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6380 - val_loss: 1.2183 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.6303 - val_loss: 0.9435 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.6260 - val_loss: 1.2933 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "18/18 - 1s - 67ms/step - loss: 0.6307 - val_loss: 1.3060 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6276 - val_loss: 1.2336 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6220 - val_loss: 0.9045 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6336 - val_loss: 1.0753 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "18/18 - 1s - 72ms/step - loss: 0.6307 - val_loss: 1.4439 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.6246 - val_loss: 1.1660 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6234 - val_loss: 0.9967 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6041 - val_loss: 0.9502 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6044 - val_loss: 1.0310 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6044 - val_loss: 1.1030 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6002 - val_loss: 1.1868 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.6043 - val_loss: 0.8848 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.6079 - val_loss: 1.0203 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5974 - val_loss: 1.2482 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5936 - val_loss: 1.2736 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5905 - val_loss: 1.0822 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.5934 - val_loss: 1.1158 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5910 - val_loss: 0.8668 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.5967 - val_loss: 0.9397 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5790 - val_loss: 1.1678 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5877 - val_loss: 0.9603 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.5827 - val_loss: 1.3044 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5748 - val_loss: 1.0320 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5688 - val_loss: 1.3313 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.5668 - val_loss: 1.1869 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5559 - val_loss: 1.0283 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.5560 - val_loss: 1.0512 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.5613 - val_loss: 1.2536 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5541 - val_loss: 1.2914 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "18/18 - 1s - 72ms/step - loss: 0.5594 - val_loss: 1.1057 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5606 - val_loss: 1.2652 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5648 - val_loss: 1.2647 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "18/18 - 1s - 74ms/step - loss: 0.5373 - val_loss: 1.0039 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "18/18 - 1s - 71ms/step - loss: 0.5440 - val_loss: 1.1381 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5577 - val_loss: 1.3937 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5619 - val_loss: 1.1886 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5495 - val_loss: 1.1318 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "18/18 - 1s - 72ms/step - loss: 0.5374 - val_loss: 1.0479 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "18/18 - 1s - 72ms/step - loss: 0.5391 - val_loss: 1.2186 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5370 - val_loss: 1.2573 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "18/18 - 1s - 69ms/step - loss: 0.5453 - val_loss: 1.3643 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5461 - val_loss: 1.4118 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5292 - val_loss: 1.2048 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5360 - val_loss: 1.2206 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "18/18 - 1s - 68ms/step - loss: 0.5326 - val_loss: 1.2260 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "18/18 - 1s - 70ms/step - loss: 0.5304 - val_loss: 1.1576 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 8\n",
      "Epoch 1/200\n",
      "19/19 - 7s - 390ms/step - loss: 2.1757 - val_loss: 1.8782 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.6729 - val_loss: 1.4359 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.4451 - val_loss: 1.4761 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.2993 - val_loss: 1.3784 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "19/19 - 1s - 69ms/step - loss: 1.1912 - val_loss: 1.3182 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.1301 - val_loss: 1.2882 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "19/19 - 1s - 67ms/step - loss: 1.0686 - val_loss: 1.2174 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "19/19 - 1s - 68ms/step - loss: 1.0143 - val_loss: 1.2251 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.9840 - val_loss: 1.2472 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.9546 - val_loss: 0.9881 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.9272 - val_loss: 0.9651 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.9025 - val_loss: 0.9151 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.8740 - val_loss: 0.7813 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.8573 - val_loss: 1.0689 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.8361 - val_loss: 1.0051 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.8247 - val_loss: 0.9922 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.8184 - val_loss: 0.7773 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.8042 - val_loss: 0.9777 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7802 - val_loss: 0.8680 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.7739 - val_loss: 1.0582 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7664 - val_loss: 0.9099 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7566 - val_loss: 0.8320 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.7449 - val_loss: 0.9048 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7365 - val_loss: 0.7927 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7328 - val_loss: 0.8767 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7256 - val_loss: 0.8903 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "19/19 - 1s - 68ms/step - loss: 0.7210 - val_loss: 0.8958 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.7131 - val_loss: 0.9120 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7108 - val_loss: 0.8089 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7087 - val_loss: 0.9089 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.7098 - val_loss: 0.8799 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7072 - val_loss: 0.8607 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.7048 - val_loss: 0.9204 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.7025 - val_loss: 0.8677 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.7043 - val_loss: 0.8452 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.7034 - val_loss: 0.7735 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6978 - val_loss: 0.9203 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6987 - val_loss: 0.8139 - learning_rate: 2.0000e-04\n",
      "Epoch 39/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6968 - val_loss: 0.8806 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6944 - val_loss: 0.8961 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6968 - val_loss: 0.8594 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6954 - val_loss: 0.8864 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6930 - val_loss: 0.7907 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6914 - val_loss: 0.8632 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6889 - val_loss: 0.8758 - learning_rate: 2.0000e-04\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "19/19 - 1s - 67ms/step - loss: 0.6881 - val_loss: 0.7945 - learning_rate: 2.0000e-04\n",
      "Epoch 47/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6867 - val_loss: 0.8123 - learning_rate: 4.0000e-05\n",
      "Epoch 48/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6884 - val_loss: 0.8429 - learning_rate: 4.0000e-05\n",
      "Epoch 49/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6884 - val_loss: 0.8422 - learning_rate: 4.0000e-05\n",
      "Epoch 50/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6881 - val_loss: 0.8301 - learning_rate: 4.0000e-05\n",
      "Epoch 51/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6854 - val_loss: 0.8382 - learning_rate: 4.0000e-05\n",
      "Epoch 52/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6860 - val_loss: 0.8415 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6855 - val_loss: 0.8436 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6839 - val_loss: 0.8290 - learning_rate: 4.0000e-05\n",
      "Epoch 55/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6863 - val_loss: 0.8246 - learning_rate: 4.0000e-05\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "19/19 - 1s - 67ms/step - loss: 0.6865 - val_loss: 0.8451 - learning_rate: 4.0000e-05\n",
      "Epoch 57/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6840 - val_loss: 0.8389 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6842 - val_loss: 0.8364 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6839 - val_loss: 0.8373 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6846 - val_loss: 0.8385 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6842 - val_loss: 0.8377 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6848 - val_loss: 0.8381 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6827 - val_loss: 0.8352 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6832 - val_loss: 0.8356 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6849 - val_loss: 0.8375 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6831 - val_loss: 0.8300 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6832 - val_loss: 0.8336 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6847 - val_loss: 0.8317 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6847 - val_loss: 0.8398 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6847 - val_loss: 0.8401 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6864 - val_loss: 0.8366 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6831 - val_loss: 0.8364 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6822 - val_loss: 0.8309 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6845 - val_loss: 0.8362 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6821 - val_loss: 0.8293 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6815 - val_loss: 0.8319 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6825 - val_loss: 0.8367 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6836 - val_loss: 0.8369 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6819 - val_loss: 0.8363 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6827 - val_loss: 0.8385 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6821 - val_loss: 0.8330 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6821 - val_loss: 0.8281 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6844 - val_loss: 0.8333 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6850 - val_loss: 0.8330 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6845 - val_loss: 0.8407 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6823 - val_loss: 0.8328 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6815 - val_loss: 0.8380 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6816 - val_loss: 0.8350 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6807 - val_loss: 0.8310 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6834 - val_loss: 0.8274 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6828 - val_loss: 0.8373 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6814 - val_loss: 0.8385 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6826 - val_loss: 0.8434 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6810 - val_loss: 0.8357 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6812 - val_loss: 0.8350 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6800 - val_loss: 0.8330 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6829 - val_loss: 0.8308 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6812 - val_loss: 0.8261 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6801 - val_loss: 0.8350 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6808 - val_loss: 0.8329 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6828 - val_loss: 0.8343 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6809 - val_loss: 0.8387 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6837 - val_loss: 0.8346 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6813 - val_loss: 0.8358 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6828 - val_loss: 0.8335 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6807 - val_loss: 0.8324 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6809 - val_loss: 0.8392 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6811 - val_loss: 0.8376 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6808 - val_loss: 0.8381 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6805 - val_loss: 0.8417 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6786 - val_loss: 0.8307 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6810 - val_loss: 0.8392 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6800 - val_loss: 0.8345 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6802 - val_loss: 0.8328 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6799 - val_loss: 0.8348 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6791 - val_loss: 0.8351 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6797 - val_loss: 0.8312 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6785 - val_loss: 0.8343 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6804 - val_loss: 0.8349 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6807 - val_loss: 0.8298 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6782 - val_loss: 0.8310 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6789 - val_loss: 0.8276 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6787 - val_loss: 0.8341 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6794 - val_loss: 0.8365 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6794 - val_loss: 0.8300 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6780 - val_loss: 0.8281 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6784 - val_loss: 0.8238 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6782 - val_loss: 0.8262 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6797 - val_loss: 0.8298 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6796 - val_loss: 0.8296 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6780 - val_loss: 0.8279 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6782 - val_loss: 0.8384 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "19/19 - 1s - 71ms/step - loss: 0.6762 - val_loss: 0.8298 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6792 - val_loss: 0.8302 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6798 - val_loss: 0.8314 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6762 - val_loss: 0.8248 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6771 - val_loss: 0.8328 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6758 - val_loss: 0.8290 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6782 - val_loss: 0.8267 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6778 - val_loss: 0.8339 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6756 - val_loss: 0.8230 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6771 - val_loss: 0.8227 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6749 - val_loss: 0.8289 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6765 - val_loss: 0.8237 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6755 - val_loss: 0.8238 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6759 - val_loss: 0.8242 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6739 - val_loss: 0.8209 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6741 - val_loss: 0.8311 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6752 - val_loss: 0.8260 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6763 - val_loss: 0.8284 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6729 - val_loss: 0.8299 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6731 - val_loss: 0.8262 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6754 - val_loss: 0.8205 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6721 - val_loss: 0.8251 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6737 - val_loss: 0.8173 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6752 - val_loss: 0.8203 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6726 - val_loss: 0.8163 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6718 - val_loss: 0.8128 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6723 - val_loss: 0.8159 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6746 - val_loss: 0.8152 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6721 - val_loss: 0.8160 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6695 - val_loss: 0.8244 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "19/19 - 1s - 72ms/step - loss: 0.6694 - val_loss: 0.8025 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6725 - val_loss: 0.8011 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6672 - val_loss: 0.8067 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6686 - val_loss: 0.8250 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6660 - val_loss: 0.8127 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6646 - val_loss: 0.7976 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6650 - val_loss: 0.8470 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6602 - val_loss: 0.8091 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "19/19 - 1s - 66ms/step - loss: 0.6609 - val_loss: 0.8042 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6556 - val_loss: 0.7821 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6534 - val_loss: 0.8044 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6485 - val_loss: 0.8054 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6431 - val_loss: 0.8154 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6400 - val_loss: 0.8834 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6409 - val_loss: 0.8013 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.6247 - val_loss: 0.7856 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6229 - val_loss: 0.8423 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.6151 - val_loss: 0.8167 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.6134 - val_loss: 0.7227 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.6049 - val_loss: 0.8277 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "19/19 - 1s - 70ms/step - loss: 0.5936 - val_loss: 0.7602 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5943 - val_loss: 0.8613 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5864 - val_loss: 0.7978 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5946 - val_loss: 0.7823 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5850 - val_loss: 0.8805 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5721 - val_loss: 0.6781 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5729 - val_loss: 0.7858 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5679 - val_loss: 0.6853 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5573 - val_loss: 0.7288 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5599 - val_loss: 0.6439 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5551 - val_loss: 0.6298 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5434 - val_loss: 0.6523 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5355 - val_loss: 0.6478 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5319 - val_loss: 0.7561 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5282 - val_loss: 0.6739 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "19/19 - 1s - 69ms/step - loss: 0.5248 - val_loss: 0.6696 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "19/19 - 1s - 68ms/step - loss: 0.5139 - val_loss: 0.6070 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "19/19 - 1s - 67ms/step - loss: 0.5113 - val_loss: 0.6707 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 9\n",
      "Epoch 1/200\n",
      "15/15 - 8s - 526ms/step - loss: 2.3979 - val_loss: 2.5958 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.8800 - val_loss: 1.7704 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "15/15 - 1s - 70ms/step - loss: 1.6615 - val_loss: 1.5562 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "15/15 - 1s - 70ms/step - loss: 1.5021 - val_loss: 1.7550 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.4039 - val_loss: 1.6526 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "15/15 - 1s - 70ms/step - loss: 1.3249 - val_loss: 1.3987 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.2706 - val_loss: 1.5026 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.2315 - val_loss: 1.4274 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.1869 - val_loss: 1.8236 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "15/15 - 1s - 70ms/step - loss: 1.1537 - val_loss: 1.4473 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "15/15 - 1s - 66ms/step - loss: 1.1208 - val_loss: 1.5337 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "15/15 - 1s - 68ms/step - loss: 1.0812 - val_loss: 1.1718 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.0659 - val_loss: 1.1440 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "15/15 - 1s - 68ms/step - loss: 1.0321 - val_loss: 1.4847 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "15/15 - 1s - 67ms/step - loss: 1.0216 - val_loss: 1.3218 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "15/15 - 1s - 71ms/step - loss: 0.9940 - val_loss: 1.1754 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.9795 - val_loss: 1.2472 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.9620 - val_loss: 1.5021 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.9505 - val_loss: 1.1295 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.9368 - val_loss: 1.3379 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.9276 - val_loss: 1.1514 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.9168 - val_loss: 1.1171 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.9003 - val_loss: 1.1594 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8903 - val_loss: 1.4523 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.8757 - val_loss: 1.0686 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8703 - val_loss: 1.2900 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8577 - val_loss: 1.2597 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8516 - val_loss: 1.1657 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8366 - val_loss: 1.3504 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8451 - val_loss: 1.3808 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8399 - val_loss: 1.1375 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.8221 - val_loss: 1.1274 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8198 - val_loss: 1.0890 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.8222 - val_loss: 0.9343 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8121 - val_loss: 0.9830 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.8116 - val_loss: 1.0278 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.8007 - val_loss: 1.0337 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7961 - val_loss: 1.1801 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7904 - val_loss: 1.1804 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7829 - val_loss: 0.8783 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7926 - val_loss: 1.0849 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7768 - val_loss: 1.2083 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7768 - val_loss: 1.2754 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7721 - val_loss: 1.2786 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7697 - val_loss: 1.2132 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7638 - val_loss: 1.0133 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "15/15 - 1s - 71ms/step - loss: 0.7692 - val_loss: 1.3101 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7614 - val_loss: 1.3499 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7614 - val_loss: 1.1833 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "15/15 - 1s - 68ms/step - loss: 0.7532 - val_loss: 1.1206 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7472 - val_loss: 1.1110 - learning_rate: 2.0000e-04\n",
      "Epoch 52/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7492 - val_loss: 1.0830 - learning_rate: 2.0000e-04\n",
      "Epoch 53/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7515 - val_loss: 1.1244 - learning_rate: 2.0000e-04\n",
      "Epoch 54/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7487 - val_loss: 1.1118 - learning_rate: 2.0000e-04\n",
      "Epoch 55/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7481 - val_loss: 1.1099 - learning_rate: 2.0000e-04\n",
      "Epoch 56/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7457 - val_loss: 1.0953 - learning_rate: 2.0000e-04\n",
      "Epoch 57/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7444 - val_loss: 1.1292 - learning_rate: 2.0000e-04\n",
      "Epoch 58/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7475 - val_loss: 1.1230 - learning_rate: 2.0000e-04\n",
      "Epoch 59/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7456 - val_loss: 1.1187 - learning_rate: 2.0000e-04\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "15/15 - 1s - 68ms/step - loss: 0.7395 - val_loss: 1.0978 - learning_rate: 2.0000e-04\n",
      "Epoch 61/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7421 - val_loss: 1.1282 - learning_rate: 4.0000e-05\n",
      "Epoch 62/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7390 - val_loss: 1.1178 - learning_rate: 4.0000e-05\n",
      "Epoch 63/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7425 - val_loss: 1.1004 - learning_rate: 4.0000e-05\n",
      "Epoch 64/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7428 - val_loss: 1.1025 - learning_rate: 4.0000e-05\n",
      "Epoch 65/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7431 - val_loss: 1.0969 - learning_rate: 4.0000e-05\n",
      "Epoch 66/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7403 - val_loss: 1.1116 - learning_rate: 4.0000e-05\n",
      "Epoch 67/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7430 - val_loss: 1.1174 - learning_rate: 4.0000e-05\n",
      "Epoch 68/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7431 - val_loss: 1.1329 - learning_rate: 4.0000e-05\n",
      "Epoch 69/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7430 - val_loss: 1.1037 - learning_rate: 4.0000e-05\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "15/15 - 1s - 68ms/step - loss: 0.7401 - val_loss: 1.1049 - learning_rate: 4.0000e-05\n",
      "Epoch 71/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7433 - val_loss: 1.1105 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7404 - val_loss: 1.1131 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7387 - val_loss: 1.1120 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7394 - val_loss: 1.1121 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7400 - val_loss: 1.1096 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.7397 - val_loss: 1.1102 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7381 - val_loss: 1.1076 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7399 - val_loss: 1.1105 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7412 - val_loss: 1.1087 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7401 - val_loss: 1.1080 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7429 - val_loss: 1.1153 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7400 - val_loss: 1.1143 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7393 - val_loss: 1.1134 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7408 - val_loss: 1.1089 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7407 - val_loss: 1.1118 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7403 - val_loss: 1.1126 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7386 - val_loss: 1.1133 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7396 - val_loss: 1.1122 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7419 - val_loss: 1.1075 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7399 - val_loss: 1.1064 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "15/15 - 1s - 71ms/step - loss: 0.7385 - val_loss: 1.1135 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.7425 - val_loss: 1.1138 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7393 - val_loss: 1.1132 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7401 - val_loss: 1.1132 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.7387 - val_loss: 1.1098 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7375 - val_loss: 1.1145 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7393 - val_loss: 1.1149 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7417 - val_loss: 1.1125 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7404 - val_loss: 1.1082 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7407 - val_loss: 1.1120 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7392 - val_loss: 1.1142 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7372 - val_loss: 1.1094 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7408 - val_loss: 1.1110 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7393 - val_loss: 1.1094 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7394 - val_loss: 1.1102 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7393 - val_loss: 1.1084 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7403 - val_loss: 1.1099 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.7399 - val_loss: 1.1130 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "15/15 - 1s - 72ms/step - loss: 0.7397 - val_loss: 1.1109 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.7413 - val_loss: 1.1109 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7425 - val_loss: 1.1124 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "15/15 - 1s - 71ms/step - loss: 0.7398 - val_loss: 1.1134 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7408 - val_loss: 1.1186 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7400 - val_loss: 1.1120 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7385 - val_loss: 1.1144 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7383 - val_loss: 1.1109 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7381 - val_loss: 1.1215 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7374 - val_loss: 1.1097 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7380 - val_loss: 1.1115 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7402 - val_loss: 1.1100 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7377 - val_loss: 1.1103 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7394 - val_loss: 1.1166 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7376 - val_loss: 1.1147 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7370 - val_loss: 1.1127 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7382 - val_loss: 1.1062 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7393 - val_loss: 1.1084 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7370 - val_loss: 1.1092 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7360 - val_loss: 1.1107 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7379 - val_loss: 1.1118 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7396 - val_loss: 1.1074 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7367 - val_loss: 1.1123 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7374 - val_loss: 1.1078 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7392 - val_loss: 1.1126 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7379 - val_loss: 1.1156 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7369 - val_loss: 1.1154 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7362 - val_loss: 1.1077 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7373 - val_loss: 1.1136 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7365 - val_loss: 1.1145 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7392 - val_loss: 1.1123 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7385 - val_loss: 1.1104 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7380 - val_loss: 1.1152 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7365 - val_loss: 1.1201 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7358 - val_loss: 1.1154 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7363 - val_loss: 1.1114 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7340 - val_loss: 1.1154 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7371 - val_loss: 1.1127 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7371 - val_loss: 1.1071 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7345 - val_loss: 1.1201 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7350 - val_loss: 1.1123 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7359 - val_loss: 1.1145 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7330 - val_loss: 1.1162 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7357 - val_loss: 1.1155 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7361 - val_loss: 1.1146 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7322 - val_loss: 1.1212 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7310 - val_loss: 1.1325 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7306 - val_loss: 1.1353 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7309 - val_loss: 1.1210 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7305 - val_loss: 1.1457 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7301 - val_loss: 1.1275 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7273 - val_loss: 1.1747 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7315 - val_loss: 1.1621 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7252 - val_loss: 1.1517 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7257 - val_loss: 1.1458 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7247 - val_loss: 1.1998 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.7250 - val_loss: 1.1177 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7204 - val_loss: 1.2918 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7207 - val_loss: 1.1856 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7152 - val_loss: 1.1157 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7155 - val_loss: 1.0652 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7219 - val_loss: 1.2342 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7156 - val_loss: 1.2215 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7109 - val_loss: 1.2033 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.7004 - val_loss: 1.1021 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7100 - val_loss: 1.0311 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7092 - val_loss: 1.1537 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.7005 - val_loss: 1.0838 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6993 - val_loss: 1.1009 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6987 - val_loss: 1.1762 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.7034 - val_loss: 1.3019 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6994 - val_loss: 1.2764 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6976 - val_loss: 1.2289 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.6966 - val_loss: 1.2739 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6894 - val_loss: 1.0852 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.6900 - val_loss: 1.1945 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.6897 - val_loss: 1.1470 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6839 - val_loss: 1.3377 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6895 - val_loss: 1.2750 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6885 - val_loss: 1.0273 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.6856 - val_loss: 1.1643 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6871 - val_loss: 1.2291 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6768 - val_loss: 1.1606 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6776 - val_loss: 1.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6808 - val_loss: 1.1883 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "15/15 - 1s - 70ms/step - loss: 0.6788 - val_loss: 1.0939 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "15/15 - 1s - 66ms/step - loss: 0.6687 - val_loss: 1.0643 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "15/15 - 1s - 69ms/step - loss: 0.6755 - val_loss: 1.1028 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "15/15 - 1s - 68ms/step - loss: 0.6675 - val_loss: 1.3601 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6761 - val_loss: 1.4628 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6752 - val_loss: 1.3505 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "15/15 - 1s - 67ms/step - loss: 0.6690 - val_loss: 1.2076 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 10\n",
      "Epoch 1/200\n",
      "14/14 - 7s - 512ms/step - loss: 2.3487 - val_loss: 2.6549 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.8215 - val_loss: 1.8978 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.5810 - val_loss: 2.0508 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.4327 - val_loss: 1.5011 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.3344 - val_loss: 1.2380 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.2613 - val_loss: 1.7146 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "14/14 - 1s - 65ms/step - loss: 1.2065 - val_loss: 1.6745 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.1604 - val_loss: 1.7248 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.1178 - val_loss: 1.4231 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.0931 - val_loss: 1.5326 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "14/14 - 1s - 65ms/step - loss: 1.0617 - val_loss: 1.5849 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.0326 - val_loss: 1.3141 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "14/14 - 1s - 66ms/step - loss: 1.0092 - val_loss: 1.4573 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9849 - val_loss: 1.5670 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "14/14 - 1s - 66ms/step - loss: 0.9672 - val_loss: 1.3103 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9527 - val_loss: 1.4535 - learning_rate: 2.0000e-04\n",
      "Epoch 17/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9444 - val_loss: 1.3528 - learning_rate: 2.0000e-04\n",
      "Epoch 18/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9475 - val_loss: 1.4488 - learning_rate: 2.0000e-04\n",
      "Epoch 19/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9410 - val_loss: 1.3417 - learning_rate: 2.0000e-04\n",
      "Epoch 20/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9364 - val_loss: 1.4271 - learning_rate: 2.0000e-04\n",
      "Epoch 21/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9320 - val_loss: 1.3430 - learning_rate: 2.0000e-04\n",
      "Epoch 22/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9301 - val_loss: 1.3819 - learning_rate: 2.0000e-04\n",
      "Epoch 23/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9248 - val_loss: 1.3569 - learning_rate: 2.0000e-04\n",
      "Epoch 24/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9225 - val_loss: 1.3644 - learning_rate: 2.0000e-04\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "14/14 - 1s - 66ms/step - loss: 0.9146 - val_loss: 1.3887 - learning_rate: 2.0000e-04\n",
      "Epoch 26/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9153 - val_loss: 1.4036 - learning_rate: 4.0000e-05\n",
      "Epoch 27/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9101 - val_loss: 1.3692 - learning_rate: 4.0000e-05\n",
      "Epoch 28/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9134 - val_loss: 1.3645 - learning_rate: 4.0000e-05\n",
      "Epoch 29/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9077 - val_loss: 1.3305 - learning_rate: 4.0000e-05\n",
      "Epoch 30/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9097 - val_loss: 1.3656 - learning_rate: 4.0000e-05\n",
      "Epoch 31/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9132 - val_loss: 1.3852 - learning_rate: 4.0000e-05\n",
      "Epoch 32/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9085 - val_loss: 1.3596 - learning_rate: 4.0000e-05\n",
      "Epoch 33/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9093 - val_loss: 1.3809 - learning_rate: 4.0000e-05\n",
      "Epoch 34/200\n",
      "14/14 - 1s - 64ms/step - loss: 0.9036 - val_loss: 1.3978 - learning_rate: 4.0000e-05\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 - 1s - 66ms/step - loss: 0.9057 - val_loss: 1.3477 - learning_rate: 4.0000e-05\n",
      "Epoch 36/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9036 - val_loss: 1.3557 - learning_rate: 1.0000e-05\n",
      "Epoch 37/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9023 - val_loss: 1.3573 - learning_rate: 1.0000e-05\n",
      "Epoch 38/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9044 - val_loss: 1.3577 - learning_rate: 1.0000e-05\n",
      "Epoch 39/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9029 - val_loss: 1.3657 - learning_rate: 1.0000e-05\n",
      "Epoch 40/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9007 - val_loss: 1.3733 - learning_rate: 1.0000e-05\n",
      "Epoch 41/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9028 - val_loss: 1.3764 - learning_rate: 1.0000e-05\n",
      "Epoch 42/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9003 - val_loss: 1.3696 - learning_rate: 1.0000e-05\n",
      "Epoch 43/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9046 - val_loss: 1.3631 - learning_rate: 1.0000e-05\n",
      "Epoch 44/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9039 - val_loss: 1.3623 - learning_rate: 1.0000e-05\n",
      "Epoch 45/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9022 - val_loss: 1.3652 - learning_rate: 1.0000e-05\n",
      "Epoch 46/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9023 - val_loss: 1.3661 - learning_rate: 1.0000e-05\n",
      "Epoch 47/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.9031 - val_loss: 1.3696 - learning_rate: 1.0000e-05\n",
      "Epoch 48/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9044 - val_loss: 1.3660 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9025 - val_loss: 1.3534 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9003 - val_loss: 1.3520 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9018 - val_loss: 1.3560 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8977 - val_loss: 1.3644 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8987 - val_loss: 1.3599 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8999 - val_loss: 1.3621 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8958 - val_loss: 1.3547 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.9004 - val_loss: 1.3549 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8983 - val_loss: 1.3589 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8966 - val_loss: 1.3544 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9011 - val_loss: 1.3561 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8996 - val_loss: 1.3549 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8987 - val_loss: 1.3557 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8969 - val_loss: 1.3580 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8980 - val_loss: 1.3656 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8981 - val_loss: 1.3577 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8973 - val_loss: 1.3579 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8957 - val_loss: 1.3636 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8977 - val_loss: 1.3654 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8969 - val_loss: 1.3588 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8921 - val_loss: 1.3668 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8934 - val_loss: 1.3576 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.9008 - val_loss: 1.3619 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8958 - val_loss: 1.3722 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8965 - val_loss: 1.3726 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8981 - val_loss: 1.3596 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8978 - val_loss: 1.3674 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8949 - val_loss: 1.3561 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8929 - val_loss: 1.3542 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8909 - val_loss: 1.3546 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8934 - val_loss: 1.3615 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8909 - val_loss: 1.3588 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8892 - val_loss: 1.3604 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8956 - val_loss: 1.3571 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8911 - val_loss: 1.3594 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8908 - val_loss: 1.3618 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8881 - val_loss: 1.3464 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8943 - val_loss: 1.3528 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8906 - val_loss: 1.3610 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8913 - val_loss: 1.3675 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8883 - val_loss: 1.3584 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8927 - val_loss: 1.3553 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8899 - val_loss: 1.3617 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8855 - val_loss: 1.3608 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8877 - val_loss: 1.3576 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8895 - val_loss: 1.3673 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8883 - val_loss: 1.3537 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8879 - val_loss: 1.3598 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8878 - val_loss: 1.3665 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8888 - val_loss: 1.3536 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8862 - val_loss: 1.3511 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8850 - val_loss: 1.3453 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8876 - val_loss: 1.3322 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8862 - val_loss: 1.3344 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8850 - val_loss: 1.3412 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8848 - val_loss: 1.3494 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8831 - val_loss: 1.3550 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8873 - val_loss: 1.3604 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8820 - val_loss: 1.3642 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8843 - val_loss: 1.3522 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8804 - val_loss: 1.3504 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8805 - val_loss: 1.3433 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8811 - val_loss: 1.3441 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8812 - val_loss: 1.3550 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8832 - val_loss: 1.3594 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8825 - val_loss: 1.3574 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8812 - val_loss: 1.3386 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8811 - val_loss: 1.3493 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8814 - val_loss: 1.3654 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8787 - val_loss: 1.3449 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8812 - val_loss: 1.3472 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8790 - val_loss: 1.3505 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8806 - val_loss: 1.3509 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8793 - val_loss: 1.3408 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8762 - val_loss: 1.3435 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8745 - val_loss: 1.3598 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8777 - val_loss: 1.3627 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8801 - val_loss: 1.3580 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8789 - val_loss: 1.3511 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8781 - val_loss: 1.3492 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8748 - val_loss: 1.3386 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8729 - val_loss: 1.3432 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8738 - val_loss: 1.3505 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8782 - val_loss: 1.3459 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8750 - val_loss: 1.3379 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8725 - val_loss: 1.3373 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8721 - val_loss: 1.3409 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8761 - val_loss: 1.3231 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8740 - val_loss: 1.3402 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8726 - val_loss: 1.3516 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8718 - val_loss: 1.3579 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8730 - val_loss: 1.3206 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8711 - val_loss: 1.3284 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8746 - val_loss: 1.3362 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8670 - val_loss: 1.3503 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8696 - val_loss: 1.3280 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8666 - val_loss: 1.3388 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8696 - val_loss: 1.3648 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8700 - val_loss: 1.3624 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8663 - val_loss: 1.3606 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8667 - val_loss: 1.3221 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8669 - val_loss: 1.3261 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8662 - val_loss: 1.3301 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8670 - val_loss: 1.3210 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8660 - val_loss: 1.3077 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8687 - val_loss: 1.3139 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8644 - val_loss: 1.3323 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8664 - val_loss: 1.3322 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8650 - val_loss: 1.3256 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8628 - val_loss: 1.3281 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8626 - val_loss: 1.3559 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8629 - val_loss: 1.3236 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8635 - val_loss: 1.3340 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8586 - val_loss: 1.3186 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8614 - val_loss: 1.3154 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8607 - val_loss: 1.3344 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8629 - val_loss: 1.3180 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8565 - val_loss: 1.3666 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8580 - val_loss: 1.3205 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8585 - val_loss: 1.3517 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8583 - val_loss: 1.3168 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8550 - val_loss: 1.3373 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8569 - val_loss: 1.3321 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8592 - val_loss: 1.3200 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8579 - val_loss: 1.3185 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8546 - val_loss: 1.3585 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8544 - val_loss: 1.3078 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8504 - val_loss: 1.3085 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8478 - val_loss: 1.3199 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8488 - val_loss: 1.3495 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8495 - val_loss: 1.3026 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8540 - val_loss: 1.3630 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8517 - val_loss: 1.2852 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8448 - val_loss: 1.3043 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8489 - val_loss: 1.3109 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8495 - val_loss: 1.2552 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8440 - val_loss: 1.3304 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8422 - val_loss: 1.2963 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8433 - val_loss: 1.3117 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8419 - val_loss: 1.2357 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8350 - val_loss: 1.3439 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8336 - val_loss: 1.3267 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8324 - val_loss: 1.2083 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8295 - val_loss: 1.2870 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8258 - val_loss: 1.3376 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8192 - val_loss: 1.2447 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.8199 - val_loss: 1.1451 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8148 - val_loss: 1.3941 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8078 - val_loss: 1.2123 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8058 - val_loss: 1.0792 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8047 - val_loss: 1.1765 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8064 - val_loss: 1.3058 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 11\n",
      "Epoch 1/200\n",
      "9/9 - 6s - 667ms/step - loss: 2.6947 - val_loss: 1.7840 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "9/9 - 1s - 69ms/step - loss: 2.0223 - val_loss: 2.0216 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "9/9 - 1s - 70ms/step - loss: 1.7995 - val_loss: 1.7884 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.6646 - val_loss: 1.8082 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.5474 - val_loss: 1.6726 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "9/9 - 1s - 67ms/step - loss: 1.4728 - val_loss: 1.4427 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.4141 - val_loss: 1.6536 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.3654 - val_loss: 1.3416 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.3273 - val_loss: 1.3878 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.3041 - val_loss: 1.7097 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.2792 - val_loss: 1.3449 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "9/9 - 1s - 70ms/step - loss: 1.2402 - val_loss: 1.2531 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.2280 - val_loss: 1.4444 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.1982 - val_loss: 1.3241 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.1841 - val_loss: 1.3801 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "9/9 - 1s - 67ms/step - loss: 1.1698 - val_loss: 1.4829 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "9/9 - 1s - 67ms/step - loss: 1.1516 - val_loss: 1.1929 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.1334 - val_loss: 1.1570 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "9/9 - 1s - 70ms/step - loss: 1.1285 - val_loss: 1.3482 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "9/9 - 1s - 67ms/step - loss: 1.1064 - val_loss: 1.4415 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "9/9 - 1s - 67ms/step - loss: 1.0928 - val_loss: 1.2971 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.0783 - val_loss: 1.3077 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.0818 - val_loss: 1.2030 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.0670 - val_loss: 0.9869 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.0506 - val_loss: 1.3230 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.0317 - val_loss: 1.2981 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.0270 - val_loss: 0.9879 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "9/9 - 1s - 68ms/step - loss: 1.0263 - val_loss: 1.3118 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "9/9 - 1s - 69ms/step - loss: 1.0035 - val_loss: 1.3741 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9939 - val_loss: 1.0865 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9895 - val_loss: 1.2632 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9795 - val_loss: 1.2807 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9671 - val_loss: 1.0588 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "9/9 - 1s - 69ms/step - loss: 0.9616 - val_loss: 1.2736 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9559 - val_loss: 1.1422 - learning_rate: 2.0000e-04\n",
      "Epoch 36/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9562 - val_loss: 1.1439 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9552 - val_loss: 1.2403 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9550 - val_loss: 1.2017 - learning_rate: 2.0000e-04\n",
      "Epoch 39/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9504 - val_loss: 1.1389 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9540 - val_loss: 1.1603 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9416 - val_loss: 1.2536 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9443 - val_loss: 1.1407 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9403 - val_loss: 1.2202 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "9/9 - 1s - 69ms/step - loss: 0.9384 - val_loss: 1.1891 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9407 - val_loss: 1.1730 - learning_rate: 4.0000e-05\n",
      "Epoch 46/200\n",
      "9/9 - 1s - 66ms/step - loss: 0.9436 - val_loss: 1.1705 - learning_rate: 4.0000e-05\n",
      "Epoch 47/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9398 - val_loss: 1.1894 - learning_rate: 4.0000e-05\n",
      "Epoch 48/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9418 - val_loss: 1.1948 - learning_rate: 4.0000e-05\n",
      "Epoch 49/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9444 - val_loss: 1.1795 - learning_rate: 4.0000e-05\n",
      "Epoch 50/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9450 - val_loss: 1.1783 - learning_rate: 4.0000e-05\n",
      "Epoch 51/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9378 - val_loss: 1.1945 - learning_rate: 4.0000e-05\n",
      "Epoch 52/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9352 - val_loss: 1.1856 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9403 - val_loss: 1.1880 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "9/9 - 1s - 67ms/step - loss: 0.9367 - val_loss: 1.1611 - learning_rate: 4.0000e-05\n",
      "Epoch 55/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9360 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9369 - val_loss: 1.1696 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9394 - val_loss: 1.1724 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9357 - val_loss: 1.1746 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9321 - val_loss: 1.1752 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9366 - val_loss: 1.1760 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9358 - val_loss: 1.1781 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9352 - val_loss: 1.1777 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "9/9 - 1s - 66ms/step - loss: 0.9343 - val_loss: 1.1785 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9366 - val_loss: 1.1764 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9339 - val_loss: 1.1781 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9349 - val_loss: 1.1772 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9322 - val_loss: 1.1781 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9346 - val_loss: 1.1784 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "9/9 - 1s - 66ms/step - loss: 0.9353 - val_loss: 1.1763 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9346 - val_loss: 1.1748 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9337 - val_loss: 1.1745 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9372 - val_loss: 1.1765 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9321 - val_loss: 1.1778 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9311 - val_loss: 1.1765 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9342 - val_loss: 1.1795 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9350 - val_loss: 1.1799 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9359 - val_loss: 1.1794 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9344 - val_loss: 1.1812 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9320 - val_loss: 1.1798 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9339 - val_loss: 1.1801 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9345 - val_loss: 1.1817 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9304 - val_loss: 1.1808 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9348 - val_loss: 1.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9361 - val_loss: 1.1802 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9354 - val_loss: 1.1778 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9338 - val_loss: 1.1767 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9311 - val_loss: 1.1763 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9327 - val_loss: 1.1779 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9339 - val_loss: 1.1784 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9320 - val_loss: 1.1740 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9303 - val_loss: 1.1754 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9322 - val_loss: 1.1787 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9301 - val_loss: 1.1794 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9344 - val_loss: 1.1779 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9308 - val_loss: 1.1746 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9298 - val_loss: 1.1747 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9304 - val_loss: 1.1777 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "9/9 - 1s - 73ms/step - loss: 0.9293 - val_loss: 1.1806 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "9/9 - 1s - 74ms/step - loss: 0.9308 - val_loss: 1.1794 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9307 - val_loss: 1.1764 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9249 - val_loss: 1.1767 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9307 - val_loss: 1.1740 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9278 - val_loss: 1.1787 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9309 - val_loss: 1.1785 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9267 - val_loss: 1.1755 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9337 - val_loss: 1.1749 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9280 - val_loss: 1.1774 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9311 - val_loss: 1.1785 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9325 - val_loss: 1.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9279 - val_loss: 1.1798 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9296 - val_loss: 1.1813 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9291 - val_loss: 1.1785 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9264 - val_loss: 1.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9297 - val_loss: 1.1790 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9282 - val_loss: 1.1799 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "9/9 - 1s - 66ms/step - loss: 0.9338 - val_loss: 1.1770 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9325 - val_loss: 1.1752 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "9/9 - 1s - 66ms/step - loss: 0.9272 - val_loss: 1.1718 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9274 - val_loss: 1.1720 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "9/9 - 1s - 75ms/step - loss: 0.9276 - val_loss: 1.1710 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "9/9 - 1s - 72ms/step - loss: 0.9282 - val_loss: 1.1723 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "9/9 - 1s - 73ms/step - loss: 0.9296 - val_loss: 1.1703 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "9/9 - 1s - 73ms/step - loss: 0.9296 - val_loss: 1.1726 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9301 - val_loss: 1.1717 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9273 - val_loss: 1.1744 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "9/9 - 1s - 76ms/step - loss: 0.9281 - val_loss: 1.1777 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9279 - val_loss: 1.1764 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9272 - val_loss: 1.1745 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9279 - val_loss: 1.1751 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "9/9 - 1s - 72ms/step - loss: 0.9278 - val_loss: 1.1748 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9247 - val_loss: 1.1757 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9292 - val_loss: 1.1790 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9271 - val_loss: 1.1734 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9284 - val_loss: 1.1728 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9257 - val_loss: 1.1756 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9239 - val_loss: 1.1734 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9254 - val_loss: 1.1750 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9271 - val_loss: 1.1765 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9243 - val_loss: 1.1749 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9247 - val_loss: 1.1763 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9284 - val_loss: 1.1731 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9236 - val_loss: 1.1773 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9268 - val_loss: 1.1769 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9278 - val_loss: 1.1706 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9231 - val_loss: 1.1733 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9228 - val_loss: 1.1779 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9238 - val_loss: 1.1795 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9212 - val_loss: 1.1703 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9238 - val_loss: 1.1715 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9243 - val_loss: 1.1707 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9212 - val_loss: 1.1647 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9238 - val_loss: 1.1692 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9240 - val_loss: 1.1685 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9253 - val_loss: 1.1716 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9238 - val_loss: 1.1726 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9213 - val_loss: 1.1757 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9243 - val_loss: 1.1775 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9221 - val_loss: 1.1767 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9183 - val_loss: 1.1739 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9212 - val_loss: 1.1728 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9214 - val_loss: 1.1787 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9212 - val_loss: 1.1756 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9207 - val_loss: 1.1739 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9200 - val_loss: 1.1735 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9247 - val_loss: 1.1785 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9166 - val_loss: 1.1717 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9210 - val_loss: 1.1752 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9225 - val_loss: 1.1708 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9207 - val_loss: 1.1666 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9247 - val_loss: 1.1693 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9202 - val_loss: 1.1734 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9170 - val_loss: 1.1781 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9203 - val_loss: 1.1726 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9156 - val_loss: 1.1687 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9196 - val_loss: 1.1708 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9184 - val_loss: 1.1648 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9191 - val_loss: 1.1725 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9174 - val_loss: 1.1692 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9215 - val_loss: 1.1686 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9183 - val_loss: 1.1672 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9212 - val_loss: 1.1681 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9172 - val_loss: 1.1690 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "9/9 - 1s - 71ms/step - loss: 0.9192 - val_loss: 1.1638 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9163 - val_loss: 1.1636 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9169 - val_loss: 1.1633 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9168 - val_loss: 1.1663 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9164 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9174 - val_loss: 1.1644 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9158 - val_loss: 1.1642 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9175 - val_loss: 1.1673 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9170 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9205 - val_loss: 1.1633 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9142 - val_loss: 1.1650 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "9/9 - 1s - 70ms/step - loss: 0.9182 - val_loss: 1.1690 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "9/9 - 1s - 68ms/step - loss: 0.9178 - val_loss: 1.1662 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9162 - val_loss: 1.1660 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "9/9 - 1s - 67ms/step - loss: 0.9165 - val_loss: 1.1647 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9186 - val_loss: 1.1653 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "9/9 - 1s - 73ms/step - loss: 0.9174 - val_loss: 1.1683 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "9/9 - 1s - 69ms/step - loss: 0.9125 - val_loss: 1.1725 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 12\n",
      "Epoch 1/200\n",
      "30/30 - 8s - 278ms/step - loss: 2.1140 - val_loss: 1.8088 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "30/30 - 2s - 67ms/step - loss: 1.5393 - val_loss: 1.5179 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "30/30 - 2s - 69ms/step - loss: 1.3294 - val_loss: 1.3252 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "30/30 - 2s - 68ms/step - loss: 1.2128 - val_loss: 1.4491 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "30/30 - 2s - 68ms/step - loss: 1.1273 - val_loss: 1.3359 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "30/30 - 2s - 69ms/step - loss: 1.0622 - val_loss: 1.2077 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "30/30 - 2s - 69ms/step - loss: 1.0186 - val_loss: 1.0963 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.9711 - val_loss: 1.2693 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.9307 - val_loss: 1.2375 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.9044 - val_loss: 1.3688 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.8884 - val_loss: 1.0624 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "30/30 - 2s - 66ms/step - loss: 0.8564 - val_loss: 1.0129 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.8378 - val_loss: 1.1079 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.8222 - val_loss: 1.0645 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.8046 - val_loss: 1.0410 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.7874 - val_loss: 1.0895 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.7805 - val_loss: 1.3222 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.7786 - val_loss: 1.1686 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.7590 - val_loss: 0.9264 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.7501 - val_loss: 1.1484 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.7433 - val_loss: 1.2744 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.7387 - val_loss: 1.0446 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.7326 - val_loss: 0.9288 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.7258 - val_loss: 1.1214 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.7165 - val_loss: 1.0266 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.7138 - val_loss: 1.0812 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.7081 - val_loss: 0.9446 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.7054 - val_loss: 0.7425 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "30/30 - 2s - 66ms/step - loss: 0.7095 - val_loss: 0.9464 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6971 - val_loss: 1.1370 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6943 - val_loss: 0.9315 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6933 - val_loss: 0.8361 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6877 - val_loss: 0.9556 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6858 - val_loss: 1.0672 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6855 - val_loss: 0.9603 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "30/30 - 2s - 66ms/step - loss: 0.6784 - val_loss: 1.0286 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "30/30 - 2s - 66ms/step - loss: 0.6761 - val_loss: 0.9522 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "30/30 - 2s - 68ms/step - loss: 0.6746 - val_loss: 1.1407 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6727 - val_loss: 0.9401 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6695 - val_loss: 0.9911 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6693 - val_loss: 0.8690 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6685 - val_loss: 0.9427 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6670 - val_loss: 1.0015 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6672 - val_loss: 0.9533 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.6654 - val_loss: 0.9218 - learning_rate: 2.0000e-04\n",
      "Epoch 46/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6669 - val_loss: 0.9983 - learning_rate: 2.0000e-04\n",
      "Epoch 47/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6663 - val_loss: 1.0311 - learning_rate: 2.0000e-04\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "30/30 - 2s - 67ms/step - loss: 0.6644 - val_loss: 0.9156 - learning_rate: 2.0000e-04\n",
      "Epoch 49/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6642 - val_loss: 0.9619 - learning_rate: 4.0000e-05\n",
      "Epoch 50/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6656 - val_loss: 0.9945 - learning_rate: 4.0000e-05\n",
      "Epoch 51/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6635 - val_loss: 0.9615 - learning_rate: 4.0000e-05\n",
      "Epoch 52/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6637 - val_loss: 0.9683 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6640 - val_loss: 0.9766 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6641 - val_loss: 0.9769 - learning_rate: 4.0000e-05\n",
      "Epoch 55/200\n",
      "30/30 - 2s - 71ms/step - loss: 0.6623 - val_loss: 0.9885 - learning_rate: 4.0000e-05\n",
      "Epoch 56/200\n",
      "30/30 - 2s - 71ms/step - loss: 0.6626 - val_loss: 0.9614 - learning_rate: 4.0000e-05\n",
      "Epoch 57/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6631 - val_loss: 0.9749 - learning_rate: 4.0000e-05\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "30/30 - 2s - 68ms/step - loss: 0.6632 - val_loss: 0.9726 - learning_rate: 4.0000e-05\n",
      "Epoch 59/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6618 - val_loss: 0.9726 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6616 - val_loss: 0.9772 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.6621 - val_loss: 0.9749 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.6632 - val_loss: 0.9732 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6618 - val_loss: 0.9681 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6630 - val_loss: 0.9759 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6626 - val_loss: 0.9776 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6610 - val_loss: 0.9771 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6619 - val_loss: 0.9762 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6614 - val_loss: 0.9799 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6624 - val_loss: 0.9798 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6618 - val_loss: 0.9832 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6635 - val_loss: 0.9715 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6623 - val_loss: 0.9744 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6622 - val_loss: 0.9798 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6627 - val_loss: 0.9801 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6615 - val_loss: 0.9761 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6606 - val_loss: 0.9757 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6612 - val_loss: 0.9729 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6604 - val_loss: 0.9829 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6635 - val_loss: 0.9763 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6607 - val_loss: 0.9769 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6595 - val_loss: 0.9832 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6632 - val_loss: 0.9725 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6615 - val_loss: 0.9764 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6610 - val_loss: 0.9696 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6608 - val_loss: 0.9784 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.6605 - val_loss: 0.9752 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6615 - val_loss: 0.9827 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6622 - val_loss: 0.9733 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6603 - val_loss: 0.9840 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6594 - val_loss: 0.9794 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.6611 - val_loss: 0.9727 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.6598 - val_loss: 0.9778 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "30/30 - 2s - 75ms/step - loss: 0.6607 - val_loss: 0.9829 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "30/30 - 2s - 75ms/step - loss: 0.6590 - val_loss: 0.9871 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6595 - val_loss: 0.9804 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "30/30 - 2s - 76ms/step - loss: 0.6594 - val_loss: 0.9782 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6586 - val_loss: 0.9663 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "30/30 - 2s - 77ms/step - loss: 0.6582 - val_loss: 0.9824 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "30/30 - 2s - 75ms/step - loss: 0.6581 - val_loss: 0.9860 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6565 - val_loss: 1.0151 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "30/30 - 2s - 76ms/step - loss: 0.6577 - val_loss: 1.0043 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6575 - val_loss: 0.9391 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "30/30 - 2s - 76ms/step - loss: 0.6560 - val_loss: 1.0121 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6554 - val_loss: 1.0569 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "30/30 - 2s - 75ms/step - loss: 0.6548 - val_loss: 1.0499 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "30/30 - 2s - 74ms/step - loss: 0.6499 - val_loss: 0.9187 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "30/30 - 2s - 76ms/step - loss: 0.6510 - val_loss: 0.9566 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "30/30 - 2s - 71ms/step - loss: 0.6484 - val_loss: 0.9147 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6399 - val_loss: 1.0233 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6312 - val_loss: 0.8992 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6319 - val_loss: 0.8847 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6203 - val_loss: 0.9173 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.6067 - val_loss: 1.1079 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.6028 - val_loss: 0.9673 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5890 - val_loss: 0.9989 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5871 - val_loss: 0.9828 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5834 - val_loss: 1.1474 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.5808 - val_loss: 0.8361 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.5762 - val_loss: 0.7696 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5793 - val_loss: 0.8403 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.5627 - val_loss: 0.8749 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5615 - val_loss: 0.9121 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.5531 - val_loss: 0.8464 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.5507 - val_loss: 0.8069 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "30/30 - 2s - 71ms/step - loss: 0.5484 - val_loss: 0.9333 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5423 - val_loss: 1.0268 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5369 - val_loss: 1.0325 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5317 - val_loss: 1.0887 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5272 - val_loss: 0.7863 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5206 - val_loss: 1.0694 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.5168 - val_loss: 0.8609 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5161 - val_loss: 0.9650 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.5140 - val_loss: 0.7575 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.5048 - val_loss: 1.0839 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4934 - val_loss: 0.7724 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4881 - val_loss: 1.0784 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4928 - val_loss: 1.0925 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4856 - val_loss: 0.8677 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4874 - val_loss: 0.9723 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4771 - val_loss: 0.8338 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4742 - val_loss: 0.7985 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4817 - val_loss: 0.7469 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4758 - val_loss: 1.0498 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4724 - val_loss: 0.8784 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4720 - val_loss: 1.3383 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4680 - val_loss: 0.8096 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4688 - val_loss: 0.9973 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4614 - val_loss: 0.9036 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4601 - val_loss: 0.6657 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4700 - val_loss: 0.9296 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4559 - val_loss: 1.0516 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4565 - val_loss: 1.2368 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4560 - val_loss: 0.7863 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4497 - val_loss: 1.2728 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4583 - val_loss: 0.9237 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.4519 - val_loss: 0.5091 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4552 - val_loss: 0.9532 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.4350 - val_loss: 0.7297 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4403 - val_loss: 0.7267 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4367 - val_loss: 1.3104 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4365 - val_loss: 0.9445 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4348 - val_loss: 0.9083 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4320 - val_loss: 0.7797 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4268 - val_loss: 1.1177 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4369 - val_loss: 0.9662 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4274 - val_loss: 0.5752 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.4220 - val_loss: 0.9550 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4112 - val_loss: 0.7180 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.4040 - val_loss: 0.8597 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.3968 - val_loss: 0.6821 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3988 - val_loss: 0.9514 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3909 - val_loss: 0.8221 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "30/30 - 2s - 70ms/step - loss: 0.3880 - val_loss: 0.5403 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.3788 - val_loss: 0.7865 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "30/30 - 2s - 71ms/step - loss: 0.3751 - val_loss: 0.9196 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.3622 - val_loss: 0.8980 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.3505 - val_loss: 0.7422 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3591 - val_loss: 1.1202 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.3363 - val_loss: 0.8387 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3267 - val_loss: 0.5245 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.3199 - val_loss: 0.7294 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3236 - val_loss: 0.4376 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.3033 - val_loss: 0.4859 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3103 - val_loss: 0.9935 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.3229 - val_loss: 0.8245 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.3137 - val_loss: 0.6726 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.2667 - val_loss: 0.4044 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2654 - val_loss: 0.5968 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.2556 - val_loss: 0.3460 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2348 - val_loss: 0.2041 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.2671 - val_loss: 0.2572 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2412 - val_loss: 0.6803 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.2521 - val_loss: 0.3790 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.2198 - val_loss: 0.3770 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.2093 - val_loss: 0.2397 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2021 - val_loss: 0.3570 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "30/30 - 2s - 69ms/step - loss: 0.1994 - val_loss: 0.2504 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2094 - val_loss: 0.3681 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "30/30 - 2s - 68ms/step - loss: 0.1904 - val_loss: 0.2594 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "30/30 - 2s - 67ms/step - loss: 0.2017 - val_loss: 0.4308 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 13\n",
      "Epoch 1/200\n",
      "10/10 - 7s - 712ms/step - loss: 2.4510 - val_loss: 1.8000 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "10/10 - 1s - 77ms/step - loss: 2.0046 - val_loss: 1.8053 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.7709 - val_loss: 1.6321 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "10/10 - 1s - 80ms/step - loss: 1.6134 - val_loss: 1.6613 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.5208 - val_loss: 1.4990 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.4347 - val_loss: 1.4192 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.3656 - val_loss: 1.2808 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.3345 - val_loss: 1.2783 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.2836 - val_loss: 1.1680 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.2650 - val_loss: 1.3361 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.2246 - val_loss: 1.5136 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.2100 - val_loss: 1.4889 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.1935 - val_loss: 1.4815 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.1681 - val_loss: 1.2906 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.1441 - val_loss: 1.1941 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.1169 - val_loss: 1.1900 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0999 - val_loss: 1.2039 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0889 - val_loss: 1.2862 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "10/10 - 1s - 76ms/step - loss: 1.0770 - val_loss: 1.5220 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "10/10 - 1s - 79ms/step - loss: 1.0812 - val_loss: 1.1042 - learning_rate: 2.0000e-04\n",
      "Epoch 21/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0658 - val_loss: 1.3083 - learning_rate: 2.0000e-04\n",
      "Epoch 22/200\n",
      "10/10 - 1s - 79ms/step - loss: 1.0569 - val_loss: 1.2283 - learning_rate: 2.0000e-04\n",
      "Epoch 23/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0649 - val_loss: 1.1843 - learning_rate: 2.0000e-04\n",
      "Epoch 24/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0493 - val_loss: 1.2380 - learning_rate: 2.0000e-04\n",
      "Epoch 25/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0476 - val_loss: 1.2416 - learning_rate: 2.0000e-04\n",
      "Epoch 26/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0399 - val_loss: 1.1966 - learning_rate: 2.0000e-04\n",
      "Epoch 27/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0441 - val_loss: 1.1928 - learning_rate: 2.0000e-04\n",
      "Epoch 28/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0429 - val_loss: 1.1975 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0368 - val_loss: 1.1792 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "10/10 - 1s - 78ms/step - loss: 1.0402 - val_loss: 1.1915 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0338 - val_loss: 1.2074 - learning_rate: 4.0000e-05\n",
      "Epoch 32/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0322 - val_loss: 1.2125 - learning_rate: 4.0000e-05\n",
      "Epoch 33/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0298 - val_loss: 1.2168 - learning_rate: 4.0000e-05\n",
      "Epoch 34/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0322 - val_loss: 1.2018 - learning_rate: 4.0000e-05\n",
      "Epoch 35/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0321 - val_loss: 1.1676 - learning_rate: 4.0000e-05\n",
      "Epoch 36/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0321 - val_loss: 1.1900 - learning_rate: 4.0000e-05\n",
      "Epoch 37/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0304 - val_loss: 1.2143 - learning_rate: 4.0000e-05\n",
      "Epoch 38/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0295 - val_loss: 1.1978 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0318 - val_loss: 1.2089 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "10/10 - 1s - 76ms/step - loss: 1.0226 - val_loss: 1.1940 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0365 - val_loss: 1.1961 - learning_rate: 1.0000e-05\n",
      "Epoch 42/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0248 - val_loss: 1.1931 - learning_rate: 1.0000e-05\n",
      "Epoch 43/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0315 - val_loss: 1.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 44/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0270 - val_loss: 1.1963 - learning_rate: 1.0000e-05\n",
      "Epoch 45/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0272 - val_loss: 1.1977 - learning_rate: 1.0000e-05\n",
      "Epoch 46/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0262 - val_loss: 1.1922 - learning_rate: 1.0000e-05\n",
      "Epoch 47/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0298 - val_loss: 1.1935 - learning_rate: 1.0000e-05\n",
      "Epoch 48/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0209 - val_loss: 1.1928 - learning_rate: 1.0000e-05\n",
      "Epoch 49/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0263 - val_loss: 1.1945 - learning_rate: 1.0000e-05\n",
      "Epoch 50/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0266 - val_loss: 1.1961 - learning_rate: 1.0000e-05\n",
      "Epoch 51/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0295 - val_loss: 1.1972 - learning_rate: 1.0000e-05\n",
      "Epoch 52/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0309 - val_loss: 1.1950 - learning_rate: 1.0000e-05\n",
      "Epoch 53/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0256 - val_loss: 1.1954 - learning_rate: 1.0000e-05\n",
      "Epoch 54/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0251 - val_loss: 1.1948 - learning_rate: 1.0000e-05\n",
      "Epoch 55/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0282 - val_loss: 1.2000 - learning_rate: 1.0000e-05\n",
      "Epoch 56/200\n",
      "10/10 - 1s - 80ms/step - loss: 1.0149 - val_loss: 1.1956 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0308 - val_loss: 1.1977 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0205 - val_loss: 1.1938 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0257 - val_loss: 1.1991 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0231 - val_loss: 1.2004 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0203 - val_loss: 1.1985 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0256 - val_loss: 1.1922 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0213 - val_loss: 1.1913 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0305 - val_loss: 1.1893 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0179 - val_loss: 1.1944 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0229 - val_loss: 1.1971 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0200 - val_loss: 1.1943 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0276 - val_loss: 1.1866 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0215 - val_loss: 1.1957 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0228 - val_loss: 1.1993 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0227 - val_loss: 1.1968 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0191 - val_loss: 1.1934 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0158 - val_loss: 1.1902 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0170 - val_loss: 1.1905 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0211 - val_loss: 1.1903 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0184 - val_loss: 1.1895 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0211 - val_loss: 1.1893 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0211 - val_loss: 1.1873 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0182 - val_loss: 1.1875 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0188 - val_loss: 1.1922 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0185 - val_loss: 1.1892 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0188 - val_loss: 1.1910 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0184 - val_loss: 1.1907 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0185 - val_loss: 1.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0146 - val_loss: 1.1927 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0204 - val_loss: 1.1891 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0160 - val_loss: 1.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0161 - val_loss: 1.1943 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0198 - val_loss: 1.1891 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0142 - val_loss: 1.1938 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0218 - val_loss: 1.1905 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0169 - val_loss: 1.1882 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0136 - val_loss: 1.1891 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0157 - val_loss: 1.1927 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0117 - val_loss: 1.1898 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0186 - val_loss: 1.1850 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0152 - val_loss: 1.1860 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0150 - val_loss: 1.1854 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0112 - val_loss: 1.1801 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0147 - val_loss: 1.1833 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0151 - val_loss: 1.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0141 - val_loss: 1.1791 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0087 - val_loss: 1.1782 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0137 - val_loss: 1.1863 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0171 - val_loss: 1.1959 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0095 - val_loss: 1.1962 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0133 - val_loss: 1.1955 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0120 - val_loss: 1.1866 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0139 - val_loss: 1.1797 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0139 - val_loss: 1.1766 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0113 - val_loss: 1.1783 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0116 - val_loss: 1.1878 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0105 - val_loss: 1.1877 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0075 - val_loss: 1.1837 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0109 - val_loss: 1.1884 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0129 - val_loss: 1.1854 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0126 - val_loss: 1.1818 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0049 - val_loss: 1.1838 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "10/10 - 1s - 75ms/step - loss: 1.0091 - val_loss: 1.1804 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0105 - val_loss: 1.1824 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0019 - val_loss: 1.1746 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0123 - val_loss: 1.1798 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "10/10 - 1s - 82ms/step - loss: 1.0118 - val_loss: 1.1819 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0103 - val_loss: 1.1861 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0054 - val_loss: 1.1855 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0095 - val_loss: 1.1822 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0080 - val_loss: 1.1807 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0079 - val_loss: 1.1779 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0044 - val_loss: 1.1768 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0020 - val_loss: 1.1724 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0081 - val_loss: 1.1791 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0126 - val_loss: 1.1912 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "10/10 - 1s - 81ms/step - loss: 1.0108 - val_loss: 1.1867 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "10/10 - 1s - 81ms/step - loss: 1.0063 - val_loss: 1.1848 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "10/10 - 1s - 80ms/step - loss: 1.0074 - val_loss: 1.1808 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0040 - val_loss: 1.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0074 - val_loss: 1.1893 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0019 - val_loss: 1.1769 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0065 - val_loss: 1.1851 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0059 - val_loss: 1.1755 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0012 - val_loss: 1.1778 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0037 - val_loss: 1.1819 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9999 - val_loss: 1.1729 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "10/10 - 1s - 76ms/step - loss: 1.0082 - val_loss: 1.1769 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9979 - val_loss: 1.1753 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0010 - val_loss: 1.1667 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0052 - val_loss: 1.1716 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0041 - val_loss: 1.1713 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0015 - val_loss: 1.1732 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0046 - val_loss: 1.1728 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0005 - val_loss: 1.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9998 - val_loss: 1.1804 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0030 - val_loss: 1.1798 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9996 - val_loss: 1.1678 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9993 - val_loss: 1.1715 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9956 - val_loss: 1.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9967 - val_loss: 1.1754 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0050 - val_loss: 1.1738 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9946 - val_loss: 1.1697 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0001 - val_loss: 1.1681 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9970 - val_loss: 1.1772 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9967 - val_loss: 1.1796 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "10/10 - 1s - 77ms/step - loss: 1.0020 - val_loss: 1.1735 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9991 - val_loss: 1.1831 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9956 - val_loss: 1.1876 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "10/10 - 1s - 79ms/step - loss: 0.9947 - val_loss: 1.1800 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9990 - val_loss: 1.1816 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "10/10 - 1s - 78ms/step - loss: 1.0022 - val_loss: 1.1726 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9937 - val_loss: 1.1801 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9989 - val_loss: 1.1856 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9941 - val_loss: 1.1734 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9950 - val_loss: 1.1764 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9972 - val_loss: 1.1845 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "10/10 - 1s - 79ms/step - loss: 0.9977 - val_loss: 1.1884 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9917 - val_loss: 1.1695 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9932 - val_loss: 1.1633 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9948 - val_loss: 1.1638 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9975 - val_loss: 1.1811 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9938 - val_loss: 1.1948 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9931 - val_loss: 1.1708 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9939 - val_loss: 1.1744 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9915 - val_loss: 1.1636 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9903 - val_loss: 1.1719 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9900 - val_loss: 1.1723 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9894 - val_loss: 1.1787 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9900 - val_loss: 1.1786 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9892 - val_loss: 1.1682 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9921 - val_loss: 1.1750 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9907 - val_loss: 1.1684 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9879 - val_loss: 1.1676 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9872 - val_loss: 1.1575 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9915 - val_loss: 1.1729 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9903 - val_loss: 1.1710 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9867 - val_loss: 1.1751 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9906 - val_loss: 1.1492 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "10/10 - 1s - 76ms/step - loss: 0.9843 - val_loss: 1.1663 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "10/10 - 1s - 78ms/step - loss: 0.9864 - val_loss: 1.1683 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9881 - val_loss: 1.1660 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "10/10 - 1s - 77ms/step - loss: 0.9872 - val_loss: 1.1709 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "10/10 - 1s - 82ms/step - loss: 0.9877 - val_loss: 1.1688 - learning_rate: 1.0000e-05\n",
      "Entrenando cluster numero: 14\n",
      "Epoch 1/200\n",
      "14/14 - 7s - 514ms/step - loss: 2.4808 - val_loss: 1.9738 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.8678 - val_loss: 1.8303 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.6362 - val_loss: 1.8411 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.5012 - val_loss: 1.4649 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.4069 - val_loss: 1.5917 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.3537 - val_loss: 1.1854 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.3054 - val_loss: 1.5537 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.2639 - val_loss: 1.3858 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.2228 - val_loss: 1.2228 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.1950 - val_loss: 1.1207 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.1853 - val_loss: 1.2395 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.1479 - val_loss: 1.1477 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.1280 - val_loss: 1.1814 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "14/14 - 1s - 69ms/step - loss: 1.0992 - val_loss: 1.3340 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.0872 - val_loss: 1.4133 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.0683 - val_loss: 1.0269 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "14/14 - 1s - 67ms/step - loss: 1.0462 - val_loss: 1.2521 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.0314 - val_loss: 1.3857 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "14/14 - 1s - 68ms/step - loss: 1.0107 - val_loss: 1.1548 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9956 - val_loss: 1.1326 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9966 - val_loss: 1.3382 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9753 - val_loss: 1.6066 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9711 - val_loss: 1.1522 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9520 - val_loss: 1.0382 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9390 - val_loss: 0.9378 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9294 - val_loss: 1.1751 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.9163 - val_loss: 1.0296 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.9159 - val_loss: 1.0941 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.9131 - val_loss: 1.1964 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8949 - val_loss: 1.0651 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8854 - val_loss: 1.0560 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8817 - val_loss: 1.0019 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8787 - val_loss: 1.0413 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8650 - val_loss: 1.1425 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "14/14 - 1s - 69ms/step - loss: 0.8647 - val_loss: 1.0319 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8570 - val_loss: 1.0486 - learning_rate: 2.0000e-04\n",
      "Epoch 37/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8527 - val_loss: 1.0127 - learning_rate: 2.0000e-04\n",
      "Epoch 38/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8512 - val_loss: 1.0388 - learning_rate: 2.0000e-04\n",
      "Epoch 39/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8492 - val_loss: 1.0021 - learning_rate: 2.0000e-04\n",
      "Epoch 40/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8485 - val_loss: 1.0142 - learning_rate: 2.0000e-04\n",
      "Epoch 41/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8475 - val_loss: 1.0217 - learning_rate: 2.0000e-04\n",
      "Epoch 42/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8491 - val_loss: 1.0245 - learning_rate: 2.0000e-04\n",
      "Epoch 43/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8481 - val_loss: 0.9761 - learning_rate: 2.0000e-04\n",
      "Epoch 44/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8470 - val_loss: 1.0319 - learning_rate: 2.0000e-04\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "14/14 - 1s - 68ms/step - loss: 0.8431 - val_loss: 1.0076 - learning_rate: 2.0000e-04\n",
      "Epoch 46/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8423 - val_loss: 1.0271 - learning_rate: 4.0000e-05\n",
      "Epoch 47/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8361 - val_loss: 1.0079 - learning_rate: 4.0000e-05\n",
      "Epoch 48/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8399 - val_loss: 1.0097 - learning_rate: 4.0000e-05\n",
      "Epoch 49/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8417 - val_loss: 1.0151 - learning_rate: 4.0000e-05\n",
      "Epoch 50/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8434 - val_loss: 1.0260 - learning_rate: 4.0000e-05\n",
      "Epoch 51/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8409 - val_loss: 1.0172 - learning_rate: 4.0000e-05\n",
      "Epoch 52/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8402 - val_loss: 1.0060 - learning_rate: 4.0000e-05\n",
      "Epoch 53/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8398 - val_loss: 1.0155 - learning_rate: 4.0000e-05\n",
      "Epoch 54/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8423 - val_loss: 1.0104 - learning_rate: 4.0000e-05\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "14/14 - 1s - 69ms/step - loss: 0.8423 - val_loss: 1.0210 - learning_rate: 4.0000e-05\n",
      "Epoch 56/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8395 - val_loss: 1.0167 - learning_rate: 1.0000e-05\n",
      "Epoch 57/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8405 - val_loss: 1.0128 - learning_rate: 1.0000e-05\n",
      "Epoch 58/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8404 - val_loss: 1.0147 - learning_rate: 1.0000e-05\n",
      "Epoch 59/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8407 - val_loss: 1.0155 - learning_rate: 1.0000e-05\n",
      "Epoch 60/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8373 - val_loss: 1.0122 - learning_rate: 1.0000e-05\n",
      "Epoch 61/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8398 - val_loss: 1.0140 - learning_rate: 1.0000e-05\n",
      "Epoch 62/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8398 - val_loss: 1.0058 - learning_rate: 1.0000e-05\n",
      "Epoch 63/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8399 - val_loss: 1.0051 - learning_rate: 1.0000e-05\n",
      "Epoch 64/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8393 - val_loss: 1.0102 - learning_rate: 1.0000e-05\n",
      "Epoch 65/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8371 - val_loss: 1.0067 - learning_rate: 1.0000e-05\n",
      "Epoch 66/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8391 - val_loss: 1.0057 - learning_rate: 1.0000e-05\n",
      "Epoch 67/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8382 - val_loss: 1.0081 - learning_rate: 1.0000e-05\n",
      "Epoch 68/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8386 - val_loss: 1.0104 - learning_rate: 1.0000e-05\n",
      "Epoch 69/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8358 - val_loss: 1.0125 - learning_rate: 1.0000e-05\n",
      "Epoch 70/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8357 - val_loss: 1.0141 - learning_rate: 1.0000e-05\n",
      "Epoch 71/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8403 - val_loss: 1.0152 - learning_rate: 1.0000e-05\n",
      "Epoch 72/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8384 - val_loss: 1.0087 - learning_rate: 1.0000e-05\n",
      "Epoch 73/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8384 - val_loss: 1.0118 - learning_rate: 1.0000e-05\n",
      "Epoch 74/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8383 - val_loss: 1.0091 - learning_rate: 1.0000e-05\n",
      "Epoch 75/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8370 - val_loss: 1.0084 - learning_rate: 1.0000e-05\n",
      "Epoch 76/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8357 - val_loss: 1.0012 - learning_rate: 1.0000e-05\n",
      "Epoch 77/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8376 - val_loss: 0.9984 - learning_rate: 1.0000e-05\n",
      "Epoch 78/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8393 - val_loss: 1.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 79/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8380 - val_loss: 1.0005 - learning_rate: 1.0000e-05\n",
      "Epoch 80/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8387 - val_loss: 1.0116 - learning_rate: 1.0000e-05\n",
      "Epoch 81/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8369 - val_loss: 1.0083 - learning_rate: 1.0000e-05\n",
      "Epoch 82/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8368 - val_loss: 1.0086 - learning_rate: 1.0000e-05\n",
      "Epoch 83/200\n",
      "14/14 - 1s - 71ms/step - loss: 0.8395 - val_loss: 1.0095 - learning_rate: 1.0000e-05\n",
      "Epoch 84/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8369 - val_loss: 1.0089 - learning_rate: 1.0000e-05\n",
      "Epoch 85/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8379 - val_loss: 1.0018 - learning_rate: 1.0000e-05\n",
      "Epoch 86/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8361 - val_loss: 1.0060 - learning_rate: 1.0000e-05\n",
      "Epoch 87/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8341 - val_loss: 0.9990 - learning_rate: 1.0000e-05\n",
      "Epoch 88/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8353 - val_loss: 1.0007 - learning_rate: 1.0000e-05\n",
      "Epoch 89/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8398 - val_loss: 1.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 90/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8348 - val_loss: 1.0063 - learning_rate: 1.0000e-05\n",
      "Epoch 91/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8357 - val_loss: 1.0060 - learning_rate: 1.0000e-05\n",
      "Epoch 92/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8383 - val_loss: 1.0073 - learning_rate: 1.0000e-05\n",
      "Epoch 93/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8367 - val_loss: 1.0116 - learning_rate: 1.0000e-05\n",
      "Epoch 94/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8367 - val_loss: 1.0108 - learning_rate: 1.0000e-05\n",
      "Epoch 95/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8355 - val_loss: 1.0113 - learning_rate: 1.0000e-05\n",
      "Epoch 96/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8357 - val_loss: 1.0108 - learning_rate: 1.0000e-05\n",
      "Epoch 97/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8370 - val_loss: 1.0149 - learning_rate: 1.0000e-05\n",
      "Epoch 98/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8360 - val_loss: 1.0099 - learning_rate: 1.0000e-05\n",
      "Epoch 99/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8343 - val_loss: 1.0065 - learning_rate: 1.0000e-05\n",
      "Epoch 100/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8361 - val_loss: 1.0044 - learning_rate: 1.0000e-05\n",
      "Epoch 101/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8346 - val_loss: 1.0053 - learning_rate: 1.0000e-05\n",
      "Epoch 102/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8340 - val_loss: 1.0032 - learning_rate: 1.0000e-05\n",
      "Epoch 103/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8350 - val_loss: 1.0028 - learning_rate: 1.0000e-05\n",
      "Epoch 104/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8339 - val_loss: 1.0036 - learning_rate: 1.0000e-05\n",
      "Epoch 105/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8367 - val_loss: 1.0055 - learning_rate: 1.0000e-05\n",
      "Epoch 106/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8374 - val_loss: 1.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 107/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8332 - val_loss: 1.0125 - learning_rate: 1.0000e-05\n",
      "Epoch 108/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8368 - val_loss: 1.0138 - learning_rate: 1.0000e-05\n",
      "Epoch 109/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8321 - val_loss: 1.0080 - learning_rate: 1.0000e-05\n",
      "Epoch 110/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8349 - val_loss: 1.0134 - learning_rate: 1.0000e-05\n",
      "Epoch 111/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8346 - val_loss: 1.0092 - learning_rate: 1.0000e-05\n",
      "Epoch 112/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8335 - val_loss: 1.0099 - learning_rate: 1.0000e-05\n",
      "Epoch 113/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8343 - val_loss: 1.0032 - learning_rate: 1.0000e-05\n",
      "Epoch 114/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8353 - val_loss: 1.0024 - learning_rate: 1.0000e-05\n",
      "Epoch 115/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8352 - val_loss: 1.0020 - learning_rate: 1.0000e-05\n",
      "Epoch 116/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8347 - val_loss: 1.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 117/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8355 - val_loss: 1.0015 - learning_rate: 1.0000e-05\n",
      "Epoch 118/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8345 - val_loss: 1.0130 - learning_rate: 1.0000e-05\n",
      "Epoch 119/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8321 - val_loss: 1.0132 - learning_rate: 1.0000e-05\n",
      "Epoch 120/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8337 - val_loss: 1.0040 - learning_rate: 1.0000e-05\n",
      "Epoch 121/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8339 - val_loss: 1.0044 - learning_rate: 1.0000e-05\n",
      "Epoch 122/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8314 - val_loss: 1.0037 - learning_rate: 1.0000e-05\n",
      "Epoch 123/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8324 - val_loss: 1.0018 - learning_rate: 1.0000e-05\n",
      "Epoch 124/200\n",
      "14/14 - 1s - 71ms/step - loss: 0.8344 - val_loss: 0.9913 - learning_rate: 1.0000e-05\n",
      "Epoch 125/200\n",
      "14/14 - 1s - 72ms/step - loss: 0.8317 - val_loss: 0.9915 - learning_rate: 1.0000e-05\n",
      "Epoch 126/200\n",
      "14/14 - 1s - 78ms/step - loss: 0.8276 - val_loss: 0.9939 - learning_rate: 1.0000e-05\n",
      "Epoch 127/200\n",
      "14/14 - 1s - 73ms/step - loss: 0.8314 - val_loss: 0.9912 - learning_rate: 1.0000e-05\n",
      "Epoch 128/200\n",
      "14/14 - 1s - 77ms/step - loss: 0.8298 - val_loss: 0.9934 - learning_rate: 1.0000e-05\n",
      "Epoch 129/200\n",
      "14/14 - 1s - 73ms/step - loss: 0.8319 - val_loss: 0.9930 - learning_rate: 1.0000e-05\n",
      "Epoch 130/200\n",
      "14/14 - 1s - 75ms/step - loss: 0.8325 - val_loss: 1.0023 - learning_rate: 1.0000e-05\n",
      "Epoch 131/200\n",
      "14/14 - 1s - 78ms/step - loss: 0.8319 - val_loss: 1.0058 - learning_rate: 1.0000e-05\n",
      "Epoch 132/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8298 - val_loss: 0.9988 - learning_rate: 1.0000e-05\n",
      "Epoch 133/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8332 - val_loss: 0.9967 - learning_rate: 1.0000e-05\n",
      "Epoch 134/200\n",
      "14/14 - 1s - 73ms/step - loss: 0.8298 - val_loss: 0.9975 - learning_rate: 1.0000e-05\n",
      "Epoch 135/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8323 - val_loss: 1.0092 - learning_rate: 1.0000e-05\n",
      "Epoch 136/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8315 - val_loss: 1.0061 - learning_rate: 1.0000e-05\n",
      "Epoch 137/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8296 - val_loss: 1.0065 - learning_rate: 1.0000e-05\n",
      "Epoch 138/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8323 - val_loss: 1.0016 - learning_rate: 1.0000e-05\n",
      "Epoch 139/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8309 - val_loss: 0.9984 - learning_rate: 1.0000e-05\n",
      "Epoch 140/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8327 - val_loss: 0.9889 - learning_rate: 1.0000e-05\n",
      "Epoch 141/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8333 - val_loss: 0.9961 - learning_rate: 1.0000e-05\n",
      "Epoch 142/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8312 - val_loss: 1.0009 - learning_rate: 1.0000e-05\n",
      "Epoch 143/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8299 - val_loss: 0.9977 - learning_rate: 1.0000e-05\n",
      "Epoch 144/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8289 - val_loss: 0.9952 - learning_rate: 1.0000e-05\n",
      "Epoch 145/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8326 - val_loss: 0.9970 - learning_rate: 1.0000e-05\n",
      "Epoch 146/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8318 - val_loss: 1.0010 - learning_rate: 1.0000e-05\n",
      "Epoch 147/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8278 - val_loss: 1.0081 - learning_rate: 1.0000e-05\n",
      "Epoch 148/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8319 - val_loss: 1.0069 - learning_rate: 1.0000e-05\n",
      "Epoch 149/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8294 - val_loss: 0.9968 - learning_rate: 1.0000e-05\n",
      "Epoch 150/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8284 - val_loss: 0.9938 - learning_rate: 1.0000e-05\n",
      "Epoch 151/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8311 - val_loss: 0.9990 - learning_rate: 1.0000e-05\n",
      "Epoch 152/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8295 - val_loss: 0.9970 - learning_rate: 1.0000e-05\n",
      "Epoch 153/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8290 - val_loss: 1.0006 - learning_rate: 1.0000e-05\n",
      "Epoch 154/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8261 - val_loss: 1.0082 - learning_rate: 1.0000e-05\n",
      "Epoch 155/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8247 - val_loss: 1.0176 - learning_rate: 1.0000e-05\n",
      "Epoch 156/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8279 - val_loss: 1.0181 - learning_rate: 1.0000e-05\n",
      "Epoch 157/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8270 - val_loss: 0.9877 - learning_rate: 1.0000e-05\n",
      "Epoch 158/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8293 - val_loss: 1.0153 - learning_rate: 1.0000e-05\n",
      "Epoch 159/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8247 - val_loss: 1.0131 - learning_rate: 1.0000e-05\n",
      "Epoch 160/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8231 - val_loss: 1.0203 - learning_rate: 1.0000e-05\n",
      "Epoch 161/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8251 - val_loss: 1.0268 - learning_rate: 1.0000e-05\n",
      "Epoch 162/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8233 - val_loss: 1.0009 - learning_rate: 1.0000e-05\n",
      "Epoch 163/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8263 - val_loss: 1.0221 - learning_rate: 1.0000e-05\n",
      "Epoch 164/200\n",
      "14/14 - 1s - 71ms/step - loss: 0.8224 - val_loss: 1.0170 - learning_rate: 1.0000e-05\n",
      "Epoch 165/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8225 - val_loss: 1.0503 - learning_rate: 1.0000e-05\n",
      "Epoch 166/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8220 - val_loss: 0.9909 - learning_rate: 1.0000e-05\n",
      "Epoch 167/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.8227 - val_loss: 1.0361 - learning_rate: 1.0000e-05\n",
      "Epoch 168/200\n",
      "14/14 - 1s - 72ms/step - loss: 0.8198 - val_loss: 1.0096 - learning_rate: 1.0000e-05\n",
      "Epoch 169/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8179 - val_loss: 1.0217 - learning_rate: 1.0000e-05\n",
      "Epoch 170/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8163 - val_loss: 1.0329 - learning_rate: 1.0000e-05\n",
      "Epoch 171/200\n",
      "14/14 - 1s - 71ms/step - loss: 0.8173 - val_loss: 1.0498 - learning_rate: 1.0000e-05\n",
      "Epoch 172/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8118 - val_loss: 1.0441 - learning_rate: 1.0000e-05\n",
      "Epoch 173/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8150 - val_loss: 1.0578 - learning_rate: 1.0000e-05\n",
      "Epoch 174/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.8112 - val_loss: 1.0926 - learning_rate: 1.0000e-05\n",
      "Epoch 175/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.8064 - val_loss: 1.0359 - learning_rate: 1.0000e-05\n",
      "Epoch 176/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.8086 - val_loss: 1.1081 - learning_rate: 1.0000e-05\n",
      "Epoch 177/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.8023 - val_loss: 1.0634 - learning_rate: 1.0000e-05\n",
      "Epoch 178/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7980 - val_loss: 1.0832 - learning_rate: 1.0000e-05\n",
      "Epoch 179/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7922 - val_loss: 1.1176 - learning_rate: 1.0000e-05\n",
      "Epoch 180/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7978 - val_loss: 1.0933 - learning_rate: 1.0000e-05\n",
      "Epoch 181/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7896 - val_loss: 0.9905 - learning_rate: 1.0000e-05\n",
      "Epoch 182/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.7899 - val_loss: 1.1344 - learning_rate: 1.0000e-05\n",
      "Epoch 183/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7905 - val_loss: 1.1751 - learning_rate: 1.0000e-05\n",
      "Epoch 184/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7900 - val_loss: 1.0482 - learning_rate: 1.0000e-05\n",
      "Epoch 185/200\n",
      "14/14 - 1s - 70ms/step - loss: 0.7790 - val_loss: 1.1342 - learning_rate: 1.0000e-05\n",
      "Epoch 186/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7796 - val_loss: 1.0888 - learning_rate: 1.0000e-05\n",
      "Epoch 187/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.7747 - val_loss: 1.1692 - learning_rate: 1.0000e-05\n",
      "Epoch 188/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7697 - val_loss: 1.0240 - learning_rate: 1.0000e-05\n",
      "Epoch 189/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.7716 - val_loss: 1.1002 - learning_rate: 1.0000e-05\n",
      "Epoch 190/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7586 - val_loss: 1.0867 - learning_rate: 1.0000e-05\n",
      "Epoch 191/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.7655 - val_loss: 1.1312 - learning_rate: 1.0000e-05\n",
      "Epoch 192/200\n",
      "14/14 - 1s - 69ms/step - loss: 0.7488 - val_loss: 0.9066 - learning_rate: 1.0000e-05\n",
      "Epoch 193/200\n",
      "14/14 - 1s - 71ms/step - loss: 0.7623 - val_loss: 1.2990 - learning_rate: 1.0000e-05\n",
      "Epoch 194/200\n",
      "14/14 - 1s - 73ms/step - loss: 0.7626 - val_loss: 1.1502 - learning_rate: 1.0000e-05\n",
      "Epoch 195/200\n",
      "14/14 - 1s - 68ms/step - loss: 0.7673 - val_loss: 1.0388 - learning_rate: 1.0000e-05\n",
      "Epoch 196/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7431 - val_loss: 1.0248 - learning_rate: 1.0000e-05\n",
      "Epoch 197/200\n",
      "14/14 - 1s - 66ms/step - loss: 0.7400 - val_loss: 1.2061 - learning_rate: 1.0000e-05\n",
      "Epoch 198/200\n",
      "14/14 - 1s - 65ms/step - loss: 0.7545 - val_loss: 0.8817 - learning_rate: 1.0000e-05\n",
      "Epoch 199/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7396 - val_loss: 1.2115 - learning_rate: 1.0000e-05\n",
      "Epoch 200/200\n",
      "14/14 - 1s - 67ms/step - loss: 0.7381 - val_loss: 1.2262 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Preparar los datos por cluster\n",
    "for cluster in range(n_clusters):\n",
    "    print(f'Entrenando cluster numero: {cluster}')\n",
    "    cluster_data = grouped_dff[grouped_dff['cluster'] == cluster].copy()\n",
    "    cluster_data.sort_values(by='periodo', inplace=True)\n",
    "    \n",
    "    # Preparar los datos para LSTM\n",
    "    X, y = [], []\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn_scaled','suma_total_2019_scaled','suma_total_2018_scaled','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']].values\n",
    "        if len(series) > 2:  # Asegurarse de que haya suficientes datos\n",
    "            X.append(series[:-2])  # Todos los datos excepto los últimos 2\n",
    "            y.append(series[-1, 0])  # Selecciona solo tn_scaled como objetivo\n",
    "\n",
    "    # Pad y reshape de X\n",
    "    max_len = max(len(seq) for seq in X)\n",
    "    X_padded = np.array([np.pad(seq, ((max_len - len(seq), 0), (0, 0)), mode='constant') for seq in X]).astype(np.float32)\n",
    "    y = np.array(y).astype(np.float32)\n",
    "    \n",
    "    if len(X_padded) == 0 or len(y) == 0:\n",
    "        continue\n",
    "\n",
    "    # Construir y entrenar el modelo\n",
    "    model = build_model((X_padded.shape[1], X_padded.shape[2]))\n",
    "    model.fit(X_padded, y, epochs=200, verbose=2, batch_size=128, validation_split=0.2, callbacks=[reduce_lr])\n",
    "    models[cluster] = model\n",
    "\n",
    "    # Hacer predicciones\n",
    "    for key, data in cluster_data.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id']):\n",
    "        series = data[['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'tn_scaled','suma_total_2019_scaled','suma_total_2018_scaled','variacion_feb2018_vs_dic2017','variacion_feb2019_vs_dic2018']].values\n",
    "        \n",
    "        if len(series) > 2:\n",
    "            max_len = len(series) - 1\n",
    "            X_pred = np.pad(series[1:], ((max_len - len(series[1:]), 0), (0, 0)), mode='constant').astype(np.float32)\n",
    "            X_pred = np.reshape(X_pred, (1, X_pred.shape[0], X_pred.shape[1]))\n",
    "            pred = model.predict(X_pred, verbose=0)\n",
    "            predictions.append([key[0], key[1], key[2], key[3], pred[0][0], key[4]])\n",
    "\n",
    "    # Guardar las predicciones en un DataFrame y exportar a CSV\n",
    "    pred_dff_temp = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediccion', 'customer_id'])\n",
    "    pred_dff_temp.to_csv(f\"predicciones_temporales_cluster_{cluster}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>prediccion</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969927</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969929</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969930</td>\n",
       "      <td>10106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969931</td>\n",
       "      <td>10144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0.969932</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  prediccion  customer_id\n",
       "0     0     0     4     22    0.969927        10001\n",
       "1     0     0     4     22    0.969929        10032\n",
       "2     0     0     4     22    0.969930        10106\n",
       "3     0     0     4     22    0.969931        10144\n",
       "4     0     0     4     22    0.969932        10225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dff_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dff_temp.to_csv(\"C:/Users/Usuario/desktop/vero2/pred_df_temp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>prediccion</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.192217</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.192225</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.192235</td>\n",
       "      <td>10106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.192240</td>\n",
       "      <td>10144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7.192249</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat1  cat2  cat3  brand  prediccion  customer_id\n",
       "0     0     0     4     22    7.192217        10001\n",
       "1     0     0     4     22    7.192225        10032\n",
       "2     0     0     4     22    7.192235        10106\n",
       "3     0     0     4     22    7.192240        10144\n",
       "4     0     0     4     22    7.192249        10225"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dff = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediccion', 'customer_id'])\n",
    "# Suponiendo que `scaler` es el objeto StandardScaler que usaste para escalar `tn` originalmente\n",
    "\n",
    "# Desescalar las predicciones\n",
    "predicciones_desescaladas = scaler.inverse_transform(pred_dff['prediccion'].values.reshape(-1, 1))\n",
    "\n",
    "# Reemplazar las predicciones escaladas con las desescaladas en el DataFrame\n",
    "pred_dff['prediccion'] = predicciones_desescaladas.flatten()\n",
    "\n",
    "# Ahora pred_df tiene las predicciones desescaladas en la columna 'prediccion'\n",
    "pred_dff.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat1  cat2  cat3  brand  prediccion  customer_id  product_id\n",
      "0     0     0     4     22    7.192217        10001       20609\n",
      "1     0     0     4     22    7.192225        10032       20609\n",
      "2     0     0     4     22    7.192235        10106       20609\n",
      "3     0     0     4     22    7.192240        10144       20609\n",
      "4     0     0     4     22    7.192249        10225       20609\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Obtener combinaciones únicas de 'product_id', 'cat1', 'cat2', 'cat3', 'brand' del DataFrame original\n",
    "distinct_combinations = df[['product_id', 'cat1', 'cat2', 'cat3', 'brand']].drop_duplicates()\n",
    "\n",
    "# Paso 2: Realizar un join con pred_df en las columnas correspondientes\n",
    "pred_dff = pred_dff.merge(distinct_combinations, on=['cat1', 'cat2', 'cat3', 'brand'], how='left')\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones y las nuevas columnas agregadas\n",
    "print(pred_dff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cat1  cat2  cat3  brand  prediccion  customer_id  product_id  \\\n",
      "0     0     0     4     22    7.192217        10001       20609   \n",
      "1     0     0     4     22    7.192225        10032       20609   \n",
      "2     0     0     4     22    7.192235        10106       20609   \n",
      "3     0     0     4     22    7.192240        10144       20609   \n",
      "4     0     0     4     22    7.192249        10225       20609   \n",
      "\n",
      "   prediccion_ajustada  \n",
      "0             7.192217  \n",
      "1             7.192225  \n",
      "2             7.192235  \n",
      "3             7.192240  \n",
      "4             7.192249  \n"
     ]
    }
   ],
   "source": [
    "# Inicializar una lista para almacenar las predicciones ajustadas\n",
    "predictions_adjusted = []\n",
    "\n",
    "# Iterar sobre las filas de pred_df\n",
    "for index, row in pred_dff.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'], row['product_id'])\n",
    "    \n",
    "    # Buscar el ratio correspondiente en ratio_dict utilizando la clave correcta\n",
    "    if key in ratio_dict:\n",
    "        ratio = ratio_dict[key]\n",
    "        # Calcular la predicción ajustada\n",
    "        prediccion_ajustada = row['prediccion'] * ratio\n",
    "        predictions_adjusted.append(prediccion_ajustada)\n",
    "    else:\n",
    "        predictions_adjusted.append(row['prediccion'])  # Mantener la predicción original si no hay ratio definido\n",
    "\n",
    "# Agregar las predicciones ajustadas al DataFrame pred_df\n",
    "pred_dff['prediccion_ajustada'] = predictions_adjusted\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones ajustadas\n",
    "print(pred_dff.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product_id  prediccion_ajustada\n",
      "0         20001          3014.724754\n",
      "1         20002          2967.203984\n",
      "2         20003          2888.175338\n",
      "3         20004          2888.175338\n",
      "4         20005          2888.175338\n",
      "..          ...                  ...\n",
      "775       21263           442.394968\n",
      "776       21265          1182.121398\n",
      "777       21266          1182.121398\n",
      "778       21267           255.482780\n",
      "779       21276           255.482780\n",
      "\n",
      "[780 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sumarizar las predicciones ajustadas por product_id\n",
    "summarized_predictions = pred_dff.groupby('product_id')['prediccion_ajustada'].sum().reset_index()\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones sumarizadas\n",
    "print(summarized_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las predicciones finales en un archivo CSV\n",
    "summarized_predictions.to_csv(\"C:/Users/Usuario/desktop/vero2/modelodtw2002.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
