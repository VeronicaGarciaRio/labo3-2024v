{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "\n",
    "# Paso 1: Cargar y preprocesar los datos\n",
    "file_path = \"C:/Users/Usuario/desktop/vero2/final_dataset_completo_con_ceros.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], var_name='periodo', value_name='tn')\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas categóricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Agrupar las ventas por periodo, cat1, cat2, cat3, brand y customer_id\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({'tn': 'sum'}).reset_index()\n",
    "\n",
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a la columna tn\n",
    "scaler = StandardScaler()\n",
    "scaled_df['tn'] = scaler.fit_transform(scaled_df[['tn']])\n",
    "scalers['tn'] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "# Pivotear los datos escalados para clustering\n",
    "pivot_df = scaled_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], columns='periodo', values='tn', fill_value=0)\n",
    "pivot_df_values = pivot_df.values\n",
    "\n",
    "# Determinar el número óptimo de clusters utilizando el diagrama de codo\n",
    "distortions = []\n",
    "K = range(6, 15)\n",
    "for k in K:\n",
    "    kmeans_model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "    kmeans_model.fit(pivot_df_values)\n",
    "    distortions.append(kmeans_model.inertia_)\n",
    "\n",
    "# Visualizar el diagrama de codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Elegir el número óptimo de clusters (e.g., 4 basado en el codo)\n",
    "optimal_k = 15\n",
    "kmeans_model = TimeSeriesKMeans(n_clusters=optimal_k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(pivot_df_values)\n",
    "\n",
    "# Añadir el cluster al DataFrame original\n",
    "pivot_df['cluster'] = clusters\n",
    "grouped_df = pd.merge(grouped_df, pivot_df['cluster'].reset_index(), on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])\n",
    "\n",
    "# Paso 2: Calcular los ratios incluyendo customer_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 12)]\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['tn'].sum().reset_index()\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index()\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['ratio'].to_dict()\n",
    "\n",
    "# Paso 4: Creación de secuencias de tiempo\n",
    "def crear_secuencias(data, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - step_ahead):\n",
    "        X.append(data[['tn']].iloc[i:i + n_steps].values)\n",
    "        y.append(data['tn'].iloc[i + n_steps + step_ahead - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Paso 5: Construcción y entrenamiento de los modelos LSTM\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((input_shape[0] // 2, 64)))\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def process_data_in_batches(scaled_df, n_steps, step_ahead=1):\n",
    "    models = {}\n",
    "    predictions = []\n",
    "\n",
    "    for cluster, group_data in scaled_df.groupby('cluster'):\n",
    "        group_data = group_data.sort_values(by='periodo')\n",
    "        X, y = crear_secuencias(group_data, n_steps, step_ahead)\n",
    "\n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        model = build_lstm_model((X.shape[1], X.shape[2]))\n",
    "        model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "        model_key = cluster\n",
    "        models[model_key] = model\n",
    "\n",
    "        X_pred = group_data[['tn']].values[-(n_steps + step_ahead):-step_ahead].reshape((1, n_steps, 1))\n",
    "        pred = model.predict(X_pred, verbose=0)\n",
    "        pred_original_scale = scalers['tn'].inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "        pred_original_scale = np.clip(pred_original_scale, 0, None)\n",
    "        predictions.append([cluster, pred_original_scale[0]])\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "# Llamada a la función con el DataFrame escalado y clusterizado\n",
    "n_steps = 13\n",
    "step_ahead = 2\n",
    "models, predictions = process_data_in_batches(scaled_df, n_steps, step_ahead)\n",
    "\n",
    "# Convertir las predicciones a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['cluster', 'prediction'])\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in predictions_df.iterrows():\n",
    "    cluster = row['cluster']\n",
    "    for (cat1, cat2, cat3, brand, customer_id, product_id), ratio in ratio_dict.items():\n",
    "        if cluster == clusters[pivot_df.index.get_loc((cat1, cat2, cat3, brand, customer_id))]:\n",
    "            final_predictions.append([product_id, row['prediction'] * ratio])\n",
    "\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediction'])\n",
    "\n",
    "# Calcular el promedio de tn para los últimos 12 meses para cada product_id\n",
    "last_12_months = df[df['periodo'] >= df['periodo'].max() - pd.DateOffset(months=12)]\n",
    "average_tn_last_12_months = last_12_months.groupby('product_id')['tn'].mean().reset_index()\n",
    "\n",
    "# Identificar los product_id faltantes en final_predictions_df\n",
    "missing_product_ids = set(df['product_id']) - set(final_predictions_df['product_id'])\n",
    "average_predictions = average_tn_last_12_months[average_tn_last_12_months['product_id'].isin(missing_product_ids)]\n",
    "\n",
    "# Concatenar las predicciones promedio con las predicciones finales\n",
    "complete_predictions = pd.concat([final_predictions_df, average_predictions], ignore_index=True)\n",
    "\n",
    "# Guardar las predicciones finales\n",
    "complete_predictions_df.to_csv(\"C:/Users/Usuario/desktop/vero2/predicciones_finales_DTWv2.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
