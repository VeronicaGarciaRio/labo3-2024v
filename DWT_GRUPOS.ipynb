{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"C:/Users/vgarciario/desktop/UA MASTER/labo3/final_dataset_descr.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Filtrar los datos por los periodos 201907, 201908 y 201909\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "\n",
    "# Pivotear los datos para tener columnas separadas para cada periodo\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "\n",
    "# Asegurar que las columnas 201907 y 201909 existen en el DataFrame\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "\n",
    "# Calcular el promedio de julio y septiembre para agosto\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "\n",
    "# Convertir de nuevo al formato largo\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], \n",
    "                                   var_name='periodo', value_name='tn')\n",
    "\n",
    "# Unir con el dataframe original\n",
    "df = df.drop(columns=['tn'])\n",
    "df = pd.merge(df, updated_sales, on=['product_id', 'customer_id', 'periodo'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 2: Buscar los 'product_id' que tengan poca historia\n",
    "training_threshold = 3\n",
    "product_history = df.groupby(['product_id', 'periodo']).size().reset_index(name='counts')\n",
    "products_to_keep = product_history[product_history['counts'] >= training_threshold]['product_id'].unique()\n",
    "df_filtered = df[df['product_id'].isin(products_to_keep)]\n",
    "\n",
    "# Crear el DataFrame \"Predicciones\" para productos con poca historia\n",
    "products_to_predict = product_history[product_history['counts'] < training_threshold]['product_id'].unique()\n",
    "predicciones = df[df['product_id'].isin(products_to_predict)].groupby('product_id')['tn'].mean().reset_index()\n",
    "predicciones.rename(columns={'tn': 'prediccion'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 3: Aplicar LabelEncoder a las columnas categoricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_filtered[col] = le.fit_transform(df_filtered[col])\n",
    "    label_encoders[col] = le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Agrupar las ventas por periodo, cat1, cat2, cat3, marca y descripcion\n",
    "# Filtrar datos para el año 2019 (si 'periodo' es numérico)\n",
    "df_2019 = df[(df['periodo'] >= 201901) & (df['periodo'] <= 201912)]\n",
    "\n",
    "# Agrupar las ventas del año 2019 por periodo, cat1, cat2, cat3, brand, descripcion y product_id\n",
    "grouped_sales_2019 = df_2019.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['tn'].sum().reset_index()\n",
    "\n",
    "# Calcular el total de ventas por grupo para el año 2019\n",
    "group_totals_2019 = grouped_sales_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['tn'].sum().reset_index()\n",
    "\n",
    "# Calcular el promedio de ventas por periodo para el año 2019\n",
    "average_sales_2019 = grouped_sales_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['tn'].mean().reset_index()\n",
    "\n",
    "# Unir para calcular el ratio\n",
    "ratios_2019 = pd.merge(average_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'descripcion'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "\n",
    "# Crear un diccionario de ratios\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['ratio'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Paso 5: Agrupar las ventas por periodo, cat1, cat2, cat3, marca y customer_id\n",
    "grouped_df = df_filtered.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['cust_request_qty', 'cust_request_tn', 'tn']:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df[col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m K:\n\u001b[0;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m TimeSeriesKMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtw\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_series_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     distortions\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39minertia_)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Graficar el codo\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:821\u001b[0m, in \u001b[0;36mTimeSeriesKMeans.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_successful \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    820\u001b[0m n_attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 821\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_one_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minertia_ \u001b[38;5;241m<\u001b[39m min_inertia:\n\u001b[0;32m    823\u001b[0m     best_correct_centroids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:675\u001b[0m, in \u001b[0;36mTimeSeriesKMeans._fit_one_init\u001b[1;34m(self, X, x_squared_norms, rs)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect metric: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftdtw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[0;32m    674\u001b[0m             )\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_centers_ \u001b[38;5;241m=\u001b[39m \u001b[43m_k_init_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdist_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrs\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    679\u001b[0m     indices \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mchoice(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:125\u001b[0m, in \u001b[0;36m_k_init_metric\u001b[1;34m(X, n_clusters, cdist_metric, random_state, n_local_trials)\u001b[0m\n\u001b[0;32m    122\u001b[0m numpy\u001b[38;5;241m.\u001b[39mclip(candidate_ids, \u001b[38;5;28;01mNone\u001b[39;00m, closest_dist_sq\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, out\u001b[38;5;241m=\u001b[39mcandidate_ids)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Compute distances to center candidates\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m distance_to_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mcdist_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidate_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# update closest distances squared and potential for each candidate\u001b[39;00m\n\u001b[0;32m    128\u001b[0m numpy\u001b[38;5;241m.\u001b[39mminimum(\n\u001b[0;32m    129\u001b[0m     closest_dist_sq, distance_to_candidates, out\u001b[38;5;241m=\u001b[39mdistance_to_candidates\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\clustering\\kmeans.py:657\u001b[0m, in \u001b[0;36mTimeSeriesKMeans._fit_one_init.<locals>.metric_fun\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetric_fun\u001b[39m(x, y):\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_dtw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_params\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\dtw_variants.py:1934\u001b[0m, in \u001b[0;36mcdist_dtw\u001b[1;34m(dataset1, dataset2, global_constraint, sakoe_chiba_radius, itakura_max_slope, n_jobs, verbose, be)\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cross-similarity matrix using Dynamic Time Warping (DTW)\u001b[39;00m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;124;03msimilarity measure.\u001b[39;00m\n\u001b[0;32m   1835\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;124;03m       Signal Processing, vol. 26(1), pp. 43--49, 1978.\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1933\u001b[0m be \u001b[38;5;241m=\u001b[39m instantiate_backend(be, dataset1, dataset2)\n\u001b[1;32m-> 1934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cdist_generic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_diagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43msakoe_chiba_radius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msakoe_chiba_radius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitakura_max_slope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitakura_max_slope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\utils.py:101\u001b[0m, in \u001b[0;36m_cdist_generic\u001b[1;34m(dist_fun, dataset1, dataset2, n_jobs, verbose, compute_diagonal, dtype, be, *args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     dataset2 \u001b[38;5;241m=\u001b[39m to_time_series_dataset(dataset2, dtype\u001b[38;5;241m=\u001b[39mdtype, be\u001b[38;5;241m=\u001b[39mbe)\n\u001b[1;32m--> 101\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m be\u001b[38;5;241m.\u001b[39mreshape(be\u001b[38;5;241m.\u001b[39marray(matrix), (\u001b[38;5;28mlen\u001b[39m(dataset1), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tslearn\\metrics\\dtw_variants.py:795\u001b[0m, in \u001b[0;36mdtw\u001b[1;34m(s1, s2, global_constraint, sakoe_chiba_radius, itakura_max_slope, be)\u001b[0m\n\u001b[0;32m    786\u001b[0m mask \u001b[38;5;241m=\u001b[39m compute_mask(\n\u001b[0;32m    787\u001b[0m     s1,\n\u001b[0;32m    788\u001b[0m     s2,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m     be\u001b[38;5;241m=\u001b[39mbe,\n\u001b[0;32m    793\u001b[0m )\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m be\u001b[38;5;241m.\u001b[39mis_numpy:\n\u001b[1;32m--> 795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_njit_dtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dtw(s1, s2, mask\u001b[38;5;241m=\u001b[39mmask, be\u001b[38;5;241m=\u001b[39mbe)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Paso 6: Agrupar en clusters con dtw y hacer grafico de codo para determinar la cantidad optima de clusters\n",
    "pivot_table = scaled_df.pivot_table(index=['customer_id', 'cat1', 'cat2', 'cat3', 'brand'], columns='periodo', values='tn', fill_value=0)\n",
    "time_series_data = pivot_table.values\n",
    "scaler = TimeSeriesScalerMinMax()\n",
    "time_series_data = scaler.fit_transform(time_series_data)\n",
    "\n",
    "# Método del codo para determinar el número óptimo de clusters\n",
    "distortions = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=10, random_state=0)\n",
    "    model.fit(time_series_data)\n",
    "    distortions.append(model.inertia_)\n",
    "\n",
    "# Graficar el codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Número de clusters')\n",
    "plt.ylabel('Distorsión (Inercia)')\n",
    "plt.title('Método del Codo para determinar el número óptimo de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Suponiendo que el codo sugiere 3 clusters (ajustar según el gráfico)\n",
    "optimal_clusters = 3\n",
    "model = TimeSeriesKMeans(n_clusters=optimal_clusters, metric=\"dtw\", max_iter=10, random_state=0)\n",
    "labels = model.fit_predict(time_series_data)\n",
    "pivot_table['cluster'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 7: Armar un modelo LSTM bidireccional para predecir las ventas\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True), input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(256, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(512, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(256, activation='tanh', kernel_regularizer=l2(0.7), return_sequences=True)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(128, activation='relu', kernel_regularizer=l2(0.7))))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Preparar datos y entrenar el modelo LSTM\n",
    "grouped_df['periodo'] = pd.to_datetime(grouped_df['periodo'], format='%Y%m')\n",
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "for (cat1, cat2, cat3, brand, descripcion), group_data in grouped_df.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion']):\n",
    "    group_data = group_data.sort_values(by='periodo')\n",
    "    n_steps = 2\n",
    "    X, y = [], []\n",
    "    for i in range(len(group_data) - n_steps):\n",
    "        X.append(group_data[['cust_request_qty', 'cust_request_tn', 'tn']].iloc[i:i+n_steps].values)\n",
    "        y.append(group_data['tn'].iloc[i+n_steps])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    model = build_lstm_model((X.shape[1], X.shape[2]))\n",
    "    model.fit(X, y, epochs=100, verbose=0)\n",
    "    models[(cat1, cat2, cat3, brand, descripcion)] = model\n",
    "    \n",
    "    X_pred = group_data[['cust_request_qty', 'cust_request_tn', 'tn']].values[-n_steps:]\n",
    "    X_pred = X_pred.reshape((1, X_pred.shape[0], X_pred.shape[1]))\n",
    "    pred = model.predict(X_pred, verbose=0)\n",
    "    predictions.append([cat1, cat2, cat3, brand, descripcion, pred[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 8: Convertir las predicciones a un DataFrame\n",
    "pred_df = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'prediccion'])\n",
    "\n",
    "# Sumarizar las predicciones por grupo\n",
    "summarized_preds = pred_df.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion'])['prediccion'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 9: Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in summarized_preds.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'], row['descripcion'])\n",
    "    for (cat1, cat2, cat3, brand, descripcion, product_id), ratio in ratio_dict.items():\n",
    "        if (cat1, cat2, cat3, brand, descripcion) == key:\n",
    "            final_predictions.append([product_id, row['prediccion'] * ratio])\n",
    "\n",
    "# Convertir las predicciones finales a un DataFrame\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediccion'])\n",
    "\n",
    "# Paso 10: Desescalar las predicciones finales\n",
    "scalers = joblib.load('scalers.pkl')\n",
    "final_predictions_df['prediccion'] = scalers['tn'].inverse_transform(final_predictions_df[['prediccion']])\n",
    "\n",
    "# Paso 11: Unificar con el DataFrame \"Predicciones\"\n",
    "final_df = pd.concat([final_predictions_df, predicciones], ignore_index=True)\n",
    "\n",
    "# Guardar el resultado en un archivo CSV\n",
    "final_df.to_csv('predicciones_finales.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
