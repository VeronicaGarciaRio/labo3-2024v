{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "en esta version tomo para el marketshare FEB19, si en algun mes de 2019 no esta le calcula el promedio de 2019 al final +  la arquitectura de German. si la prediccion es negativa pone 0. clusteriza con 15 (categorias, brand y customer)\n",
    "el dtwv1 tira muchos productos, el dtwv1.1 lo corregi con chatgpt, el dtw2 es anterior en algun intento de modificar el codigo pero tambien creo terminaba. esta version trato de unir con el codigo cuando era por agrupacion no por cluster que funcionaba bien (adaptando a clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
    "import joblib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "\n",
    "# Paso 1: Cargar y preprocesar los datos\n",
    "file_path = \"C:/Users/Usuario/desktop/vero2/final_dataset_completo_con_ceros.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>close_quarter</th>\n",
       "      <th>age</th>\n",
       "      <th>mes_inicial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>254.62373</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>393.26092</td>\n",
       "      <td>386.60688</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>309.90610</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>142.87158</td>\n",
       "      <td>130.54927</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>20001</td>\n",
       "      <td>201904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>364.37071</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000</td>\n",
       "      <td>genoma</td>\n",
       "      <td>Q2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  periodo  plan_precios_cuidados  cust_request_qty  \\\n",
       "0        10001       20001   201812                    0.0              20.0   \n",
       "1        10001       20001   201901                    0.0              53.0   \n",
       "2        10001       20001   201902                    0.0              39.0   \n",
       "3        10001       20001   201903                    0.0              23.0   \n",
       "4        10001       20001   201904                    0.0              33.0   \n",
       "\n",
       "   cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
       "0        254.62373  254.62373   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "1        393.26092  386.60688   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "2        309.90610  309.90610   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "3        142.87158  130.54927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "4        364.37071  364.37071   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
       "\n",
       "  descripcion quarter  month  close_quarter   age mes_inicial  \n",
       "0      genoma      Q4     12            1.0  23.0  2018-12-01  \n",
       "1      genoma      Q1      1            0.0  24.0  2018-12-01  \n",
       "2      genoma      Q1      2            0.0  25.0  2018-12-01  \n",
       "3      genoma      Q1      3            1.0  26.0  2018-12-01  \n",
       "4      genoma      Q2      4            0.0  27.0  2018-12-01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], var_name='periodo', value_name='tn')\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas categóricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Agrupar las ventas por periodo, cat1, cat2, cat3, brand y customer_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m', errors='coerce')\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({'tn': 'sum'}).reset_index()\n",
    "\n",
    "# Paso 2: Calcular los ratios incluyendo customer_id\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 12)]\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['tn'].sum().reset_index()\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index()\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['ratio'].to_dict()\n",
    "\n",
    "# Paso 3: Clustering con DTW\n",
    "pivot_df = grouped_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], columns='periodo', values='tn', fill_value=0)\n",
    "pivot_df_values = pivot_df.values\n",
    "\n",
    "# Escalar los datos\n",
    "scalers = {}\n",
    "scaler = StandardScaler()\n",
    "pivot_df_scaled = scaler.fit_transform(pivot_df_values)\n",
    "\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['tn']:\n",
    "    scaler = StandardScaler()\n",
    "    pivot_df_scaled [col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinar el número óptimo de clusters utilizando el diagrama de codo\n",
    "distortions = []\n",
    "K = range(6, 15)\n",
    "for k in K:\n",
    "    kmeans_model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "    kmeans_model.fit(pivot_df_scaled)\n",
    "    distortions.append(kmeans_model.inertia_)\n",
    "\n",
    " Visualizar el diagrama de codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002471CAB6CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002471E41F420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|██████████| 15/15 [1:16:44<00:00, 306.99s/it]   \n"
     ]
    }
   ],
   "source": [
    "LLEGUE HASTA ACA\n",
    "\n",
    "# Cargar los scalers guardados\n",
    "scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "# Asumiendo que scaled_df ya está cargado y escalado\n",
    "scaled_df['periodo'] = pd.to_datetime(scaled_df['periodo'], format='%Y%m')\n",
    "\n",
    "# Función para crear secuencias de tiempo\n",
    "def crear_secuencias(data, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - step_ahead):\n",
    "        X.append(data[['tn']].iloc[i:i + n_steps].values)\n",
    "        y.append(data['tn'].iloc[i + n_steps + step_ahead - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Función para construir el modelo LSTM\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(input_shape)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((input_shape[0] // 2, 64)))\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Función para procesar los datos en lotes\n",
    "def process_data_in_batches(scaled_df, clusters, pivot_df, ratio_dict, n_steps, step_ahead=1, batch_size=10000):\n",
    "    models = {}\n",
    "    predictions = []\n",
    "    scalers = {}\n",
    "\n",
    "    # Iterar sobre cada cluster\n",
    "    unique_clusters = scaled_df['cluster'].unique()\n",
    "\n",
    "    for cluster in tqdm(unique_clusters, desc='Processing clusters'):\n",
    "        group_data = scaled_df[scaled_df['cluster'] == cluster]\n",
    "        group_data = group_data.sort_values(by='periodo')\n",
    "\n",
    "        # Dividir en lotes\n",
    "        num_batches = len(group_data) // batch_size\n",
    "        for i in range(num_batches + 1):\n",
    "            batch_data = group_data.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            X, y = crear_secuencias(batch_data, n_steps, step_ahead)\n",
    "\n",
    "            if len(X) == 0 or len(y) == 0:\n",
    "                continue\n",
    "\n",
    "            # Escalar los datos\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "            # Construir y entrenar modelo LSTM\n",
    "            model = build_lstm_model((X_scaled.shape[1], X_scaled.shape[2]))\n",
    "            model.fit(X_scaled, y, epochs=100, verbose=0)\n",
    "\n",
    "            # Guardar modelo en el diccionario\n",
    "            model_key = f\"{cluster}_batch_{i}\"\n",
    "            models[model_key] = model\n",
    "            scalers[model_key] = scaler\n",
    "\n",
    "            # Hacer predicciones para el último paso de tiempo\n",
    "            X_pred = batch_data[['tn']].values[-(n_steps + step_ahead):-step_ahead].reshape((1, n_steps, 1))\n",
    "            X_pred_scaled = scalers[model_key].transform(X_pred.reshape(-1, 1)).reshape(X_pred.shape)\n",
    "            pred = model.predict(X_pred_scaled, verbose=0)\n",
    "            pred_original_scale = scalers[model_key].inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "            pred_original_scale = np.clip(pred_original_scale, 0, None)\n",
    "            predictions.append([cluster, pred_original_scale[0]])\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "# Suponiendo que tienes pivot_df_scaled, grouped_df, ratio_dict, y df definidos previamente\n",
    "optimal_k = 15\n",
    "\n",
    "# Ajustar KMeans a los datos escalados\n",
    "kmeans_model = TimeSeriesKMeans(n_clusters=optimal_k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(pivot_df_scaled)\n",
    "\n",
    "# Añadir el cluster al DataFrame original\n",
    "pivot_df['cluster'] = clusters\n",
    "grouped_df = pd.merge(grouped_df, pivot_df['cluster'].reset_index(), on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])\n",
    "\n",
    "\n",
    "# Escalar los datos en grouped_df\n",
    "for col in ['tn']:\n",
    "    scaler = StandardScaler()\n",
    "    grouped_df[col] = scaler.fit_transform(grouped_df[[col]])\n",
    "\n",
    "# Definir parámetros de procesamiento en lotes\n",
    "n_steps = 13\n",
    "step_ahead = 2\n",
    "batch_size = 10000\n",
    "\n",
    "# Procesar datos en lotes y obtener modelos y predicciones\n",
    "models, predictions = process_data_in_batches(grouped_df, clusters, pivot_df, ratio_dict, n_steps, step_ahead, batch_size)\n",
    "\n",
    "# Convertir las predicciones a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['cluster', 'prediction'])\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in predictions_df.iterrows():\n",
    "    cluster = row['cluster']\n",
    "    for (cat1, cat2, cat3, brand, customer_id, product_id), ratio in ratio_dict.items():\n",
    "        if cluster == clusters[pivot_df.index.get_loc((cat1, cat2, cat3, brand, customer_id))]:\n",
    "            final_predictions.append([product_id, row['prediction'] * ratio])\n",
    "\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediction'])\n",
    "\n",
    "# Calcular el promedio de tn para los últimos 12 meses para cada product_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "last_12_months = df[df['periodo'] >= df['periodo'].max() - pd.DateOffset(months=12)]\n",
    "average_tn_last_12_months = last_12_months.groupby('product_id')['tn'].mean().reset_index()\n",
    "\n",
    "# Identificar los product_id faltantes en final_predictions_df\n",
    "missing_product_ids = set(df['product_id']) - set(final_predictions_df['product_id'])\n",
    "average_predictions = average_tn_last_12_months[average_tn_last_12_months['product_id'].isin(missing_product_ids)]\n",
    "\n",
    "# Concatenar predicciones finales y promedios\n",
    "complete_predictions_df = pd.concat([final_predictions_df, average_predictions], ignore_index=True)\n",
    "\n",
    "# Guardar las predicciones finales con el nombre de archivo original\n",
    "complete_predictions_df.to_csv(\"C:/Users/Usuario/desktop/vero2/predicciones_finales_DTWv1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>tn</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10001</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10002</td>\n",
       "      <td>-0.082939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10003</td>\n",
       "      <td>-0.083639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10004</td>\n",
       "      <td>-0.101830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>10005</td>\n",
       "      <td>-0.111625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     periodo  cat1  cat2  cat3  brand  customer_id        tn  cluster\n",
       "0 2018-12-01     0     0     4     22        10001 -0.003176        0\n",
       "1 2018-12-01     0     0     4     22        10002 -0.082939        0\n",
       "2 2018-12-01     0     0     4     22        10003 -0.083639        0\n",
       "3 2018-12-01     0     0     4     22        10004 -0.101830        0\n",
       "4 2018-12-01     0     0     4     22        10005 -0.111625        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores distintos de cluster_x: [ 0 13  7  2 10  3  6  1  8  9 11  4 12  5 14]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar valores distintos de cluster_x\n",
    "distinct_cluster= grouped_df['cluster'].unique()\n",
    "print(\"Valores distintos de cluster_x:\", distinct_cluster)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "# Función para crear secuencias de tiempo\n",
    "def crear_secuencias(data, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - step_ahead):\n",
    "        X.append(data[['tn']].iloc[i:i + n_steps].values)\n",
    "        y.append(data['tn'].iloc[i + n_steps + step_ahead - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Función para construir el modelo LSTM\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((input_shape[0] // 2, 64)))\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Función para procesar los datos en lotes\n",
    "def process_data_in_batches(scaled_df, clusters, n_steps, step_ahead=1, batch_size=10000):\n",
    "    models = {}\n",
    "    predictions = []\n",
    "    scalers = {}\n",
    "\n",
    "    # Iterar sobre cada cluster\n",
    "    unique_clusters = scaled_df['cluster'].unique()\n",
    "\n",
    "    for cluster in tqdm(unique_clusters, desc='Processing clusters'):\n",
    "        group_data = scaled_df[scaled_df['cluster'] == cluster]\n",
    "        group_data = group_data.sort_values(by='periodo')\n",
    "\n",
    "        # Dividir en lotes\n",
    "        num_batches = len(group_data) // batch_size + 1\n",
    "        for i in range(num_batches):\n",
    "            batch_data = group_data.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            X, y = crear_secuencias(batch_data, n_steps, step_ahead)\n",
    "\n",
    "            if len(X) == 0 or len(y) == 0:\n",
    "                continue\n",
    "\n",
    "            # Escalar los datos\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "            # Construir y entrenar modelo LSTM\n",
    "            model = build_lstm_model((X_scaled.shape[1], X_scaled.shape[2]))\n",
    "            model.fit(X_scaled, y, epochs=100, verbose=0)\n",
    "\n",
    "            # Guardar modelo en el diccionario\n",
    "            model_key = f\"{cluster}_batch_{i}\"\n",
    "            models[model_key] = model\n",
    "            scalers[model_key] = scaler\n",
    "\n",
    "            # Hacer predicciones para el último paso de tiempo\n",
    "            X_pred = batch_data[['tn']].values[-(n_steps + step_ahead):-step_ahead].reshape((1, n_steps, 1))\n",
    "            X_pred_scaled = scalers[model_key].transform(X_pred.reshape(-1, 1)).reshape(X_pred.shape)\n",
    "            pred = model.predict(X_pred_scaled, verbose=0)\n",
    "            pred_original_scale = scalers[model_key].inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "            pred_original_scale = np.clip(pred_original_scale, 0, None)\n",
    "            predictions.append([cluster, pred_original_scale[0]])\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "# Suponiendo que tienes pivot_df_scaled, grouped_df, ratio_dict, y df definidos previamente\n",
    "optimal_k = 15\n",
    "\n",
    "# Ajustar KMeans a los datos escalados\n",
    "kmeans_model = TimeSeriesKMeans(n_clusters=optimal_k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(pivot_df_scaled)\n",
    "\n",
    "# Añadir el cluster al DataFrame original\n",
    "pivot_df['cluster'] = clusters\n",
    "grouped_df = pd.merge(grouped_df, pivot_df[['cluster']].reset_index(), on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])\n",
    "\n",
    "# Escalar los datos en grouped_df\n",
    "scaler = StandardScaler()\n",
    "grouped_df['tn'] = scaler.fit_transform(grouped_df[['tn']])\n",
    "\n",
    "# Definir parámetros de procesamiento en lotes\n",
    "n_steps = 13\n",
    "step_ahead = 2\n",
    "batch_size = 10000\n",
    "\n",
    "# Procesar datos en lotes y obtener modelos y predicciones\n",
    "models, predictions = process_data_in_batches(grouped_df, clusters, n_steps, step_ahead, batch_size)\n",
    "\n",
    "# Convertir las predicciones a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['cluster', 'prediction'])\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in predictions_df.iterrows():\n",
    "    cluster = row['cluster']\n",
    "    cluster_indices = pivot_df[pivot_df['cluster'] == cluster].index\n",
    "    for (cat1, cat2, cat3, brand, customer_id, product_id), ratio in ratio_dict.items():\n",
    "        if any((pivot_df.loc[idx, ['cat1', 'cat2', 'cat3', 'brand', 'customer_id']] == [cat1, cat2, cat3, brand, customer_id]).all() for idx in cluster_indices):\n",
    "            final_predictions.append([product_id, row['prediction'] * ratio])\n",
    "\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediction'])\n",
    "\n",
    "# Calcular el promedio de tn para los últimos 12 meses para cada product_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "last_12_months = df[df['periodo'] >= df['periodo'].max() - pd.DateOffset(months=12)]\n",
    "average_tn_last_12_months = last_12_months.groupby('product_id')['tn'].mean().reset_index()\n",
    "\n",
    "# Identificar los product_id faltantes en final_predictions_df\n",
    "missing_product_ids = set(df['product_id']) - set(final_predictions_df['product_id'])\n",
    "average_predictions = average_tn_last_12_months[average_tn_last_12_months['product_id'].isin(missing_product_ids)]\n",
    "\n",
    "# Concatenar predicciones finales y promedios\n",
    "complete_predictions_df = pd.concat([final_predictions_df, average_predictions], ignore_index=True)\n",
    "\n",
    "# Guardar las predicciones finales con el nombre de archivo original\n",
    "complete_predictions_df.to_csv(\"C:/Users/Usuario/desktop/vero2/predicciones_finales_DTWv1.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
