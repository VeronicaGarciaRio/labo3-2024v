{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en esta version tomo para el marketshare FEB19, si en algun mes de 2019 no esta le calcula el promedio de 2019 al final +  la arquitectura de German. si la prediccion es negativa pone 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional, Dense, TimeDistributed, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Bidirectional\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')  # Ignorar las advertencias de Keras\n",
    "\n",
    "\n",
    "\n",
    "file_path= \"C:/Users/Usuario/desktop/vero2/final_dataset_descr.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "\n",
    "#Paso1: reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "\n",
    "# Filtrar los datos por los periodos 201907, 201908 y 201909\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "\n",
    "# # Pivotear los datos para tener columnas separadas para cada periodo\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "\n",
    "# # Asegurar que las columnas 201907 y 201909 existen en el DataFrame\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "\n",
    "# # Calcular el promedio de julio y septiembre\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "\n",
    "# # Convertir de nuevo al formato largo\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], \n",
    "                                   var_name='periodo', value_name='tn')\n",
    "\n",
    "# # Unir con el dataframe original\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "# Paso 3: Aplicar LabelEncoder a las columnas categoricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "\n",
    "# Paso 4: Agrupar las ventas por periodo, cat1, cat2, cat3, marca y descripcion\n",
    "\n",
    "# Supongamos que 'periodo' es una columna de tipo string con formato 'YYYYMM'\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m', errors='coerce')\n",
    "\n",
    "# Filtrar datos para diciembre de 2019\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 2)]\n",
    "\n",
    "# Agrupar las ventas del año 2019 por periodo, cat1, cat2, cat3, brand, descripcion y product_id\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand','product_id'])['tn'].sum().reset_index()\n",
    "#'periodo', 'cat1', 'cat2', 'cat3', 'brand', 'descripcion', \n",
    "\n",
    "# Calcular el promedio de ventas por periodo para el año 2019 por grupo de cat1, cat2, cat3, brand, descripcion y product_id\n",
    "#average_sales_2019 = grouped_sales_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'descripcion', 'product_id'])['tn'].mean().reset_index()\n",
    "\n",
    "# Calcular el promedio total de ventas por grupo de cat1, cat2, cat3, brand y descripcion para el año 2019\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand'])['tn'].sum().reset_index()\n",
    "\n",
    "# Unir para calcular el ratio\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "\n",
    "# Crear un diccionario de ratios\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'product_id'])['ratio'].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "# Filtrar el DataFrame para los meses y IDs específicos\n",
    "df_subset = ratios_2019[(ratios_2019['product_id'] == 20001)]\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(df_subset)\n",
    "df_subset.tail(10)\n",
    "\n",
    "\n",
    "# Paso 5: Agrupar las ventas por periodo, cat1, cat2, cat3, marca y customer_id\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand']).agg({\n",
    "    'cust_request_qty': 'sum',\n",
    "    'cust_request_tn': 'sum',\n",
    "    'tn': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Crear un diccionario para almacenar los scalers\n",
    "scalers = {}\n",
    "scaled_df = grouped_df.copy()\n",
    "\n",
    "# Aplicar StandardScaler a cada columna de interés\n",
    "for col in ['cust_request_qty', 'cust_request_tn', 'tn']:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_df[col] = scaler.fit_transform(scaled_df[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "\n",
    "# Cargar los scalers guardados\n",
    "scalers = joblib.load('scalers.pkl')\n",
    "\n",
    "# Asumiendo que scaled_df ya está cargado y escalado\n",
    "scaled_df['periodo'] = pd.to_datetime(scaled_df['periodo'], format='%Y%m')\n",
    "\n",
    "# Función para crear secuencias de tiempo\n",
    "def crear_secuencias(data, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - step_ahead):\n",
    "        X.append(data[['tn']].iloc[i:i+n_steps].values)\n",
    "        y.append(data['tn'].iloc[i+n_steps+step_ahead-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Función para construir el modelo LSTM bidireccional\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(input_shape)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((input_shape[0] // 2, 64)))  # Reformatear para que sea compatible con Bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "    model.add(Dense(1))  # Ajustado a 1 unidad de salida\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Procesar los datos en lotes para múltiples variables\n",
    "def process_data_in_batches(scaled_df, n_steps, step_ahead=1):\n",
    "    models = {}\n",
    "    predictions = []\n",
    "\n",
    "    # Agrupar por 'cat1', 'cat2', 'cat3', 'brand'\n",
    "    for (cat1, cat2, cat3, brand), group_data in scaled_df.groupby(['cat1', 'cat2', 'cat3', 'brand']):\n",
    "        group_data = group_data.sort_values(by='periodo')\n",
    "        \n",
    "        # Crear secuencias de tiempo\n",
    "        X, y = crear_secuencias(group_data, n_steps, step_ahead)\n",
    "\n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            continue\n",
    "\n",
    "        # Escalar los datos de entrada\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "        # Construir y entrenar el modelo LSTM\n",
    "        model = build_lstm_model((X_scaled.shape[1], X_scaled.shape[2]))\n",
    "        model.fit(X_scaled, y, epochs=100, verbose=0)\n",
    "\n",
    "        # Guardar el modelo\n",
    "        model_key = (cat1, cat2, cat3, brand)\n",
    "        models[model_key] = model\n",
    "\n",
    "        # Predecir las ventas para los próximos 2 meses (ejemplo)\n",
    "        X_pred = group_data[['tn']].values[-(n_steps+step_ahead):-step_ahead].reshape((1, n_steps, 1))\n",
    "        X_pred_scaled = scaler.transform(X_pred.reshape(-1, 1)).reshape(X_pred.shape)\n",
    "        pred = model.predict(X_pred_scaled, verbose=0)\n",
    "\n",
    "        # Desescalar la predicción usando el scaler original para 'tn'\n",
    "        pred_original_scale = scalers['tn'].inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Asegurarse de que las predicciones no sean negativas\n",
    "        pred_original_scale = np.clip(pred_original_scale, 0, None)\n",
    "        \n",
    "        predictions.append([cat1, cat2, cat3, brand, pred_original_scale[0]])\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "# Llamada a la función con el DataFrame scaled_df\n",
    "n_steps = 13\n",
    "step_ahead = 2\n",
    "models, predictions = process_data_in_batches(scaled_df, n_steps, step_ahead)\n",
    "\n",
    "# Convertir las predicciones a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['cat1', 'cat2', 'cat3', 'brand', 'prediction'])\n",
    "\n",
    "# Sumarizar las predicciones por 'cat1', 'cat2', 'cat3', 'brand'\n",
    "summarized_preds = predictions_df.groupby(['cat1', 'cat2', 'cat3', 'brand'])['prediction'].sum().reset_index()\n",
    "\n",
    "# Paso 9: Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in summarized_preds.iterrows():\n",
    "    key = (row['cat1'], row['cat2'], row['cat3'], row['brand'])\n",
    "    for (cat1, cat2, cat3, brand, product_id), ratio in ratio_dict.items():\n",
    "        if (cat1, cat2, cat3, brand) == key:\n",
    "            final_predictions.append([product_id, row['prediction'] * ratio])\n",
    "\n",
    "# Convertir las predicciones finales a un DataFrame\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediction'])\n",
    "\n",
    "# Calcular el promedio de tn para los últimos 12 meses para cada product_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "last_12_months = df[df['periodo'] >= df['periodo'].max() - pd.DateOffset(months=12)]\n",
    "average_tn_last_12_months = last_12_months.groupby('product_id')['tn'].mean().reset_index()\n",
    "\n",
    "# Identificar los product_id faltantes en final_predictions_df\n",
    "missing_product_ids = set(df['product_id']) - set(final_predictions_df['product_id'])\n",
    "\n",
    "# Agregar los product_id faltantes con el promedio de tn de los últimos 12 meses\n",
    "for product_id in missing_product_ids:\n",
    "    avg_tn = average_tn_last_12_months.loc[average_tn_last_12_months['product_id'] == product_id, 'tn']\n",
    "    if not avg_tn.empty:\n",
    "        final_predictions_df = pd.concat([final_predictions_df, pd.DataFrame({'product_id': [product_id], 'prediction': [avg_tn.values[0]]})], ignore_index=True)\n",
    "\n",
    "# Guardar las predicciones finales en un archivo CSV\n",
    "final_predictions_df.to_csv(\"C:/Users/Usuario/desktop/vero2/predicciones_LSTM_GRUPOS_7v8.csv\", index=False)\n",
    "\n",
    "print(final_predictions_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
