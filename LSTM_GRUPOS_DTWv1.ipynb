{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en esta version tomo para el marketshare FEB19, si en algun mes de 2019 no esta le calcula el promedio de 2019 al final +  la arquitectura de German. si la prediccion es negativa pone 0. clusteriza con 15 (categorias, brand y customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtslearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesKMeans\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtslearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesScalerMinMax\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\colorbar.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, collections, cm, colors, contour, ticker\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\collections.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, cm, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, _docstring,\n\u001b[0;32m     21\u001b[0m                hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\lines.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artist, allow_rasterization\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     _to_unmasked_float_array, ls_mapper, ls_mapper_r, STEP_LOOKUP_MAP)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmarkers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkerStyle\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bbox, BboxTransformTo, TransformedPath\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\markers.py:150\u001b[0m\n\u001b[0;32m    143\u001b[0m (TICKLEFT, TICKRIGHT, TICKUP, TICKDOWN,\n\u001b[0;32m    144\u001b[0m  CARETLEFT, CARETRIGHT, CARETUP, CARETDOWN,\n\u001b[0;32m    145\u001b[0m  CARETLEFTBASE, CARETRIGHTBASE, CARETUPBASE, CARETDOWNBASE) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m    147\u001b[0m _empty_path \u001b[38;5;241m=\u001b[39m Path(np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMarkerStyle\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;43;03m    A class representing marker types.\u001b[39;49;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;43;03m        The supported fillstyles.\u001b[39;49;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpixel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnothing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\markers.py:868\u001b[0m, in \u001b[0;36mMarkerStyle\u001b[1;34m()\u001b[0m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x_path\n\u001b[1;32m--> 868\u001b[0m _plus_filled_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_closed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    871\u001b[0m _plus_filled_path_t \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39m_create_closed(np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m    872\u001b[0m     (\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m    873\u001b[0m     (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m)]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_plus_filled\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\path.py:199\u001b[0m, in \u001b[0;36mPath._create_closed\u001b[1;34m(cls, vertices)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_closed\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vertices):\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    Create a closed polygonal path going through *vertices*.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    Unlike ``Path(..., closed=True)``, *vertices* should **not** end with\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    an entry for the CLOSEPATH; this entry is added by `._create_closed`.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43m_to_unmasked_float_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(np\u001b[38;5;241m.\u001b[39mconcatenate([v, v[:\u001b[38;5;241m1\u001b[39m]]), closed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vgARCIARIO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\cbook.py:1390\u001b[0m, in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     even_dollars \u001b[38;5;241m=\u001b[39m (dollar_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dollar_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m even_dollars\n\u001b[1;32m-> 1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_unmasked_float_array\u001b[39m(x):\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;124;03m    Convert a sequence to a float array; if input was a masked array, masked\u001b[39;00m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;124;03m    values are converted to nans.\u001b[39;00m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  product_id  periodo  plan_precios_cuidados  cust_request_qty  \\\n",
      "0        10001       20001   201812                    0.0              20.0   \n",
      "1        10001       20001   201901                    0.0              53.0   \n",
      "2        10001       20001   201902                    0.0              39.0   \n",
      "3        10001       20001   201903                    0.0              23.0   \n",
      "4        10001       20001   201904                    0.0              33.0   \n",
      "\n",
      "   cust_request_tn         tn cat1         cat2     cat3  brand  sku_size  \\\n",
      "0        254.62373  254.62373   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
      "1        393.26092  386.60688   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
      "2        309.90610  309.90610   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
      "3        142.87158  130.54927   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
      "4        364.37071  364.37071   HC  ROPA LAVADO  Liquido  ARIEL      3000   \n",
      "\n",
      "  descripcion quarter  month  close_quarter   age mes_inicial  \n",
      "0      genoma      Q4     12            1.0  23.0  2018-12-01  \n",
      "1      genoma      Q1      1            0.0  24.0  2018-12-01  \n",
      "2      genoma      Q1      2            0.0  25.0  2018-12-01  \n",
      "3      genoma      Q1      3            1.0  26.0  2018-12-01  \n",
      "4      genoma      Q2      4            0.0  27.0  2018-12-01  \n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Cargar y preprocesar los datos\n",
    "file_path = \"C:/Users/Usuario/desktop/vero2/final_dataset_completo_con_ceros.csv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head(12)\n",
    "\n",
    "filtered_df = df[df['product_id'].between(20001, 20012)]\n",
    "\n",
    "# Opcional: Ver las primeras filas del DataFrame filtrado\n",
    "print(filtered_df.head())\n",
    "\n",
    "filtered_df=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reemplazar 082019 por promedio 07 y 09\n",
    "df['periodo'] = df['periodo'].astype(str).str.strip()\n",
    "df_filtered = df[df['periodo'].isin(['201907', '201908', '201909'])]\n",
    "pivoted_sales = df_filtered.pivot_table(index=['product_id', 'customer_id'], columns='periodo', values='tn').reset_index()\n",
    "pivoted_sales = pivoted_sales.reindex(columns=['product_id', 'customer_id', '201907', '201908', '201909'])\n",
    "pivoted_sales['201908'] = pivoted_sales[['201907', '201909']].mean(axis=1)\n",
    "updated_sales = pivoted_sales.melt(id_vars=['product_id', 'customer_id'], value_vars=['201907', '201908', '201909'], var_name='periodo', value_name='tn')\n",
    "df.set_index(['product_id', 'customer_id', 'periodo'], inplace=True)\n",
    "df.update(updated_sales.set_index(['product_id', 'customer_id', 'periodo']))\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Aplicar LabelEncoder a las columnas categóricas\n",
    "categorical_cols = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Agrupar las ventas por periodo, cat1, cat2, cat3, brand y customer_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m', errors='coerce')\n",
    "grouped_df = df.groupby(['periodo', 'cat1', 'cat2', 'cat3', 'brand', 'customer_id']).agg({'tn': 'sum'}).reset_index()\n",
    "\n",
    "# Paso 2: Calcular los ratios incluyendo customer_id\n",
    "df_diciembre_2019 = df[(df['periodo'].dt.year == 2019) & (df['periodo'].dt.month == 12)]\n",
    "grouped_sales_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['tn'].sum().reset_index()\n",
    "group_totals_2019 = df_diciembre_2019.groupby(['cat1', 'cat2', 'cat3', 'brand', 'customer_id'])['tn'].sum().reset_index()\n",
    "ratios_2019 = pd.merge(grouped_sales_2019, group_totals_2019, on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], suffixes=('', '_total'))\n",
    "ratios_2019['ratio'] = ratios_2019['tn'] / ratios_2019['tn_total']\n",
    "ratio_dict = ratios_2019.set_index(['cat1', 'cat2', 'cat3', 'brand', 'customer_id', 'product_id'])['ratio'].to_dict()\n",
    "\n",
    "# Paso 3: Clustering con DTW\n",
    "pivot_df = grouped_df.pivot_table(index=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], columns='periodo', values='tn', fill_value=0)\n",
    "pivot_df_values = pivot_df.values\n",
    "\n",
    "# Escalar los datos\n",
    "scalers = {}\n",
    "scaler = StandardScaler()\n",
    "pivot_df_scaled = scaler.fit_transform(pivot_df_values)\n",
    "\n",
    "\n",
    "# Guardar los scalers para su uso posterior\n",
    "joblib.dump(scalers, 'scalers.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinar el número óptimo de clusters utilizando el diagrama de codo\n",
    "distortions = []\n",
    "K = range(6, 15)\n",
    "for k in K:\n",
    "    kmeans_model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "    kmeans_model.fit(pivot_df_scaled)\n",
    "    distortions.append(kmeans_model.inertia_)\n",
    "\n",
    " Visualizar el diagrama de codo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\usuario\\downloads\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pivot_df no contiene todas las columnas necesarias: ['cat1', 'cat2', 'cat3', 'brand', 'customer_id']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m required_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m pivot_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_df no contiene todas las columnas necesarias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m grouped_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrouped_df no contiene todas las columnas necesarias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: pivot_df no contiene todas las columnas necesarias: ['cat1', 'cat2', 'cat3', 'brand', 'customer_id']"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, Bidirectional, LSTM, Dense\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para crear secuencias de tiempo\n",
    "def crear_secuencias(data, n_steps, step_ahead=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps - step_ahead):\n",
    "        X.append(data[['tn']].iloc[i:i + n_steps].values)\n",
    "        y.append(data['tn'].iloc[i + n_steps + step_ahead - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Función para construir el modelo LSTM\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((input_shape[0] // 2, 64)))\n",
    "    model.add(Bidirectional(LSTM(50, activation='relu')))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Función para procesar los datos en lotes\n",
    "def process_data_in_batches(scaled_df, clusters, n_steps, step_ahead=1, batch_size=10000):\n",
    "    models = {}\n",
    "    predictions = []\n",
    "    scalers = {}\n",
    "\n",
    "    # Iterar sobre cada cluster\n",
    "    unique_clusters = scaled_df['cluster'].unique()\n",
    "\n",
    "    for cluster in tqdm(unique_clusters, desc='Processing clusters'):\n",
    "        group_data = scaled_df[scaled_df['cluster'] == cluster]\n",
    "        group_data = group_data.sort_values(by='periodo')\n",
    "\n",
    "        # Dividir en lotes\n",
    "        num_batches = len(group_data) // batch_size + 1\n",
    "        for i in range(num_batches):\n",
    "            batch_data = group_data.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "            X, y = crear_secuencias(batch_data, n_steps, step_ahead)\n",
    "\n",
    "            if len(X) == 0 or len(y) == 0:\n",
    "                continue\n",
    "\n",
    "            # Escalar los datos\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "            # Construir y entrenar modelo LSTM\n",
    "            model = build_lstm_model((X_scaled.shape[1], X_scaled.shape[2]))\n",
    "            model.fit(X_scaled, y, epochs=100, verbose=0)\n",
    "\n",
    "            # Guardar modelo en el diccionario\n",
    "            model_key = f\"{cluster}_batch_{i}\"\n",
    "            models[model_key] = model\n",
    "            scalers[model_key] = scaler\n",
    "\n",
    "            # Hacer predicciones para el último paso de tiempo\n",
    "            X_pred = batch_data[['tn']].values[-(n_steps + step_ahead):-step_ahead].reshape((1, n_steps, 1))\n",
    "            X_pred_scaled = scalers[model_key].transform(X_pred.reshape(-1, 1)).reshape(X_pred.shape)\n",
    "            pred = model.predict(X_pred_scaled, verbose=0)\n",
    "            pred_original_scale = scalers[model_key].inverse_transform(pred.reshape(-1, 1)).flatten()\n",
    "            pred_original_scale = np.clip(pred_original_scale, 0, None)\n",
    "            predictions.append([cluster, pred_original_scale[0]])\n",
    "\n",
    "    return models, predictions\n",
    "\n",
    "# Suponiendo que tienes pivot_df_scaled, grouped_df, ratio_dict, y df definidos previamente\n",
    "optimal_k = 15\n",
    "\n",
    "# Ajustar KMeans a los datos escalados\n",
    "kmeans_model = TimeSeriesKMeans(n_clusters=optimal_k, metric=\"dtw\", verbose=0, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(pivot_df_scaled)\n",
    "\n",
    "\n",
    "# Añadir el cluster al DataFrame original\n",
    "pivot_df['cluster'] = clusters\n",
    "grouped_df = pd.merge(grouped_df, pivot_df[['cluster']], on=['cat1', 'cat2', 'cat3', 'brand', 'customer_id'], right_index=True)\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "grouped_df.to_csv('grouped_with_clusters.csv', index=False)\n",
    "# Mostrar el DataFrame final con los números de grupo\n",
    "display(grouped_df)\n",
    "\n",
    "\n",
    "\n",
    "# Escalar los datos en grouped_df\n",
    "scaler = StandardScaler()\n",
    "grouped_df['tn'] = scaler.fit_transform(grouped_df[['tn']])\n",
    "\n",
    "# Definir parámetros de procesamiento en lotes\n",
    "n_steps = 13\n",
    "step_ahead = 2\n",
    "batch_size = 10000\n",
    "\n",
    "# Procesar datos en lotes y obtener modelos y predicciones\n",
    "models, predictions = process_data_in_batches(grouped_df, clusters, n_steps, step_ahead, batch_size)\n",
    "\n",
    "# Convertir las predicciones a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['cluster', 'prediction'])\n",
    "\n",
    "# Aplicar los ratios para obtener las predicciones finales por product_id\n",
    "final_predictions = []\n",
    "for _, row in predictions_df.iterrows():\n",
    "    cluster = row['cluster']\n",
    "    cluster_indices = pivot_df[pivot_df['cluster'] == cluster].index\n",
    "    for (cat1, cat2, cat3, brand, customer_id, product_id), ratio in ratio_dict.items():\n",
    "        if any((pivot_df.loc[idx, ['cat1', 'cat2', 'cat3', 'brand', 'customer_id']] == [cat1, cat2, cat3, brand, customer_id]).all() for idx in cluster_indices):\n",
    "            final_predictions.append([product_id, row['prediction'] * ratio])\n",
    "\n",
    "final_predictions_df = pd.DataFrame(final_predictions, columns=['product_id', 'prediction'])\n",
    "\n",
    "# Calcular el promedio de tn para los últimos 12 meses para cada product_id\n",
    "df['periodo'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "last_12_months = df[df['periodo'] >= df['periodo'].max() - pd.DateOffset(months=12)]\n",
    "average_tn_last_12_months = last_12_months.groupby('product_id')['tn'].mean().reset_index()\n",
    "\n",
    "# Identificar los product_id faltantes en final_predictions_df\n",
    "missing_product_ids = set(df['product_id']) - set(final_predictions_df['product_id'])\n",
    "average_predictions = average_tn_last_12_months[average_tn_last_12_months['product_id'].isin(missing_product_ids)]\n",
    "\n",
    "# Concatenar predicciones finales y promedios\n",
    "complete_predictions_df = pd.concat([final_predictions_df, average_predictions], ignore_index=True)\n",
    "\n",
    "# Guardar las predicciones finales con el nombre de archivo original\n",
    "complete_predictions_df.to_csv(\"C:/Users/Usuario/desktop/vero2/predicciones_finales_DTWv1.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
